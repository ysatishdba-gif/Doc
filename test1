# ============================================================
# ENHANCED CLINICALLY HARDENED CUI REDUCTION ENGINE
# ============================================================

import time
import numpy as np
import networkx as nx
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass, asdict
from google.cloud import bigquery
from sklearn.metrics.pairwise import cosine_distances
import logging
import pickle

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------
# Stats
# ------------------------------------------------------------
@dataclass
class ReductionStats:
    initial_count: int
    final_count: int
    total_reduction_pct: float
    coverage_score: float
    processing_time: float
    dropped_by_ic: int
    dropped_by_similarity: int
    dropped_by_aggressive: int
    forced_retained: int
    groups_processed: int
    missing_embeddings: int

    def to_dict(self):
        return asdict(self)

# ------------------------------------------------------------
# Enhanced Clinical Reducer
# ------------------------------------------------------------
class EnhancedClinicalCUIReducer:
    """
    Enhanced clinically hardened reducer:
    - Hierarchy-first approach
    - IC-aware selection
    - Multi-stage similarity reduction
    - Clinical safety guards
    """

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        embeddings_table: str,
        mrsty_table: str,
        umls_graph,
        # IC selection parameters
        ic_low_pct: int = 30,
        ic_high_pct: int = 70,
        # Similarity thresholds
        similarity_threshold: float = 0.88,
        aggressive_threshold: float = 0.85,
        # Control parameters
        enable_aggressive: bool = True,
        target_reduction_pct: float = 80.0
    ):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.embeddings_table = embeddings_table
        self.mrsty_table = mrsty_table
        self.graph = umls_graph

        # Parameters
        self.ic_low_pct = ic_low_pct
        self.ic_high_pct = ic_high_pct
        self.similarity_threshold = similarity_threshold
        self.aggressive_threshold = aggressive_threshold
        self.enable_aggressive = enable_aggressive
        self.target_reduction_pct = target_reduction_pct

        # Caches
        self._emb_cache = {}
        self._ic_cache = {}
        self._missing_embeddings = 0

    # ========================================================
    # PUBLIC API
    # ========================================================
    def reduce(self, cuis: List[str]) -> Tuple[List[str], ReductionStats]:
        """
        Main reduction pipeline with enhanced features
        """
        start = time.time()
        cuis = list(set(cuis))
        initial_count = len(cuis)
        
        logger.info(f"Starting enhanced reduction for {initial_count} CUIs")
        logger.info(f"Target reduction: {self.target_reduction_pct}%")

        # 1. Semantic grouping
        groups = self._group_by_semantic_type(cuis)
        logger.info(f"Created {len(groups)} semantic groups")

        retained: Set[str] = set()
        dropped_by_ic = 0
        dropped_by_sim = 0
        dropped_by_aggressive = 0
        forced_retained = 0

        for sty, group_cuis in groups.items():
            if not group_cuis:
                continue
                
            logger.info(f"\nProcessing {sty}: {len(group_cuis)} CUIs")

            # 2. Build induced subgraph
            hierarchy = self._induced_subgraph(group_cuis)
            logger.info(f"  Subgraph: {hierarchy.number_of_nodes()} nodes, {hierarchy.number_of_edges()} edges")

            # 3. Compute IC scores
            ic_scores = self._compute_ic(group_cuis, hierarchy)
            
            # 4. Clinical IC selection
            selected, forced = self._clinical_ic_selection(
                group_cuis, ic_scores, hierarchy
            )
            forced_retained += forced
            dropped_by_ic += len(group_cuis) - len(selected)
            logger.info(f"  IC selection: {len(group_cuis)} → {len(selected)} ({forced} forced)")

            # 5. Hierarchy-guarded similarity pruning
            pruned, sim_dropped = self._clinical_similarity_prune(
                selected, hierarchy
            )
            dropped_by_sim += sim_dropped
            logger.info(f"  Similarity pruning: {len(selected)} → {len(pruned)}")

            # 6. Aggressive reduction if enabled and needed
            if self.enable_aggressive and self._needs_more_reduction(initial_count, len(retained) + len(pruned)):
                final_group, agg_dropped = self._aggressive_reduction(pruned, hierarchy)
                dropped_by_aggressive += agg_dropped
                logger.info(f"  Aggressive reduction: {len(pruned)} → {len(final_group)}")
            else:
                final_group = pruned

            retained.update(final_group)

        # 7. Final cross-group deduplication
        final_retained = self._cross_group_deduplication(retained)
        logger.info(f"\nCross-group deduplication: {len(retained)} → {len(final_retained)}")

        # 8. Calculate coverage
        coverage = self._coverage_score(cuis, final_retained)

        final = list(final_retained)
        stats = ReductionStats(
            initial_count=initial_count,
            final_count=len(final),
            total_reduction_pct=((initial_count - len(final)) / initial_count * 100) if initial_count else 0,
            coverage_score=coverage,
            processing_time=time.time() - start,
            dropped_by_ic=dropped_by_ic,
            dropped_by_similarity=dropped_by_sim,
            dropped_by_aggressive=dropped_by_aggressive,
            forced_retained=forced_retained,
            groups_processed=len(groups),
            missing_embeddings=self._missing_embeddings
        )

        logger.info(f"\n{'='*60}")
        logger.info(f"REDUCTION COMPLETE")
        logger.info(f"{'='*60}")
        logger.info(f"Initial: {initial_count}")
        logger.info(f"Final: {len(final)}")
        logger.info(f"Reduction: {stats.total_reduction_pct:.1f}%")
        logger.info(f"Coverage: {coverage:.1%}")
        logger.info(f"Processing time: {stats.processing_time:.2f}s")

        return final, stats

    # ========================================================
    # SEMANTIC GROUPING
    # ========================================================
    def _group_by_semantic_type(self, cuis: List[str]) -> Dict[str, List[str]]:
        """Group CUIs by semantic type"""
        query = f"""
        SELECT DISTINCT CUI, STY
        FROM `{self.project_id}.{self.dataset_id}.{self.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        df = self.client.query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cuis)]
            )
        ).to_dataframe()

        if df.empty:
            logger.warning("No semantic types found, using UNKNOWN group")
            return {"UNKNOWN": cuis}

        # Handle CUIs with multiple types
        cui_to_types = defaultdict(set)
        for _, row in df.iterrows():
            cui_to_types[row['CUI']].add(row['STY'])

        groups = defaultdict(list)
        for cui in cuis:
            types = cui_to_types.get(cui, {'UNKNOWN'})
            # Assign to most specific type (longest name)
            best_type = max(types, key=lambda x: len(x))
            groups[best_type].append(cui)

        return dict(groups)

    # ========================================================
    # HIERARCHY OPERATIONS
    # ========================================================
    def _induced_subgraph(self, cuis: List[str]) -> nx.DiGraph:
        """Create induced subgraph for CUIs"""
        nodes = set(c for c in cuis if self.graph.has_node(c))
        
        if not nodes:
            return nx.DiGraph()
        
        # Include immediate parents and children for better context
        extended_nodes = set(nodes)
        for node in nodes:
            if self.graph.has_node(node):
                # Add immediate parents
                extended_nodes.update(self.graph.predecessors(node))
                # Add immediate children
                extended_nodes.update(self.graph.successors(node))
        
        # Filter to only include nodes that connect our CUIs
        sub = self.graph.subgraph(extended_nodes).copy()
        
        return sub

    # ========================================================
    # INFORMATION CONTENT
    # ========================================================
    def _compute_ic(self, cuis: List[str], graph: nx.DiGraph) -> Dict[str, float]:
        """
        Compute Information Content scores
        IC = -log((descendants + 1) / group_size)
        Higher IC = fewer descendants (more specific)
        Lower IC = more descendants (more general)
        """
        N = len(cuis)
        ic = {}

        for cui in cuis:
            try:
                if graph.has_node(cui):
                    # Count descendants within the group
                    descendants = set()
                    for desc in nx.descendants(graph, cui):
                        if desc in cuis:
                            descendants.add(desc)
                    ic[cui] = -np.log((len(descendants) + 1) / N)
                else:
                    # No hierarchy info → assign high IC (specific)
                    ic[cui] = -np.log(1 / N)
            except Exception as e:
                logger.debug(f"IC computation failed for {cui}: {e}")
                # Fail closed → assign high IC to retain
                ic[cui] = float("inf")

        return ic

    # ========================================================
    # CLINICAL IC SELECTION
    # ========================================================
    def _clinical_ic_selection(
        self,
        cuis: List[str],
        ic: Dict[str, float],
        graph: nx.DiGraph
    ) -> Tuple[Set[str], int]:
        """
        Strategic IC-based selection:
        - Select low IC (general) as anchors
        - Add high IC (specific) only if not covered
        """
        if not ic:
            return set(cuis), 0

        values = [v for v in ic.values() if not np.isinf(v)]
        if not values:
            return set(cuis), len(cuis)

        low_threshold = np.percentile(values, self.ic_low_pct)
        high_threshold = np.percentile(values, self.ic_high_pct)

        # 1. Get anchors (general concepts with low IC)
        anchors = {c for c in cuis if ic.get(c, float('inf')) <= low_threshold}
        
        # 2. Calculate coverage
        covered = set(anchors)
        for anchor in anchors:
            if graph.has_node(anchor):
                try:
                    # Add all descendants
                    for desc in nx.descendants(graph, anchor):
                        if desc in cuis:
                            covered.add(desc)
                except:
                    pass

        # 3. Add uncovered specifics (high IC)
        essentials = {
            c for c in cuis
            if ic.get(c, 0) >= high_threshold and c not in covered
        }

        selected = anchors | essentials
        forced = len(essentials)

        return selected, forced

    # ========================================================
    # SIMILARITY PRUNING (HIERARCHY-GUARDED)
    # ========================================================
    def _clinical_similarity_prune(
        self,
        cuis: Set[str],
        graph: nx.DiGraph
    ) -> Tuple[Set[str], int]:
        """
        Remove similar CUIs only if hierarchically related
        This prevents dropping clinically distinct but similar concepts
        """
        if len(cuis) <= 1:
            return cuis, 0

        embeddings = self._fetch_embeddings(list(cuis))
        valid = [c for c in cuis if c in embeddings]

        if len(valid) < 2:
            return cuis, 0

        X = np.vstack([embeddings[c] for c in valid])
        sim = 1 - cosine_distances(X, X)

        dropped = set()
        
        for i, c1 in enumerate(valid):
            if c1 in dropped:
                continue
            for j in range(i + 1, len(valid)):
                c2 = valid[j]
                if c2 in dropped:
                    continue
                    
                if sim[i, j] >= self.similarity_threshold:
                    # Only drop if hierarchically related
                    try:
                        if graph.has_node(c1) and graph.has_node(c2):
                            if nx.has_path(graph, c1, c2):
                                # c1 is ancestor of c2, drop c2 (more specific)
                                dropped.add(c2)
                            elif nx.has_path(graph, c2, c1):
                                # c2 is ancestor of c1, drop c1 (more specific)
                                dropped.add(c1)
                                break  # c1 is dropped, move to next i
                    except:
                        # If path check fails, keep both (safe)
                        pass

        final = cuis - dropped
        return final, len(dropped)

    # ========================================================
    # AGGRESSIVE REDUCTION
    # ========================================================
    def _aggressive_reduction(
        self,
        cuis: Set[str],
        graph: nx.DiGraph
    ) -> Tuple[Set[str], int]:
        """
        More aggressive reduction when needed
        Still hierarchy-aware but with lower threshold
        """
        if len(cuis) <= 2:
            return cuis, 0

        embeddings = self._fetch_embeddings(list(cuis))
        valid = [c for c in cuis if c in embeddings]
        no_emb = [c for c in cuis if c not in embeddings]

        if len(valid) < 2:
            return cuis, 0

        X = np.vstack([embeddings[c] for c in valid])
        sim = 1 - cosine_distances(X, X)

        # Greedy selection for maximum diversity
        selected_indices = [0]  # Start with first
        
        for i in range(1, len(valid)):
            # Check similarity to all selected
            max_sim = max(sim[i, j] for j in selected_indices)
            
            if max_sim < self.aggressive_threshold:
                selected_indices.append(i)
            else:
                # Check if hierarchically important
                cui = valid[i]
                is_important = False
                
                # Keep if it's a root or has many children
                if graph.has_node(cui):
                    try:
                        if graph.in_degree(cui) == 0:  # Root node
                            is_important = True
                        elif graph.out_degree(cui) > 5:  # Many children
                            is_important = True
                    except:
                        pass
                
                if is_important:
                    selected_indices.append(i)

        selected = {valid[i] for i in selected_indices}
        selected.update(no_emb)  # Keep CUIs without embeddings
        
        return selected, len(cuis) - len(selected)

    # ========================================================
    # CROSS-GROUP DEDUPLICATION
    # ========================================================
    def _cross_group_deduplication(self, cuis: Set[str]) -> Set[str]:
        """
        Final deduplication across semantic groups
        """
        if len(cuis) <= 1:
            return cuis

        embeddings = self._fetch_embeddings(list(cuis))
        valid = [c for c in cuis if c in embeddings]
        no_emb = set(c for c in cuis if c not in embeddings)

        if len(valid) < 2:
            return cuis

        X = np.vstack([embeddings[c] for c in valid])
        sim = 1 - cosine_distances(X, X)

        # Find very similar pairs across groups
        dropped = set()
        for i in range(len(valid)):
            if valid[i] in dropped:
                continue
            for j in range(i + 1, len(valid)):
                if valid[j] in dropped:
                    continue
                if sim[i, j] >= 0.95:  # Very high threshold for cross-group
                    # Keep the one with more graph connections
                    if self.graph.has_node(valid[i]) and self.graph.has_node(valid[j]):
                        deg_i = self.graph.degree(valid[i])
                        deg_j = self.graph.degree(valid[j])
                        if deg_i >= deg_j:
                            dropped.add(valid[j])
                        else:
                            dropped.add(valid[i])
                            break
                    else:
                        dropped.add(valid[j])  # Arbitrary choice

        return (set(valid) - dropped) | no_emb

    # ========================================================
    # EMBEDDINGS
    # ========================================================
    def _fetch_embeddings(self, cuis: List[str]) -> Dict[str, np.ndarray]:
        """Fetch embeddings with caching"""
        missing = [c for c in cuis if c not in self._emb_cache]
        
        if missing:
            query = f"""
            SELECT CUI, embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.embeddings_table}`
            WHERE CUI IN UNNEST(@cuis)
            """
            df = self.client.query(
                query,
                job_config=bigquery.QueryJobConfig(
                    query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", missing)]
                )
            ).to_dataframe()

            found = set()
            for _, r in df.iterrows():
                if r.embedding is not None:
                    emb = np.asarray(r.embedding, dtype=np.float32)
                    if emb.ndim == 1 and emb.size > 0:
                        self._emb_cache[r.CUI] = emb
                        found.add(r.CUI)
            
            self._missing_embeddings += len(missing) - len(found)

        return {c: self._emb_cache[c] for c in cuis if c in self._emb_cache}

    # ========================================================
    # COVERAGE CALCULATION
    # ========================================================
    def _coverage_score(self, original: List[str], retained: Set[str]) -> float:
        """
        Calculate coverage using transitive closure
        A CUI is covered if it or its ancestor is retained
        """
        if not original:
            return 0.0
            
        covered = set(retained)
        
        # Add all descendants of retained CUIs
        for cui in retained:
            if self.graph.has_node(cui):
                try:
                    descendants = nx.descendants(self.graph, cui)
                    covered.update(descendants)
                except:
                    pass
        
        # Check coverage
        original_set = set(original)
        covered_count = len(original_set & covered)
        
        return covered_count / len(original)

    # ========================================================
    # UTILITY
    # ========================================================
    def _needs_more_reduction(self, initial: int, current: int) -> bool:
        """Check if we need more aggressive reduction"""
        current_reduction = ((initial - current) / initial * 100) if initial else 0
        return current_reduction < self.target_reduction_pct


# ============================================================
# MAIN PIPELINE
# ============================================================
if __name__ == "__main__":
    
    # Load UMLS network graph
    NETWORK_PKL = "/path/to/umls_network.pkl"
    with open(NETWORK_PKL, "rb") as f:
        UMLS_GRAPH = pickle.load(f)
    logger.info(f"Loaded UMLS graph: {UMLS_GRAPH.number_of_nodes()} nodes, {UMLS_GRAPH.number_of_edges()} edges")
    
    # Configuration
    project_id = "your_project_id"
    dataset_id = "your_dataset"
    embeddings_table = "cui_embeddings"
    mrsty_table = "MRSTY"
    
    # Test CUIs (example)
    test_cuis = [
        "C0011849",  # Diabetes Mellitus
        "C0011860",  # Type 2 Diabetes
        "C0011854",  # Type 1 Diabetes
        "C0006826",  # Cancer
        "C0242379",  # Lung Cancer
        "C0006142",  # Breast Cancer
        "C0020538",  # Hypertension
        "C0018799",  # Heart Disease
    ]
    
    # Initialize reducer with clinical parameters
    reducer = EnhancedClinicalCUIReducer(
        project_id=project_id,
        dataset_id=dataset_id,
        embeddings_table=embeddings_table,
        mrsty_table=mrsty_table,
        umls_graph=UMLS_GRAPH,
        # IC selection
        ic_low_pct=30,   # Keep general concepts below 30th percentile
        ic_high_pct=70,  # Keep specific concepts above 70th if not covered
        # Similarity thresholds
        similarity_threshold=0.88,  # Standard similarity
        aggressive_threshold=0.85,  # Aggressive similarity
        # Control
        enable_aggressive=True,
        target_reduction_pct=80.0  # Target 80% reduction
    )
    
    # Run reduction
    final_cuis, stats = reducer.reduce(test_cuis)
    
    # Print detailed results
    print("\n" + "="*60)
    print("REDUCTION RESULTS")
    print("="*60)
    print(f"Initial CUIs: {stats.initial_count}")
    print(f"Final CUIs: {stats.final_count}")
    print(f"Total Reduction: {stats.total_reduction_pct:.1f}%")
    print(f"Coverage Score: {stats.coverage_score:.1%}")
    print(f"\nBreakdown:")
    print(f"  Dropped by IC: {stats.dropped_by_ic}")
    print(f"  Dropped by similarity: {stats.dropped_by_similarity}")
    print(f"  Dropped by aggressive: {stats.dropped_by_aggressive}")
    print(f"  Forced retained: {stats.forced_retained}")
    print(f"  Missing embeddings: {stats.missing_embeddings}")
    print(f"\nProcessing:")
    print(f"  Groups processed: {stats.groups_processed}")
    print(f"  Time: {stats.processing_time:.2f} seconds")
    
    print(f"\nFinal CUIs: {final_cuis}")
