# === IMPORTS & SETUP ===
import os
import random
import numpy as np
import torch
import nltk
import logging
import re

from collections import defaultdict, Counter
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from hdbscan import HDBSCAN
from umap import UMAP
import scann

from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary
from sklearn.metrics import silhouette_score, precision_recall_fscore_support
from sklearn.metrics.pairwise import cosine_similarity

# === SEED FIXING ===
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.manual_seed_all(SEED)
torch.use_deterministic_algorithms(True)
nltk.download("punkt")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === GLOBAL WARNING & LOG SUPPRESSION ===
import warnings
warnings.filterwarnings('ignore')
 
import os
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
 
import logging
logging.basicConfig(level=logging.ERROR)  # Only show errors and above


# === CLEANING & CONTEXT EXTRACTION (IMPROVED) ===
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    return re.sub(r"\s+", " ", text).strip()


def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):
    entity_context_pairs = []
    for idx, ents in enumerate(entities_per_chunk):
        chunk = clean_text(chunks[idx])
        sentences = sent_tokenize(chunk)
        for ent in ents:
            ent_lower = ent.lower()
            matched = False
            for i, sent in enumerate(sentences):
                if ent_lower in sent:
                    context = " ".join(sentences[max(0, i - 1): i + 2]) if use_multi_sentence else sent.strip()
                    enriched = f"The concept '{ent_lower}' appears in the following context: {context}"
                    entity_context_pairs.append((ent_lower, enriched.strip()))
                    matched = True
                    break
            if not matched:
                fallback = f"The concept '{ent_lower}' appears in the following context: {chunk}"
                entity_context_pairs.append((ent_lower, fallback.strip()))
    return entity_context_pairs


# === TOPIC SEARCHER CLASS (WITH DEDUPLICATION, NOISE FILTERING) ===
class AllergyTopicSearcher:
    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,
                 model_name="pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"):
        self.chunks = chunks
        self.entities_per_chunk = entities_per_chunk
        self.embedding_model = SentenceTransformer(model_name)

        self.umap_params = umap_params
        self.hdbscan_params = hdbscan_params

        self.topic_model = None
        self.topic_metadata = []
        self.topic_embeddings = None
        self.searcher = None

        self._prepare()

    def _prepare(self):
        entity_context_pairs = extract_entity_contexts(self.chunks, self.entities_per_chunk)
        contextual_texts = [ctx for _, ctx in entity_context_pairs]
        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=False)

        umap_model = UMAP(**self.umap_params, random_state=SEED)
        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)

        self.topic_model = BERTopic(
            embedding_model=self.embedding_model,
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            representation_model=KeyBERTInspired(),
            calculate_probabilities=True,
            verbose=False,
        )

        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)

        topic_to_contexts = defaultdict(list)
        topic_to_entities = defaultdict(set)
        topic_to_embeddings = defaultdict(list)

        for i, topic in enumerate(topics):
            if topic == -1:
                continue  # Skip noisy topics
            ent, ctx = entity_context_pairs[i]
            topic_to_contexts[topic].append(ctx)
            topic_to_entities[topic].add(ent)
            topic_to_embeddings[topic].append(contextual_embeddings[i])

        topic_embeddings = []
        topic_metadata = []

        for topic_id in topic_to_contexts:
            emb = topic_to_embeddings[topic_id]
            centroid = np.mean(emb, axis=0)
            centroid /= np.linalg.norm(centroid) + 1e-10
            topic_embeddings.append(centroid)
            topic_metadata.append({
                "topic_id": topic_id,
                "entities": list(topic_to_entities[topic_id]),
                "sentences": topic_to_contexts[topic_id],
                "sentence_embeddings": np.array(emb)
            })

        # === OPTIONAL: Merge semantically similar topics (cosine sim > 0.95)
        deduped_metadata = []
        used = set()

        for i, emb_i in enumerate(topic_embeddings):
            if i in used:
                continue
            group = [i]
            sim_scores = cosine_similarity([emb_i], topic_embeddings)[0]
            for j in range(i + 1, len(sim_scores)):
                if sim_scores[j] > 0.95:
                    group.append(j)
                    used.add(j)

            merged = {
                "topic_id": i,
                "sentences": [],
                "entities": [],
                "sentence_embeddings": []
            }
            for g in group:
                merged["sentences"] += topic_metadata[g]["sentences"]
                merged["entities"] += topic_metadata[g]["entities"]
                merged["sentence_embeddings"] += list(topic_metadata[g]["sentence_embeddings"])

            merged["sentence_embeddings"] = np.array(merged["sentence_embeddings"])
            merged["entities"] = list(set(merged["entities"]))
            deduped_metadata.append(merged)

        self.topic_metadata = deduped_metadata
        self.topic_embeddings = np.array([
            np.mean(m["sentence_embeddings"], axis=0) /
            (np.linalg.norm(np.mean(m["sentence_embeddings"], axis=0)) + 1e-10)
            for m in deduped_metadata
        ])

        self.searcher = (
            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, "dot_product")
            .tree(num_leaves=10, num_leaves_to_search=5, training_sample_size=len(self.topic_embeddings))
            .score_brute_force()
            .reorder(5)
            .build()
        )

    import re

    def search(self, query, top_k_topics=3, top_k_sents=3):
        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]
        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)

        results = []
        prefix_pattern = r"^the concept '.*?' appears in (the following )?context:\s*"

        for i, idx in enumerate(neighbors):
            meta = self.topic_metadata[idx]
            topic_score = float(scores[i])

            # Deduplicate sentences
            seen = set()
            cleaned_sentences = []
            cleaned_embeddings = []

            for sent, emb in zip(meta["sentences"], meta["sentence_embeddings"]):
            # Apply regex to remove beginning prefix
                cleaned = re.sub(prefix_pattern, "", sent, flags=re.IGNORECASE).strip()

        # No duplicates
                if cleaned not in seen:
                    seen.add(cleaned)
                    cleaned_sentences.append(cleaned)
                    cleaned_embeddings.append(emb)

            if not cleaned_sentences:
                continue

            emb_array = np.array(cleaned_embeddings)
            sims = np.dot(emb_array / np.linalg.norm(emb_array, axis=1, keepdims=True), query_emb)
            top_ids = sims.argsort()[::-1][:top_k_sents]

            top_sents = [(cleaned_sentences[j], float(sims[j])) for j in top_ids]
            results.append({
            "topic_id": meta["topic_id"],
            "topic_score": topic_score,
            "entities": meta["entities"],
            "sentences": top_sents,
            })

        return results




# === METRICS ===
def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):
    topics = [topic_model.get_topic(meta["topic_id"])[:topk] for meta in topic_metadata]
    word_lists = [[word for word, _ in topic] for topic in topics]

    texts = []
    for meta in topic_metadata:
        for s in meta["sentences"]:
            tokens = clean_text(s).split()
            texts.append(tokens)

    dictionary = Dictionary(texts)
    corpus = [dictionary.doc2bow(t) for t in texts]

    cm = CoherenceModel(
        topics=word_lists,
        texts=texts,
        dictionary=dictionary,
        coherence="c_v"
    )
    return cm.get_coherence()


def compute_topic_diversity(topic_model, topic_metadata, topk=10):
    topic_words = [topic_model.get_topic(meta["topic_id"])[:topk] for meta in topic_metadata]
    unique_words = set(word for topic in topic_words for word, _ in topic)
    return len(unique_words) / (len(topic_words) * topk)


def compute_silhouette_score_custom(topic_metadata):
    all_embeddings = []
    all_labels = []

    for meta in topic_metadata:
        emb = meta["sentence_embeddings"]
        if len(emb) < 2:  # skip small clusters
            continue
        all_embeddings.extend(emb)
        all_labels.extend([meta["topic_id"]] * len(emb))

    if len(all_embeddings) < 3:
        return None

    all_embeddings = np.vstack(all_embeddings)
    return silhouette_score(all_embeddings, all_labels, metric="cosine")


# === DATASET & INITIALIZATION ===
allergy_dataset = {
  "chunks": [
    "Mrs. M is a 68-year-old female with a complex medical history including hypertension, type 2 diabetes mellitus, and stage 3 chronic kidney disease.",
    "She presents to the internal medicine clinic with ongoing complaints of dull, persistent occipital headaches for the past six weeks.",
    "The headaches are associated with episodes of dizziness and intermittent blurred vision, particularly in the morning hours.",
    "She reports that these symptoms have gradually increased in frequency and intensity.",
    "At-home blood pressure monitoring consistently shows elevated readings above 160/95 mmHg.",
    "These elevated blood pressure values are most pronounced during early mornings.",
    "Her current antihypertensive regimen includes amlodipine 10 mg daily and hydrochlorothiazide 25 mg daily.",
    "Despite adherence, her blood pressure remains poorly controlled.",
    "She also has a history of hyperlipidemia managed with atorvastatin.",
    "Her most recent HbA1c is 7.8%, indicating suboptimal glycemic control.",
    "She suffers from osteoarthritis, predominantly affecting her knees and hips.",
    "Osteoarthritis significantly limits her mobility and daily activity.",
    "She averages fewer than 1,000 steps per day, as recorded by her wearable tracker.",
    "She has recently developed bilateral ankle edema, particularly in the evenings.",
    "Mild shortness of breath is present with moderate physical exertion.",
    "These symptoms raise concerns for evolving heart failure.",
    "Her clinic vitals reveal a blood pressure of 168/98 mmHg while seated.",
    "Heart rate is 88 bpm, respiratory rate 18, and oxygen saturation is 96% on room air.",
    "Temperature is within normal limits.",
    "Cardiovascular exam reveals a displaced apical impulse at the 6th intercostal space.",
    "A soft systolic murmur (grade 2/6) is heard over the cardiac apex.",
    "No jugular venous distension is appreciated.",
    "Pulmonary examination shows clear lung fields bilaterally.",
    "Mild 1+ pitting edema is noted at both ankles.",
    "Fundoscopic exam shows arteriolar narrowing and scattered cotton wool spots.",
    "There is no evidence of papilledema.",
    "Recent labs show stable serum creatinine at 1.4 mg/dL.",
    "Her eGFR is estimated at 48 mL/min/1.73 m².",
    "Electrolyte levels remain within normal limits.",
    "Her lipid panel reveals LDL cholesterol at 130 mg/dL.",
    "HDL is 38 mg/dL and triglycerides are measured at 160 mg/dL.",
    "A 12-lead ECG reveals left ventricular hypertrophy with strain pattern.",
    "There are no arrhythmias or conduction defects observed on ECG.",
    "Echocardiography demonstrates concentric LV hypertrophy with LVEF of 60%.",
    "Mild left atrial enlargement is present on echocardiogram.",
    "Trace mitral regurgitation is also identified.",
    "She expresses challenges adhering to a low-sodium diet.",
    "Meal preparation is difficult due to joint pain and fatigue.",
    "She frequently consumes processed or ready-made meals.",
    "Her husband assists with shopping but she manages meal planning.",
    "Despite previous dietary counseling, her weight has remained stable.",
    "She reports feeling frustrated with her lack of progress.",
    "Her hydrochlorothiazide dose was reduced due to borderline hypokalemia.",
    "Lisinopril 10 mg daily was added to improve BP control and renal protection.",
    "She was instructed to record BP twice daily and maintain a log.",
    "Additional labs were ordered to reassess renal function and electrolytes.",
    "A follow-up lipid panel was also requested.",
    "She was counseled on symptoms of hypertensive urgency.",
    "Educational materials were provided about recognizing chest pain, confusion, or sudden weakness.",
    "She was encouraged to gradually increase activity within tolerance.",
    "Physical therapy referral was made to support her with joint-friendly exercises.",
    "Dietitian consultation was arranged to address nutritional gaps.",
    "The diet plan focuses on kidney-friendly and low-sodium strategies.",
    "Home health services were initiated to aid with BP monitoring.",
    "Medication adherence support and lifestyle reinforcement are included in home visits.",
    "Cardiology follow-up is scheduled within 4 weeks.",
    "Nephrology will reassess renal function and medication tolerance.",
    "She lives with her husband in a single-story home.",
    "Her husband offers daily assistance and emotional support.",
    "She is the primary caregiver for her elderly mother-in-law with dementia.",
    "Caregiver duties contribute to emotional stress and fatigue.",
    "She reports occasional insomnia and feelings of anxiety.",
    "There is no history of tobacco or alcohol use.",
    "She denies symptoms of depression.",
    "Social work referral was made to explore caregiver resources.",
    "Multidisciplinary care was emphasized to support long-term BP management.",
    "She was advised to follow up within 6 weeks or sooner if symptoms worsen.",
    "She agreed to maintain a BP log and bring it to her next visit.",
    "Nutritional handouts and physical therapy instructions were given in print.",
    "She was educated on avoiding NSAIDs due to kidney function.",
    "She occasionally used over-the-counter pain relievers for joint discomfort.",
    "Acetaminophen was recommended instead of ibuprofen.",
    "Fluid status will be monitored to avoid volume overload.",
    "She was advised to elevate legs periodically throughout the day.",
    "Compression stockings were discussed for managing ankle edema.",
    "Psychosocial support and stress management were reviewed during the visit.",
    "Behavioral therapy referral was considered if insomnia persists.",
    "Cognitive function was grossly intact during the visit.",
    "There was no evidence of delirium or memory impairment.",
    "Speech, gait, and motor function appeared normal during physical exam.",
    "Her last eye exam was over a year ago; ophthalmology referral was placed.",
    "Diabetic foot screening was normal, with intact sensation and no ulceration.",
    "She is due for her pneumococcal and influenza vaccinations.",
    "Vaccination updates were administered during this visit.",
    "Her BMI is 31, placing her in the obese category.",
    "Weight reduction strategies were discussed, including dietary adjustments.",
    "She agreed to log meals and reduce sugary beverages.",
    "She was encouraged to attend group diabetes education sessions.",
    "A glucose meter was prescribed for home use.",
    "Target fasting blood glucose levels were reviewed with her.",
    "She demonstrated appropriate technique for fingerstick glucose testing.",
    "Pharmacist consultation was arranged to review medication regimen.",
    "Her medication list was reconciled and updated in the EHR.",
    "There are no current drug allergies reported.",
    "Her creatinine will be monitored every 3 months going forward.",
    "UACR (urine albumin-to-creatinine ratio) was ordered to assess proteinuria.",
    "Recent lab results were reviewed and explained in detail.",
    "Patient verbalized understanding of the treatment plan.",
    "Emergency contact information was verified and updated.",
    "Instructions were provided on when to seek urgent care.",
    "She was advised to avoid high-potassium foods temporarily.",
    "A potassium supplement was not needed at this time.",
    "She requested information on local support groups for caregivers.",
    "Social worker will follow up within 2 weeks.",
    "Progress notes and care plan summary were printed for her records.",
    "Next appointment was scheduled before she left the clinic.",
    "Clinic staff reviewed transportation resources if needed.",
    "She prefers morning appointments due to caregiver duties in the afternoon.",
    "Her husband accompanied her to the visit and asked questions about medications.",
    "Together, they expressed appreciation for coordinated care.",
    "She was reminded of the importance of medication timing consistency.",
    "BP monitor calibration was reviewed and verified as accurate.",
    "Telehealth follow-up was discussed as an option for future visits.",
    "Patient was comfortable with the use of video visits.",
    "Portal access instructions were provided to view lab results online.",
    "Patient's adherence barriers were reviewed in depth.",
    "She reported no issues obtaining her prescriptions.",
    "Medication copays are affordable with current insurance.",
    "She uses a pill organizer to stay consistent with medications.",
    "No adverse effects were noted with lisinopril initiation.",
    "She will follow up earlier if she experiences cough or dizziness.",
    "Lab orders were sent electronically to her preferred lab facility.",
    "Her primary care physician was updated with a detailed note.",
    "All referrals were placed and communicated via the EHR.",
    "She verbalized a commitment to improve her dietary habits.",
    "Caregiver support remains a critical concern in her daily life.",
    "A family meeting was suggested to discuss shared caregiving responsibilities.",
    "Advance directives were briefly discussed and documented in the chart.",
    "She has not completed a living will but expressed interest in doing so.",
    "Goals of care conversation was scheduled for her next visit.",
    "The care team concluded the visit with a review of next steps.",
    "Mrs. M was thanked for her active participation and engagement in her care.",
    "Patient has peanut allergy causing hives and swelling. Anaphylaxis noted once during a reaction.",
    "Allergic rhinitis, or hay fever, results from exposure to pollen, dust, or pet dander.",
    "Severe anaphylaxis symptoms require immediate treatment with epinephrine.",
    "Food allergies to milk and eggs can cause skin reactions like urticaria and eczema.",
    "Cold weather does not cause allergy symptoms in this patient."
  ]

,
"entities":[
    [ "Mrs. M", "68-year-old female", "hypertension", "type 2 diabetes mellitus", "stage 3 chronic kidney disease" ],
  [ "internal medicine clinic", "dull, persistent occipital headaches", "past six weeks" ],
  [ "headaches", "dizziness", "intermittent blurred vision", "morning hours" ],
  [ "symptoms" ],
  [ "At-home blood pressure monitoring", "elevated readings", "160/95 mmHg" ],
  [ "elevated blood pressure values", "early mornings" ],
  [ "antihypertensive regimen", "amlodipine", "10 mg daily", "hydrochlorothiazide", "25 mg daily" ],
  [ "blood pressure" ],
  [ "hyperlipidemia", "atorvastatin" ],
  [ "HbA1c", "7.8%", "suboptimal glycemic control" ],
  [ "osteoarthritis", "knees", "hips" ],
  [ "Osteoarthritis", "mobility", "daily activity" ],
  [ "1,000 steps per day", "wearable tracker" ],
  [ "bilateral ankle edema", "evenings" ],
  [ "Mild shortness of breath", "moderate physical exertion" ],
  [ "symptoms", "heart failure" ],
  [ "clinic vitals", "blood pressure", "168/98 mmHg" ],
  [ "Heart rate", "88 bpm", "respiratory rate", "18", "oxygen saturation", "96%", "room air" ],
  [ "Temperature" ],
  [ "Cardiovascular exam", "displaced apical impulse", "6th intercostal space" ],
  [ "soft systolic murmur (grade 2/6)", "cardiac apex" ],
  [ "jugular venous distension" ],
  [ "Pulmonary examination", "clear lung fields" ],
  [ "Mild 1+ pitting edema", "ankles" ],
  [ "Fundoscopic exam", "arteriolar narrowing", "scattered cotton wool spots" ],
  [ "papilledema" ],
  [ "labs", "serum creatinine", "1.4 mg/dL" ],
  [ "eGFR", "48 mL/min/1.73 m²" ],
  [ "Electrolyte levels" ],
  [ "lipid panel", "LDL cholesterol", "130 mg/dL" ],
  [ "HDL", "38 mg/dL", "triglycerides", "160 mg/dL" ],
  [ "12-lead ECG", "left ventricular hypertrophy with strain pattern" ],
  [ "arrhythmias", "conduction defects", "ECG" ],
  [ "Echocardiography", "concentric LV hypertrophy", "LVEF", "60%" ],
  [ "Mild left atrial enlargement", "echocardiogram" ],
  [ "Trace mitral regurgitation" ],
  [ "low-sodium diet" ],
  [ "Meal preparation", "joint pain", "fatigue" ],
  [ "processed or ready-made meals" ],
  [ "husband", "shopping", "meal planning" ],
  [ "dietary counseling", "weight" ],
  [ "frustrated" ],
  [ "hydrochlorothiazide", "borderline hypokalemia" ],
  [ "Lisinopril", "10 mg daily", "BP control", "renal protection" ],
  [ "BP", "twice daily", "log" ],
  [ "labs", "renal function", "electrolytes" ],
  [ "lipid panel" ],
  [ "symptoms", "hypertensive urgency" ],
  [ "chest pain", "confusion", "sudden weakness" ],
  [ "activity" ],
  [ "Physical therapy", "joint-friendly exercises" ],
  [ "Dietitian", "consultation", "nutritional gaps" ],
  [ "diet plan", "kidney-friendly", "low-sodium" ],
  [ "Home health services", "BP monitoring" ],
  [ "Medication adherence support", "lifestyle reinforcement", "home visits" ],
  [ "Cardiology", "follow-up", "4 weeks" ],
  [ "Nephrology", "renal function", "medication tolerance" ],
  [ "husband", "single-story home" ],
  [ "husband", "daily assistance", "emotional support" ],
  [ "primary caregiver", "elderly mother-in-law", "dementia" ],
  [ "Caregiver duties", "emotional stress", "fatigue" ],
  [ "insomnia", "anxiety" ],
  [ "tobacco", "alcohol" ],
  [ "symptoms of depression" ],
  [ "Social work", "caregiver resources" ],
  [ "Multidisciplinary care", "long-term BP management" ],
  [ "follow up", "6 weeks", "symptoms" ],
  [ "BP log", "next visit" ],
  [ "Nutritional handouts", "physical therapy instructions" ],
  [ "NSAIDs", "kidney function" ],
  [ "over-the-counter pain relievers", "joint discomfort" ],
  [ "Acetaminophen", "ibuprofen" ],
  [ "Fluid status", "volume overload" ],
  [ "legs" ],
  [ "Compression stockings", "ankle edema" ],
  [ "Psychosocial support", "stress management", "visit" ],
  [ "Behavioral therapy", "insomnia" ],
  [ "Cognitive function", "visit" ],
  [ "delirium", "memory impairment" ],
  [ "Speech", "gait", "motor function", "physical exam" ],
  [ "eye exam", "over a year ago", "ophthalmology" ],
  [ "Diabetic foot screening", "intact sensation", "ulceration" ],
  [ "pneumococcal", "influenza vaccinations" ],
  [ "Vaccination", "visit" ],
  [ "BMI", "31", "obese category" ],
  [ "Weight reduction strategies", "dietary adjustments" ],
  [ "log meals", "sugary beverages" ],
  [ "group diabetes education sessions" ],
  [ "glucose meter" ],
  [ "Target fasting blood glucose levels" ],
  [ "fingerstick glucose testing" ],
  [ "Pharmacist", "consultation", "medication regimen" ],
  [ "medication list", "EHR" ],
  [ "drug allergies" ],
  [ "creatinine", "every 3 months" ],
  [ "UACR (urine albumin-to-creatinine ratio)", "proteinuria" ],
  [ "lab results" ],
  [ "Patient", "treatment plan" ],
  [ "Emergency contact information" ],
  [ "urgent care" ],
  [ "high-potassium foods", "temporarily" ],
  [ "potassium supplement" ],
  [ "local support groups for caregivers" ],
  [ "Social worker", "2 weeks" ],
  [ "Progress notes", "care plan summary" ],
  [ "Next appointment", "clinic" ],
  [ "Clinic staff", "transportation resources" ],
  [ "morning appointments", "caregiver duties", "afternoon" ],
  [ "husband", "visit", "medications" ],
  [ "coordinated care" ],
  [ "medication timing consistency" ],
  [ "BP monitor", "calibration" ],
  [ "Telehealth follow-up", "future visits" ],
  [ "Patient", "video visits" ],
  [ "Portal", "lab results" ],
  [ "Patient's adherence barriers" ],
  [ "prescriptions" ],
  [ "Medication copays", "current insurance" ],
  [ "pill organizer", "medications" ],
  [ "adverse effects", "lisinopril" ],
  [ "cough", "dizziness" ],
  [ "Lab orders", "lab facility" ],
  [ "primary care physician", "note" ],
  [ "referrals", "EHR" ],
  [ "dietary habits" ],
  [ "Caregiver support" ],
  [ "family meeting", "shared caregiving responsibilities" ],
  [ "Advance directives", "chart" ],
  [ "living will" ],
  [ "Goals of care conversation", "next visit" ],
  [ "care team", "visit", "next steps" ],
  [ "Mrs. M", "care" ],
  ["peanut allergy", "hives", "swelling", "anaphylaxis"],
  ["allergic rhinitis", "hay fever", "pollen", "dust", "pet dander"],
  ["anaphylaxis", "epinephrine", "treatment"],
  ["food allergies", "milk", "eggs", "urticaria", "eczema"],
  ["cold weather", "allergy symptoms"]
]

}

best_umap = {"n_neighbors": 10, "n_components": 3, "min_dist":0.1,  "metric": "cosine"}
best_hdbscan = {"min_cluster_size": 2, "min_samples": 1, "metric": "euclidean"}

searcher = AllergyTopicSearcher(
    chunks=allergy_dataset["chunks"],
    entities_per_chunk=allergy_dataset["entities"],
    umap_params=best_umap,
    hdbscan_params=best_hdbscan,
    model_name="pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"
)
print(" Model ready for querying.")
print("\n===  Generated Topics and Entities ===")
for meta in searcher.topic_metadata:
    topic_id = meta["topic_id"]
    entities = ", ".join(meta["entities"])
    print(f" Topic ID: {topic_id} — Entities: {entities}")

# === METRICS ===
coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)
diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)
sil_score = compute_silhouette_score_custom(searcher.topic_metadata)

print("\n=== Topic Quality Metrics ===")
print(f" Coherence Score (c_v): {coherence:.4f}")
print(f" Topic Diversity: {diversity:.4f}")
if sil_score is not None:
    print(f" Silhouette Score: {sil_score:.4f}")
else:
    print(" Silhouette Score: Not applicable.")

import numpy as np
import pandas as pd
from google.cloud import bigquery
from sklearn.metrics.pairwise import cosine_similarity


def normalize_vectors(vectors: np.ndarray) -> np.ndarray:

    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    # Avoid division by zero by replacing zeros with small epsilon
    norms = np.where(norms == 0, 1e-10, norms)
    return vectors / norms


# === 1. Load CUI definitions and embeddings from BigQuery ===
project_id = "aif-usr-p-ep-cg-wcdm-0767"
dataset_id = "aif_wcdm_osm_pipeline"
table_id = "CUI_EMBEDDINGS_GEMINI_EMBEDDING_001"
table_ref = f"`{project_id}.{dataset_id}.{table_id}`"

client = bigquery.Client(project=project_id)
query = f"SELECT cui, definition, embedding FROM {table_ref} limit 1000"
df_cui = client.query(query).to_dataframe()

# Stack all embeddings into np.array of shape (num_concepts, embedding_dim)
cui_embeddings = np.vstack(df_cui['embedding'].values).astype(float)

# Normalize CUI embeddings here explicitly
cui_embeddings = normalize_vectors(cui_embeddings)

cui_defs = df_cui['definition'].tolist()

top_k = 3  # Number of top candidate labels for each topic

rows = []

# === 2. LOOP OVER TOPICS from your AllergyTopicSearcher instance ===
# Assumes searcher.topic_metadata elements with keys:
# 'topic_id', 'entities', 'sentence_embeddings' (np.array, shape=(num_entities, embedding_dim))
for meta in searcher.topic_metadata:
    topic_id = meta["topic_id"]
    entities = meta["entities"]

    # Normalize all sentence embeddings before averaging
    sentence_embeddings_normalized = normalize_vectors(meta["sentence_embeddings"])

    # Compute average topic embedding and normalize again
    topic_emb = np.mean(sentence_embeddings_normalized, axis=0)
    topic_emb /= (np.linalg.norm(topic_emb) + 1e-10)

    # Compute cosine similarity: topic embedding vs all CUI embeddings (dot product since normalized)
    topic_term_sims = np.dot(cui_embeddings, topic_emb)

    # Clip similarity scores to [-1, 1]
    topic_term_sims = np.clip(topic_term_sims, -1.0, 1.0)

    # Select top-k candidate label indices sorted by similarity descending
    top_indices = topic_term_sims.argsort()[::-1][:top_k]

    candidate_embeddings = cui_embeddings[top_indices]
    candidate_defs = [cui_defs[i] for i in top_indices]
    candidate_similarities = topic_term_sims[top_indices]

    # Compute pairwise cosine similarity among top-k candidate labels
    pairwise_sims = cosine_similarity(candidate_embeddings)
    off_diag_mask = ~np.eye(pairwise_sims.shape[0], dtype=bool)
    off_diagonal_sims = pairwise_sims[off_diag_mask]
    topic_label_similarity = round(float(np.mean(off_diagonal_sims)), 2) if off_diagonal_sims.size > 0 else None

    # Assign each entity to the closest candidate label based on similarity
    entity_assignments = []
    entity_scores = []

    if entities and len(meta["sentence_embeddings"]) == len(entities):
        # Normalize entity embeddings as well
        entity_embeddings_normalized = normalize_vectors(meta["sentence_embeddings"])

        # Compute similarity matrix (entities x candidate labels)
        sims_entities_candidates = np.dot(entity_embeddings_normalized, candidate_embeddings.T)

        # Clip similarity scores
        sims_entities_candidates = np.clip(sims_entities_candidates, -1.0, 1.0)

        for i, ent in enumerate(entities):
            best_idx = sims_entities_candidates[i].argmax()
            best_score = float(sims_entities_candidates[i][best_idx])
            assigned_label = candidate_defs[best_idx]
            entity_assignments.append({
                "entity": ent,
                "assigned_label_definition": assigned_label,
                "sim_score": round(best_score, 2)
            })
            entity_scores.append(best_score)

        entity_label_score = round(float(np.mean(entity_scores)), 2) if entity_scores else None
    else:
        entity_label_score = None
        entity_assignments = []

    # Prepare topic-level label list with definition and similarity
    topic_level_labels = [(candidate_defs[i], round(float(candidate_similarities[i]), 2)) for i in range(len(top_indices))]

    rows.append({
        "topic_id": topic_id,
        "entities": entities,
        "topic_level_labels": topic_level_labels,  # list of (definition, similarity)
        "topic_Avg_label_similarity": topic_label_similarity,
        "entity_level_assignments": entity_assignments,  # list of dicts {entity, assigned_label_definition, sim_score}
        "entity_Avg_label_score": entity_label_score
    })

# === 3. Create pandas DataFrame and display ===
df = pd.DataFrame(rows)
pd.set_option('display.max_colwidth', None)

df

import numpy as np
import plotly.graph_objects as go
import umap

# Assuming you have these from your previous steps
topics = searcher.topic_metadata[:3]

all_entities = []
all_assigned_labels = []
all_topic_ids = []
all_embeddings = []

cui_to_label = {cui: label for cui, label in cui_term_list}

for topic in topics:
    topic_id = topic["topic_id"]
    entities = topic["entities"]
    row = df2[df2["topic_id"] == topic_id].iloc[0]
    entity_assignments = row["entity_level_assignments"]
    embeddings = topic["sentence_embeddings"]

    all_entities.extend([ea["entity"] for ea in entity_assignments])
    all_assigned_labels.extend([ea["assigned__cui"] for ea in entity_assignments])
    all_topic_ids.extend([topic_id] * len(entity_assignments))
    all_embeddings.extend(embeddings[:len(entity_assignments)])

all_embeddings = np.array(all_embeddings)

reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric="cosine", random_state=42)
embedding_2d = reducer.fit_transform(all_embeddings)

def get_short_label(full_label):
    return full_label.split(':')[0] if ':' in full_label else full_label

topic_ids_unique = list(set(all_topic_ids))
colors = ["red", "blue", "green", "orange"]
topic_color_map = {tid: colors[i % len(colors)] for i, tid in enumerate(topic_ids_unique)}

fig = go.Figure()

topic_centers = {}

for topic_id in topic_ids_unique:
    idxs = [i for i, t in enumerate(all_topic_ids) if t == topic_id]
    x = embedding_2d[idxs, 0]
    y = embedding_2d[idxs, 1]
    color = topic_color_map[topic_id]

    text_labels = [all_entities[i] for i in idxs]

    # Scatter plot for entities with names displayed
    fig.add_trace(go.Scatter(
        x=x, y=y,
        mode='markers+text',
        marker=dict(color=color, size=10, opacity=0.8),
        name=f"Topic {topic_id}",
        text=text_labels,
        textposition="top center",
        textfont=dict(size=10, color=color),
        hoverinfo='skip'
    ))

    # Circle cluster around entities
    center = np.mean(np.column_stack((x, y)), axis=0)
    topic_centers[topic_id] = center
    radius = np.max(np.linalg.norm(np.column_stack((x, y)) - center, axis=1)) + 0.3

    theta = np.linspace(0, 2 * np.pi, 100)
    circle_x = center[0] + radius * np.cos(theta)
    circle_y = center[1] + radius * np.sin(theta)

    fig.add_trace(go.Scatter(
        x=circle_x, y=circle_y,
        mode='lines',
        line=dict(color=color, width=2, dash='dash'),
        showlegend=False,
        hoverinfo='skip'
    ))

# Now add the label legends next to each topic cluster
label_x_offset = 0.7  # horizontal distance right from circle center

for topic_id in topic_ids_unique:
    center = topic_centers[topic_id]
    row = df2[df2["topic_id"] == topic_id].iloc[0]
    topic_labels = row["topic_level_labels"]  # list of (CUI, score)
    color = topic_color_map[topic_id]

    # Title label for this topic's labels
    fig.add_annotation(
        x=center[0] + label_x_offset,
        y=center[1] + 0.2,
        text=f"<b>Topic {topic_id} Labels:</b>",
        showarrow=False,
        font=dict(color=color, size=12),
        xanchor='left',
        yanchor='bottom'
    )

    # Individual labels listed vertically below the title
    for i, (cui, _) in enumerate(topic_labels):
        short_label = get_short_label(cui_to_label.get(cui, cui))
        fig.add_annotation(
            x=center[0] + label_x_offset,
            y=center[1] + 0.2 - (i + 1) * 0.06,
            text=f"{short_label} ({cui})",
            showarrow=False,
            font=dict(color=color, size=10),
            xanchor='left',
            yanchor='top'
        )

fig.update_layout(
    title="UMAP Visualization of Topics with Entities and Assigned Labels",
    xaxis_title="UMAP Dimension 1",
    yaxis_title="UMAP Dimension 2",
    legend_title="Topics",
    width=1000,
    height=700,
    template='plotly_white',
    margin=dict(r=150)
)

fig.show()
