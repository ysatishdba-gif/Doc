"""
CUI Reduction System
====================
Extract → Enrich → Filter → Hierarchy → Embed → Reduce → Topics

All thresholds derived from data. Zero hardcoded values.
Works for any input text at any scale.

pip install google-cloud-bigquery networkx numpy scipy requests scann
"""

# %% [1] Imports
import re
import time
import logging
import threading
import unicodedata
import subprocess
from typing import List, Dict, Optional, Set, Tuple, FrozenSet
from dataclasses import dataclass
from collections import defaultdict, deque, OrderedDict

import numpy as np
import networkx as nx
import requests
import scann
from google.cloud import bigquery
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform


# %% [2] Logging and Timing
logger = logging.getLogger("cui_reduction")
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s %(message)s", datefmt="%H:%M:%S"))
    logger.addHandler(_handler)


def log(msg: str, level: str = "INFO"):
    getattr(logger, level.lower(), logger.info)(msg)


_timings: Dict[str, list] = defaultdict(list)
_timing_lock = threading.Lock()


def timed(name):
    def dec(fn):
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                with _timing_lock:
                    _timings[name].append(
                        (time.perf_counter() - t0) * 1000)
        return wrapper
    return dec


# %% [3] Data Models
@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    normalized_term: str
    term_tokens: FrozenSet[str]
    semantic_types: List[str]
    semantic_type_ids: List[str]
    ic_score: float
    source_vocabs: List[str]


@dataclass
class TopicInfo:
    topic_id: str
    semantic_types: Set[str]
    cuis: List[str]
    cui_terms: Dict[str, str]


@dataclass
class AuditEntry:
    removed_cui: str
    removed_term: str
    kept_cui: str
    kept_term: str
    reason: str
    detail: str


@dataclass
class ReductionResult:
    context_string: str
    input_count: int
    after_filter_count: int
    after_hierarchy_count: int
    after_redundancy_count: int
    topics: Dict[str, TopicInfo]
    all_reduced_cuis: List[str]
    audit_trail: List[AuditEntry]
    processing_time_ms: float
    metadata_coverage: float
    embedding_coverage: float


# %% [4] Text Utilities
def normalize_term(term: str) -> str:
    t = unicodedata.normalize("NFKD", term).lower()
    t = re.sub(r"[^a-z0-9\s]", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def tokenize_term(normalized: str) -> FrozenSet[str]:
    if not normalized:
        return frozenset()
    return frozenset(normalized.split())


def terms_diverge(tokens_a: FrozenSet[str],
                  tokens_b: FrozenSet[str]) -> bool:
    """Structural divergence: both sides contribute unique tokens.

    "left knee pain" vs "right knee pain"
      unique_a={left}, unique_b={right} → DIVERGE
    "knee pain" vs "pain in knee"
      unique_a={}, unique_b={in} → allow
    """
    if not tokens_a or not tokens_b:
        return False
    return bool((tokens_a - tokens_b) and (tokens_b - tokens_a))


# %% [5] Cache (LRU, thread-safe)
class Cache:
    def __init__(self, max_size: int):
        self._max = max_size
        self._d: OrderedDict = OrderedDict()
        self._lock = threading.Lock()

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
        return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# %% [6] Metadata from BigQuery
_BQ_BATCH = 5000


class MetadataFetcher:
    """Fetches CUI metadata from UMLS tables in BigQuery.
    Preferred term fallback: ISPREF → longest STR → any STR."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str):
        self.bq = bq
        self.pid = pid
        self.did = did
        self._cache = Cache(50_000)

    @timed("metadata_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, CUIMetadata]:
        result = {}
        missing = []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH):
            batch = missing[i:i + _BQ_BATCH]
            query = f"""
            WITH ranked AS (
              SELECT CUI, STR, ISPREF,
                ROW_NUMBER() OVER (
                  PARTITION BY CUI
                  ORDER BY
                    CASE WHEN ISPREF='Y' THEN 0 ELSE 1 END,
                    LENGTH(STR) DESC
                ) AS rn
              FROM `{self.pid}.{self.did}.MRCONSO`
              WHERE CUI IN UNNEST(@cuis) AND LAT = 'ENG'
            )
            SELECT
              r.CUI AS cui,
              MAX(CASE WHEN r.rn = 1 THEN r.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT s.STY IGNORE NULLS) AS stys,
              ARRAY_AGG(DISTINCT c2.SAB IGNORE NULLS) AS sabs
            FROM ranked r
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s
              ON r.CUI = s.CUI
            LEFT JOIN `{self.pid}.{self.did}.MRCONSO` c2
              ON r.CUI = c2.CUI AND c2.LAT = 'ENG'
            WHERE r.rn = 1
            GROUP BY r.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            for row in self.bq.query(
                    query, job_config=jc, timeout=300).result():
                term = row.pref_term or row.cui
                norm = normalize_term(term)
                meta = CUIMetadata(
                    cui=row.cui,
                    preferred_term=term,
                    normalized_term=norm,
                    term_tokens=tokenize_term(norm),
                    semantic_types=list(row.stys or []),
                    semantic_type_ids=list(row.tuis or []),
                    ic_score=0.0,
                    source_vocabs=list(row.sabs or []),
                )
                self._cache.put(row.cui, meta)
                result[row.cui] = meta

        return result


# %% [7] IC from Hierarchy Graph
class ICComputer:
    """IC(c) = -log((descendants(c) + 1) / total_nodes).
    Pass precomputed dict for production speed."""

    def __init__(self, graph: nx.DiGraph,
                 precomputed: Optional[Dict[str, float]] = None):
        self.graph = graph
        self.precomputed = precomputed or {}
        self._cache = Cache(100_000)
        self._total = max(graph.number_of_nodes(), 1)

    @timed("ic_compute")
    def compute(self, cuis: List[str],
                metadata: Dict[str, CUIMetadata]) -> Dict[str, float]:
        result = {}
        for cui in cuis:
            if cui in self.precomputed:
                ic = self.precomputed[cui]
            else:
                cached = self._cache.get(cui)
                if cached is not None:
                    ic = cached
                else:
                    desc = self._descendants(cui)
                    ic = -np.log((desc + 1) / self._total)
                    self._cache.put(cui, ic)
            result[cui] = ic
            m = metadata.get(cui)
            if m:
                m.ic_score = ic
        return result

    def _descendants(self, cui: str) -> int:
        if not self.graph.has_node(cui):
            return 0
        visited = set()
        q = deque([cui])
        while q:
            n = q.popleft()
            for child in self.graph.successors(n):
                if child not in visited:
                    visited.add(child)
                    q.append(child)
        return len(visited)


# %% [8] Filters
class SABFilter:
    """Keep CUIs from allowed vocabularies."""

    def __init__(self, allowed: Set[str]):
        self.allowed = allowed

    @timed("sab_filter")
    def run(self, cuis: List[str],
            metadata: Dict[str, CUIMetadata]
            ) -> Tuple[List[str], List[AuditEntry]]:
        kept, audit = [], []
        for cui in cuis:
            m = metadata.get(cui)
            if not m:
                audit.append(AuditEntry(
                    cui, "", "", "", "no_metadata",
                    "CUI not found in BQ"))
                continue
            if not any(s in self.allowed for s in m.source_vocabs):
                audit.append(AuditEntry(
                    cui, m.preferred_term, "", "",
                    "sab_filtered",
                    f"vocabs {m.source_vocabs} not in allowed"))
                continue
            kept.append(cui)
        return kept, audit


class SemanticTypeFilter:
    """Remove CUIs whose types are all excluded."""

    def __init__(self, excluded: Set[str]):
        self.excluded = excluded

    @timed("semtype_filter")
    def run(self, cuis: List[str],
            metadata: Dict[str, CUIMetadata]
            ) -> Tuple[List[str], List[AuditEntry]]:
        if not self.excluded:
            return cuis, []
        kept, audit = [], []
        for cui in cuis:
            m = metadata.get(cui)
            if not m or not m.semantic_types:
                kept.append(cui)
                continue
            if all(st in self.excluded for st in m.semantic_types):
                audit.append(AuditEntry(
                    cui, m.preferred_term, "", "",
                    "semtype_excluded",
                    f"all types {m.semantic_types} excluded"))
                continue
            kept.append(cui)
        return kept, audit


class TermDeduplicator:
    """Collapse identical normalized terms, keep highest IC."""

    @staticmethod
    @timed("term_dedup")
    def run(cuis: List[str],
            metadata: Dict[str, CUIMetadata],
            ic_map: Dict[str, float]
            ) -> Tuple[List[str], List[AuditEntry]]:
        groups: Dict[str, List[str]] = defaultdict(list)
        for cui in cuis:
            m = metadata.get(cui)
            key = m.normalized_term if m else cui
            groups[key].append(cui)

        kept, audit = [], []
        for term, members in groups.items():
            best = max(members, key=lambda c: ic_map.get(c, 0))
            kept.append(best)
            for c in members:
                if c != best:
                    bm = metadata.get(best)
                    cm = metadata.get(c)
                    audit.append(AuditEntry(
                        c, cm.preferred_term if cm else "",
                        best, bm.preferred_term if bm else "",
                        "term_dedup",
                        f"identical norm term '{term}'"))
        return kept, audit


# %% [9] Hierarchy Reduction
class HierarchyReducer:
    """Remove ancestors where child IC >= parent IC
    and they share at least one semantic type."""

    def __init__(self, graph: nx.DiGraph):
        self.graph = graph

    @timed("hierarchy_reduce")
    def run(self, cuis: List[str],
            ic_map: Dict[str, float],
            metadata: Dict[str, CUIMetadata],
            ) -> Tuple[List[str], List[AuditEntry],
                       Set[Tuple[str, str]]]:

        cui_set = set(cuis)
        edges: Set[Tuple[str, str]] = set()
        parent_children: Dict[str, Set[str]] = defaultdict(set)

        for cui in cuis:
            if not self.graph.has_node(cui):
                continue
            for parent in self.graph.predecessors(cui):
                if parent in cui_set and parent != cui:
                    parent_children[parent].add(cui)
                    edges.add((parent, cui))

        to_remove: Dict[str, Tuple[str, float, float]] = {}
        for parent, children in parent_children.items():
            p_ic = ic_map.get(parent, 0)
            p_meta = metadata.get(parent)
            p_types = set(p_meta.semantic_types) if p_meta and p_meta.semantic_types else set()

            for child in children:
                c_ic = ic_map.get(child, 0)
                if c_ic < p_ic:
                    continue
                c_meta = metadata.get(child)
                c_types = set(c_meta.semantic_types) if c_meta and c_meta.semantic_types else set()
                if p_types and c_types and not (p_types & c_types):
                    continue
                if parent not in to_remove or c_ic > to_remove[parent][1]:
                    to_remove[parent] = (child, c_ic, p_ic)

        remove_set = set(to_remove.keys())
        surviving = [c for c in cuis if c not in remove_set]

        audit = []
        for parent, (child, c_ic, p_ic) in to_remove.items():
            pm = metadata.get(parent)
            cm = metadata.get(child)
            audit.append(AuditEntry(
                parent, pm.preferred_term if pm else "",
                child, cm.preferred_term if cm else "",
                "ancestor_subsumed",
                f"child IC {c_ic:.2f} >= parent IC {p_ic:.2f}"))

        return surviving, audit, edges


# %% [10] Embedding Fetch from BigQuery
class EmbeddingFetcher:
    """Fetches pre-computed CUI embeddings from BQ."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 embedding_table: str):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.table = embedding_table
        self._cache = Cache(50_000)

    @timed("embedding_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, np.ndarray]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH):
            batch = missing[i:i + _BQ_BATCH]
            query = f"""
            SELECT cui, embedding
            FROM `{self.pid}.{self.did}.{self.table}`
            WHERE cui IN UNNEST(@cuis)
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            for row in self.bq.query(
                    query, job_config=jc, timeout=300).result():
                raw = row.embedding
                if raw is None or len(raw) == 0:
                    continue
                vec = np.array(raw, dtype=np.float32)
                if not np.all(np.isfinite(vec)):
                    continue
                norm = np.linalg.norm(vec)
                if norm == 0:
                    continue
                vec = vec / norm
                self._cache.put(row.cui, vec)
                result[row.cui] = vec

        return result


# %% [11] ScaNN Topic Builder
class TopicBuilder:
    """Builds topic groups using ScaNN KNN graph.
    O(n log n). Threshold derived from KNN distance distribution."""

    @staticmethod
    @timed("scann_topics")
    def build(cuis: List[str],
              embeddings: Dict[str, np.ndarray],
              k_neighbors: int = 20,
              ) -> Dict[int, List[str]]:

        matrix = np.array([embeddings[c] for c in cuis],
                          dtype=np.float32)
        n = len(cuis)
        dim = matrix.shape[1] if matrix.ndim == 2 else 0

        # Guard: need at least 2 points for KNN
        if n < 2 or dim == 0:
            return {i: [c] for i, c in enumerate(cuis)}

        k = min(k_neighbors, n - 1)
        k = max(k, 1)

        # AH quantization needs:
        #   - >= 16 points to train codebook
        #   - embedding dim >= dimensions_per_block (2)
        _AH_MIN = 16
        num_leaves = max(2, int(np.sqrt(n)))
        num_leaves = min(num_leaves, n)
        builder = scann.scann_ops_pybind.builder(
            matrix, k, "dot_product")

        if n >= _AH_MIN and dim >= 2:
            dims_per_block = 2 if dim % 2 == 0 else 1
            builder = (
                builder
                .tree(
                    num_leaves=num_leaves,
                    num_leaves_to_search=max(1, num_leaves // 5),
                    training_sample_size=min(n, 250_000))
                .score_ah(dims_per_block,
                          anisotropic_quantization_threshold=0.2)
                .reorder(min(k * 4, n))
            )
        else:
            builder = builder.score_brute_force()

        searcher = builder.build()

        neighbors, scores = searcher.search_batched(matrix)

        # Threshold from data: mean of each point's farthest neighbor distance
        kth_dists = []
        for i in range(n):
            valid = scores[i][scores[i] > -1e30]
            if len(valid) > 0:
                kth_dists.append(1.0 - float(valid[-1]))

        if kth_dists:
            threshold = float(np.mean(kth_dists))
        else:
            # All scores invalid — compute from raw dot products
            all_sims = (matrix @ matrix.T)
            np.fill_diagonal(all_sims, -np.inf)
            threshold = float(1.0 - np.mean(all_sims[all_sims > -np.inf]))

        # Connected components from KNN adjacency
        adj: Dict[int, Set[int]] = defaultdict(set)
        for i in range(n):
            for j_pos in range(len(neighbors[i])):
                j = int(neighbors[i][j_pos])
                if j < 0 or j >= n or j == i:
                    continue
                if (1.0 - float(scores[i][j_pos])) < threshold:
                    adj[i].add(j)
                    adj[j].add(i)

        visited: Set[int] = set()
        groups: Dict[int, List[str]] = {}
        label = 0
        for i in range(n):
            if i in visited:
                continue
            component = []
            q = deque([i])
            visited.add(i)
            while q:
                node = q.popleft()
                component.append(cuis[node])
                for nb in adj.get(node, set()):
                    if nb not in visited:
                        visited.add(nb)
                        q.append(nb)
            groups[label] = component
            label += 1

        log(f"  ScaNN: {n} CUIs → {label} topics "
            f"(k={k}, threshold={threshold:.3f})")
        return groups


# %% [12] Embedding Reduction
class EmbeddingReducer:
    """Group by semantic type → ScaNN topics → tight redundancy
    → safety checks → keep highest IC.

    Safety checks before collapse:
      (a) semantic type overlap required
      (b) parent-child hierarchy blocks collapse
      (c) term token divergence blocks collapse
    """

    @staticmethod
    @timed("embedding_reduce")
    def run(cuis: List[str],
            metadata: Dict[str, CUIMetadata],
            embeddings: Dict[str, np.ndarray],
            ic_map: Dict[str, float],
            hierarchy_edges: Set[Tuple[str, str]],
            ) -> Tuple[Dict[str, List[str]], List[str], List[AuditEntry]]:

        # Group by ALL semantic types (multi-type assignment)
        type_groups: Dict[str, List[str]] = defaultdict(list)
        for cui in cuis:
            meta = metadata.get(cui)
            if meta and meta.semantic_types:
                for sty in meta.semantic_types:
                    type_groups[sty].append(cui)
            else:
                type_groups["Unknown"].append(cui)

        all_topics: Dict[str, List[str]] = {}
        globally_surviving: Set[str] = set()
        removal_candidates: Dict[str, AuditEntry] = {}
        tid = 0

        for sty in sorted(type_groups):
            group_cuis = sorted(set(type_groups[sty]))
            with_emb = [c for c in group_cuis if c in embeddings]
            without_emb = [c for c in group_cuis if c not in embeddings]

            for c in without_emb:
                globally_surviving.add(c)

            if len(with_emb) <= 1:
                all_topics[f"topic_{tid}"] = group_cuis
                for c in group_cuis:
                    globally_surviving.add(c)
                tid += 1
                continue

            # ScaNN topic clustering
            scann_topics = TopicBuilder.build(with_emb, embeddings)

            for t_cuis in scann_topics.values():
                topic_id = f"topic_{tid}"
                tid += 1

                if len(t_cuis) <= 1:
                    all_topics[topic_id] = t_cuis
                    for c in t_cuis:
                        globally_surviving.add(c)
                    continue

                # Within-topic: dense redundancy (topics are small)
                surviving, removed = _reduce_topic(
                    t_cuis, embeddings, metadata,
                    ic_map, hierarchy_edges)
                all_topics[topic_id] = surviving
                for c in surviving:
                    globally_surviving.add(c)
                for entry in removed:
                    removal_candidates[entry.removed_cui] = entry

            for cui in without_emb:
                all_topics[f"topic_{tid}"] = [cui]
                tid += 1

        # Deduplicate across type groups
        final_audit = []
        for cui, entry in removal_candidates.items():
            if cui not in globally_surviving:
                final_audit.append(entry)

        seen: Set[str] = set()
        surviving: List[str] = []
        for t_cuis in all_topics.values():
            for c in t_cuis:
                if c in globally_surviving and c not in seen:
                    seen.add(c)
                    surviving.append(c)

        cleaned: Dict[str, List[str]] = {}
        for topic_id, t_cuis in all_topics.items():
            members = []
            seen_t: Set[str] = set()
            for c in t_cuis:
                if c in globally_surviving and c not in seen_t:
                    seen_t.add(c)
                    members.append(c)
            if members:
                cleaned[topic_id] = members

        return cleaned, surviving, final_audit


def _reduce_topic(
    t_cuis: List[str],
    embeddings: Dict[str, np.ndarray],
    metadata: Dict[str, CUIMetadata],
    ic_map: Dict[str, float],
    hierarchy_edges: Set[Tuple[str, str]],
) -> Tuple[List[str], List[AuditEntry]]:
    """Dense pairwise within a topic → tight cluster → safety → keep best IC."""

    matrix = np.array([embeddings[c] for c in t_cuis])
    sim = matrix @ matrix.T
    np.clip(sim, -1, 1, out=sim)
    dist = 1.0 - sim
    np.fill_diagonal(dist, 0)
    dist = np.maximum(dist, 0)

    # Guard: NaN from degenerate vectors
    if np.any(~np.isfinite(dist)):
        dist = np.nan_to_num(dist, nan=1.0, posinf=1.0, neginf=0.0)

    n = len(t_cuis)
    idx = {c: i for i, c in enumerate(t_cuis)}
    upper = dist[np.triu_indices(n, k=1)]

    if len(upper) == 0 or np.std(upper) < 1e-9:
        return t_cuis, []

    # Tight threshold — derived from data
    guardrail = float(np.median(upper))
    tight_t = float(np.mean(upper) - np.std(upper))
    nonzero = upper[upper > 0]
    floor = (float(np.min(nonzero)) if len(nonzero) > 0
             else float(np.finfo(np.float32).eps))
    tight_t = max(tight_t, floor)
    tight_t = min(tight_t, guardrail)

    condensed = squareform(dist, checks=False)
    Z = linkage(condensed, method="average")
    labels = fcluster(Z, t=tight_t, criterion="distance")

    r_groups: Dict[int, List[str]] = defaultdict(list)
    for cui, label in zip(t_cuis, labels):
        r_groups[label].append(cui)

    all_kept, all_removed = [], []
    for r_cuis in r_groups.values():
        kept, removed = _resolve_group(
            r_cuis, metadata, ic_map,
            hierarchy_edges, dist, idx,
            tight_t, guardrail)
        all_kept.extend(kept)
        all_removed.extend(removed)

    return all_kept, all_removed


def _resolve_group(
    r_cuis: List[str],
    metadata: Dict[str, CUIMetadata],
    ic_map: Dict[str, float],
    hierarchy_edges: Set[Tuple[str, str]],
    dist: np.ndarray,
    index_map: Dict[str, int],
    tight_t: float,
    guardrail: float,
) -> Tuple[List[str], List[AuditEntry]]:
    """Safety checks → keep highest IC."""

    if len(r_cuis) <= 1:
        return r_cuis, []

    best = max(r_cuis, key=lambda c: ic_map.get(c, 0))
    b_meta = metadata.get(best)
    b_types = set(b_meta.semantic_types) if b_meta and b_meta.semantic_types else set()

    kept = [best]
    audit = []

    for c in r_cuis:
        if c == best:
            continue

        c_meta = metadata.get(c)
        c_types = set(c_meta.semantic_types) if c_meta and c_meta.semantic_types else set()

        # (a) semantic type overlap
        if c_types and b_types and not (c_types & b_types):
            kept.append(c)
            continue

        # (b) parent-child hierarchy
        if (c, best) in hierarchy_edges or (best, c) in hierarchy_edges:
            kept.append(c)
            continue

        # (c) term token divergence
        if c_meta and b_meta and terms_diverge(c_meta.term_tokens, b_meta.term_tokens):
            kept.append(c)
            continue

        # Redundant — remove
        ci = index_map.get(c)
        bi = index_map.get(best)
        pair_dist = dist[ci, bi] if ci is not None and bi is not None else 0.0

        audit.append(AuditEntry(
            removed_cui=c,
            removed_term=c_meta.preferred_term if c_meta else "",
            kept_cui=best,
            kept_term=b_meta.preferred_term if b_meta else "",
            reason="embedding_redundant",
            detail=f"dist {pair_dist:.3f} < tight {tight_t:.3f} | "
                   f"guard {guardrail:.3f} | "
                   f"IC {ic_map.get(best,0):.2f} vs {ic_map.get(c,0):.2f}"))

    return kept, audit


# %% [13] Pipeline
class CUIReductionSystem:
    """Orchestrates full reduction pipeline."""

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        full_network: nx.DiGraph,
        allowed_sabs: List[str],
        embedding_table: str,
        excluded_semantic_types: Optional[Set[str]] = None,
        ic_scores: Optional[Dict[str, float]] = None,
    ):
        self.bq = bigquery.Client(project=project_id)
        self.fetcher = MetadataFetcher(self.bq, project_id, dataset_id)
        self.ic_computer = ICComputer(full_network, ic_scores)
        self.sab_filter = SABFilter(set(allowed_sabs))
        self.semtype_filter = SemanticTypeFilter(excluded_semantic_types or set())
        self.hierarchy_reducer = HierarchyReducer(full_network)
        self.emb_fetcher = EmbeddingFetcher(
            self.bq, project_id, dataset_id, embedding_table)

        log(f"[Stage 0] System Init         : graph={full_network.number_of_nodes()} nodes | "
            f"sabs={allowed_sabs}")

    @timed("full_pipeline")
    def reduce(self, cuis: List[str],
               context_string: str) -> ReductionResult:

        t0 = time.perf_counter()
        all_audit: List[AuditEntry] = []

        if not cuis:
            return self._empty(context_string, t0)

        log(f"[Stage 1] API Extraction     : {len(cuis)} CUIs")

        # Step 2: Metadata
        metadata = self.fetcher.fetch(cuis)
        meta_cov = len(metadata) / len(cuis) if cuis else 0
        log(f"[Stage 2] Metadata Enrichment : {len(metadata)}/{len(cuis)} "
            f"matched ({meta_cov:.0%})")

        # Step 2B: IC
        ic_map = self.ic_computer.compute(cuis, metadata)
        log(f"[Stage 2B] IC Scores          : {len(ic_map)} computed")

        # Step 3A: SAB filter
        after_sab, sab_audit = self.sab_filter.run(cuis, metadata)
        all_audit.extend(sab_audit)
        log(f"[Stage 3A] SAB Filter         : {len(cuis)} → {len(after_sab)} "
            f"(removed {len(cuis) - len(after_sab)})")

        if not after_sab:
            log(f"[Stage 3A] All CUIs filtered out — stopping")
            return self._empty(context_string, t0, all_audit,
                               meta_cov=meta_cov)

        # Step 3B: Semantic type filter
        after_sty, sty_audit = self.semtype_filter.run(after_sab, metadata)
        all_audit.extend(sty_audit)
        log(f"[Stage 3B] Semtype Filter     : {len(after_sab)} → {len(after_sty)} "
            f"(removed {len(after_sab) - len(after_sty)})")

        # Step 3C: Term dedup
        after_term, term_audit = TermDeduplicator.run(
            after_sty, metadata, ic_map)
        all_audit.extend(term_audit)
        log(f"[Stage 3C] Term Dedup         : {len(after_sty)} → {len(after_term)} "
            f"(removed {len(after_sty) - len(after_term)})")

        after_filter = len(after_term)

        # Step 4: Hierarchy
        after_hier, hier_audit, hier_edges = self.hierarchy_reducer.run(
            after_term, ic_map, metadata)
        all_audit.extend(hier_audit)
        log(f"[Stage 4] Hierarchy Reduction : {len(after_term)} → {len(after_hier)} "
            f"(removed {len(after_term) - len(after_hier)})")

        # Step 5A: Embeddings
        embeddings = self.emb_fetcher.fetch(after_hier)
        emb_cov = len(embeddings) / len(after_hier) if after_hier else 0
        log(f"[Stage 5A] Embedding Fetch    : {len(embeddings)}/{len(after_hier)} "
            f"matched ({emb_cov:.0%})")

        # Step 5B-G: Embedding reduction (ScaNN topics + dense redundancy)
        topics, surviving, emb_audit = EmbeddingReducer.run(
            after_hier, metadata, embeddings, ic_map, hier_edges)
        all_audit.extend(emb_audit)
        log(f"[Stage 5] Embedding Reduction : {len(after_hier)} → {len(surviving)} "
            f"(removed {len(after_hier) - len(surviving)})")

        # Step 6: Build topic output
        topics_out: Dict[str, TopicInfo] = {}
        for topic_id, t_cuis in topics.items():
            if not t_cuis:
                continue
            topic_types: Set[str] = set()
            cui_terms: Dict[str, str] = {}
            for c in t_cuis:
                m = metadata.get(c)
                if m and m.semantic_types:
                    topic_types.update(m.semantic_types)
                cui_terms[c] = m.preferred_term if m else c
            topics_out[topic_id] = TopicInfo(
                topic_id=topic_id,
                semantic_types=topic_types or {"Unknown"},
                cuis=t_cuis,
                cui_terms=cui_terms)

        log(f"[Stage 6] Topic Grouping      : {len(surviving)} CUIs → "
            f"{len(topics_out)} topics")

        # Step 7: 100% accounting
        audited = {e.removed_cui for e in all_audit}
        survived = set(surviving)
        unaccounted = set(cuis) - survived - audited
        if unaccounted:
            log(f"[Stage 7] WARNING: {len(unaccounted)} unaccounted CUIs",
                "WARNING")
            for cui in unaccounted:
                m = metadata.get(cui)
                all_audit.append(AuditEntry(
                    cui, m.preferred_term if m else "",
                    "", "", "unaccounted",
                    "CUI not in surviving or audit trail"))
        else:
            log(f"[Stage 7] Audit              : 100% accounted "
                f"({len(survived)} survived + {len(audited)} removed "
                f"= {len(survived) + len(audited)})")

        elapsed = (time.perf_counter() - t0) * 1000

        log(f"")
        log(f"{'─' * 55}")
        log(f"  SUMMARY: {len(cuis)} → {len(surviving)} CUIs "
            f"in {len(topics_out)} topics ({elapsed:.0f}ms)")
        log(f"{'─' * 55}")

        return ReductionResult(
            context_string=context_string,
            input_count=len(cuis),
            after_filter_count=after_filter,
            after_hierarchy_count=len(after_hier),
            after_redundancy_count=len(surviving),
            topics=topics_out,
            all_reduced_cuis=surviving,
            audit_trail=all_audit,
            processing_time_ms=elapsed,
            metadata_coverage=meta_cov,
            embedding_coverage=emb_cov)

    def _empty(self, ctx: str, t0: float,
               audit: Optional[List[AuditEntry]] = None,
               meta_cov: float = 0.0) -> ReductionResult:
        return ReductionResult(
            ctx, 0, 0, 0, 0, {}, [], audit or [],
            (time.perf_counter() - t0) * 1000, meta_cov, 0.0)

    def get_stats(self) -> Dict:
        with _timing_lock:
            out = {}
            for k, v in _timings.items():
                if v:
                    vals = list(v)
                    out[k] = {
                        "count": len(vals),
                        "mean_ms": float(np.mean(vals)),
                        "p99_ms": float(np.percentile(vals, 99)),
                    }
            return out


# %% [14] CUI Extraction API
class CUIExtractor:
    """Calls extraction API. Filters by confidence. Raises on failure."""

    def __init__(self, api_url: str,
                 min_confidence: float = 0.0,
                 top_k: int = 3):
        self.url = api_url
        self.min_confidence = min_confidence
        self.top_k = top_k
        self.session = requests.Session()

        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text: str) -> List[str]:
        resp = self.session.post(
            self.url, headers=self.headers,
            json={"query_texts": [text], "top_k": self.top_k},
            timeout=200)
        resp.raise_for_status()
        data = resp.json()

        cuis = []
        if isinstance(data, dict):
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        raw_score = item.get("score",
                                    item.get("confidence", 1.0))
                        try:
                            score = float(raw_score)
                        except (ValueError, TypeError):
                            score = 0.0
                        if score >= self.min_confidence:
                            cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
        return list(set(cuis))


# %% [15] Run
def main():
    import pickle

    PROJECT_ID = "your-project-id"
    DATASET_ID = "your-dataset"
    API_URL = "https://your-api/extract"
    NETWORK_PATH = "networkx_cui_context_v1_1_0.pkl"
    EMBEDDING_TABLE = "cui_embeddings"
    ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM",
                     "SNOMEDCT_US", "LOINC"]

    log("Loading hierarchy...")
    with open(NETWORK_PATH, "rb") as f:
        network = pickle.load(f)
    log(f"[Pickle] Hierarchy loaded     : {network.number_of_nodes()} nodes, "
        f"{network.number_of_edges()} edges")

    system = CUIReductionSystem(
        PROJECT_ID, DATASET_ID, network, ALLOWED_SABS,
        EMBEDDING_TABLE,
        # ic_scores=precomputed,  # pass precomputed for speed
    )
    extractor = CUIExtractor(API_URL)

    texts = [
        "Patient has severe pain in left knee with swelling",
        "History of type 2 diabetes mellitus with peripheral neuropathy",
        "Acute exacerbation of chronic obstructive pulmonary disease",
    ]

    for text in texts:
        log(f"\n{'=' * 60}")
        log(f"TEXT: {text}")
        log(f"{'=' * 60}")

        cuis = extractor.extract(text)
        if not cuis:
            log("  No CUIs extracted")
            continue

        result = system.reduce(cuis, text)

        # ── OUTPUT 1: Reduced CUI List ──
        log(f"\n┌─ OUTPUT 1: Reduced CUIs ({len(result.all_reduced_cuis)}) ─┐")
        for c in result.all_reduced_cuis:
            m = result.topics  # get term from topics
            term = ""
            for t in result.topics.values():
                if c in t.cui_terms:
                    term = t.cui_terms[c]
                    break
            log(f"  {c}  {term}")
        log(f"└─ Total: {len(result.all_reduced_cuis)} CUIs ─┘")

        # ── OUTPUT 2: Topic Groupings ──
        log(f"\n┌─ OUTPUT 2: Topics ({len(result.topics)}) ─┐")
        for tid, topic in result.topics.items():
            log(f"  {tid} [{', '.join(topic.semantic_types)}]")
            for c, term in topic.cui_terms.items():
                log(f"    ├─ {c}  {term}")
        log(f"└─ Total: {len(result.topics)} topics ─┘")


if __name__ == "__main__":
    main()
