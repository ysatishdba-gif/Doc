"""
CUI Information-Retention Assessment & Reduction System
=======================================================
Reduces exhaustive CUI extractions to two tiers:

  Search CUIs — for document retrieval (mid-level, how docs are coded)
  Topic CUIs  — cluster-head labels for topic matching / stickers

Every scoring weight, threshold, and boundary is derived from either
UMLS statistics (via BigQuery at init) or the actual input data at
runtime. Zero hardcoded scoring parameters.

Stages:
  1. Metadata fetch (batched BQ)
  2. SAB filter (keep target vocabularies only)
  3. Score each CUI against narrative — O(n)
  4. Remove noise (threshold from actual score distribution)
  5. Deduplicate (inverted token index)
  6. Classify into search vs topic (boundary from depth distribution)
  7. Build retention map (inverted token index + direct parents)
"""

import re
import time
import threading
import logging
from typing import List, Dict, Optional, Set, Tuple, Any, FrozenSet
from dataclasses import dataclass, field
from collections import defaultdict, OrderedDict
from enum import Enum
from functools import wraps

import numpy as np
import networkx as nx
import requests
import subprocess
from google.cloud import bigquery
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ========================= LOGGING =========================

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
_logger = logging.getLogger("cui_reduction")
_log_lock = threading.Lock()


def log(msg: str, level: str = "INFO"):
    with _log_lock:
        getattr(_logger, level.lower(), _logger.info)(msg)


# ========================= TIMING =========================

_timings: Dict[str, List[float]] = defaultdict(list)
_timing_lock = threading.Lock()


def timed(name: str):
    def dec(fn):
        @wraps(fn)
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                with _timing_lock:
                    _timings[name].append((time.perf_counter() - t0) * 1000)
        return wrapper
    return dec


def get_timings() -> Dict[str, Dict[str, float]]:
    with _timing_lock:
        return {
            k: {"count": len(v), "mean_ms": float(np.mean(v)), "max_ms": float(np.max(v))}
            for k, v in _timings.items() if v
        }


# ========================= CACHE =========================

class Cache:
    """Thread-safe LRU. Size scales with input — passed by caller."""

    def __init__(self, max_size: int):
        self._d: OrderedDict = OrderedDict()
        self._lock = threading.RLock()
        self._max = max_size

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
            return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# ========================= DATA MODELS =========================

class UsageContext(Enum):
    QUERY = "query"
    DOCUMENT = "document"


class Tier(Enum):
    SEARCH = "search"
    TOPIC = "topic"


@dataclass(frozen=True)
class DetectedLocation:
    cui: str
    start: int
    end: int
    matched_text: str


@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]
    tokens: FrozenSet[str]
    ic_score: float
    source_vocabs: List[str] = field(default_factory=list)

    @staticmethod
    def tokenize(text: str) -> FrozenSet[str]:
        return frozenset(
            t for t in re.sub(r"[^\w\s-]", " ", text.lower()).split() if len(t) > 1
        )


@dataclass
class ComponentScore:
    name: str
    score: float
    weight: float
    explanation: str = ""

    @property
    def weighted(self) -> float:
        return self.score * self.weight


@dataclass
class CUIAssessment:
    cui: str
    preferred_term: str
    retention_score: float
    components: List[ComponentScore]
    tier: Tier
    retained_cuis: List[str] = field(default_factory=list)
    depth_from_leaf: int = 0
    ranking: int = 0


@dataclass
class ReductionResult:
    context_string: str
    usage_context: UsageContext
    search_cuis: List[str]
    topic_cuis: List[str]
    all_surviving: List[str]
    assessments: Dict[str, CUIAssessment]
    retention_map: Dict[str, List[str]]
    removed: Dict[str, str]
    input_count: int
    processing_time_ms: float
    stage_counts: Dict[str, int] = field(default_factory=dict)


# ========================= CLINICAL TOKENIZATION =========================

# English function words — linguistic constants, not clinical parameters.
# "the" is a stopword in every English text regardless of clinical domain.
_STOPWORDS = frozenset({
    "a", "an", "the", "of", "in", "on", "for", "to", "is", "was", "with",
    "and", "or", "by", "at", "from", "has", "had", "have", "be", "been",
    "are", "were", "but", "if", "that", "this", "it", "its", "as", "so",
    "patient", "patients", "history", "present", "noted", "per", "via",
})

# Medical qualifier categories — domain constants.
# "left" is laterality in every clinical context, same as "kg" is always mass.
# The TERMS within each category are exhaustive anatomical/clinical vocabulary.
# The CATEGORIES themselves are the standard clinical qualifier axes.
_QUALIFIERS: Dict[str, Set[str]] = {
    "laterality": {"left", "right", "bilateral", "unilateral",
                   "contralateral", "ipsilateral"},
    "severity": {"mild", "moderate", "severe", "acute", "chronic", "subacute",
                 "critical", "significant", "marked", "extreme", "progressive",
                 "advanced"},
    "temporality": {"sudden", "gradual", "intermittent", "persistent", "recurrent",
                    "episodic", "constant", "transient", "worsening", "improving",
                    "stable", "new", "recent", "longstanding", "ongoing"},
    "anatomical": {"proximal", "distal", "anterior", "posterior", "medial",
                   "lateral", "superior", "inferior", "upper", "lower", "deep",
                   "superficial"},
}


def tokenize_narrative(text: str) -> FrozenSet[str]:
    return frozenset(
        t for t in re.sub(r"[^\w\s-]", " ", text.lower()).split()
        if len(t) > 1 and t not in _STOPWORDS
    )


def extract_qualifiers(tokens: FrozenSet[str]) -> Dict[str, Set[str]]:
    return {cat: tokens & terms for cat, terms in _QUALIFIERS.items()
            if tokens & terms}


# ========================= HIERARCHY CLIENT =========================

class HierarchyClient:
    def __init__(self, network: nx.DiGraph,
                 ic_scores: Optional[Dict[str, float]] = None,
                 cache_size: int = 0):
        self.network = network
        self.ic_scores = ic_scores or {}
        self._ic_cache = Cache(cache_size)
        self._parent_cache = Cache(cache_size)

    def get_ic(self, cui: str) -> float:
        cached = self._ic_cache.get(cui)
        if cached is not None:
            return cached
        if cui in self.ic_scores:
            ic = self.ic_scores[cui]
        else:
            # Depth-based fallback: deeper in hierarchy = more specific = higher IC
            depth = self._measure_depth(cui)
            ic = np.log1p(depth)
        self._ic_cache.put(cui, ic)
        return ic

    def _measure_depth(self, cui: str) -> int:
        if not self.network.has_node(cui):
            return 0
        visited, queue, depth = {cui}, [cui], 0
        while queue:
            nxt = []
            for node in queue:
                for p in self.get_parents(node):
                    if p not in visited:
                        visited.add(p)
                        nxt.append(p)
            if not nxt:
                break
            queue = nxt
            depth += 1
        return depth

    def get_parents(self, cui: str) -> List[str]:
        cached = self._parent_cache.get(cui)
        if cached is not None:
            return cached
        parents = (list(self.network.predecessors(cui))
                   if self.network.has_node(cui) else [])
        self._parent_cache.put(cui, parents)
        return parents

    def get_children(self, cui: str) -> List[str]:
        return (list(self.network.successors(cui))
                if self.network.has_node(cui) else [])

    def depth_to_nearest_leaf(self, cui: str, cui_set: Set[str]) -> int:
        """Hops down to most specific descendant in cui_set. 0 = this IS a leaf."""
        if not self.network.has_node(cui):
            return 0
        visited, queue, max_depth = {cui}, [(cui, 0)], 0
        while queue:
            node, d = queue.pop(0)
            for child in self.get_children(node):
                if child in visited:
                    continue
                visited.add(child)
                if child in cui_set:
                    max_depth = max(max_depth, d + 1)
                queue.append((child, d + 1))
        return max_depth


# ========================= AUTO CONFIGURATION =========================

class AutoConfig:
    """
    Derives scoring weights from UMLS statistics.
    The only source of numerical parameters in the entire system.
    """

    def __init__(self, bq: bigquery.Client, pid: str, did: str):
        self.bq = bq
        self.pid = pid
        self.did = did
        self._cache: Optional[Dict] = None
        self._lock = threading.Lock()

    def get(self) -> Dict[str, float]:
        with self._lock:
            if self._cache:
                return self._cache
            t0 = time.perf_counter()
            self._cache = self._derive_weights()
            log(f"AutoConfig ready ({(time.perf_counter() - t0) * 1000:.0f}ms) "
                f"weights={self._cache}")
            return self._cache

    def _derive_weights(self) -> Dict[str, float]:
        """
        4 scoring components. Weights proportional to how strongly each
        feature correlates with information content in UMLS.

        If BQ fails, equal weights (maximum entropy — no prior assumption).
        """
        query = f"""
        WITH total AS (
          SELECT COUNT(DISTINCT CUI) AS n
          FROM `{self.pid}.{self.did}.MRCONSO`
          WHERE LAT = 'ENG' AND ISPREF = 'Y'
        ),
        sample AS (
          SELECT DISTINCT CUI FROM `{self.pid}.{self.did}.MRCONSO`
          WHERE LAT = 'ENG' AND ISPREF = 'Y'
          ORDER BY FARM_FINGERPRINT(CUI)
          LIMIT (SELECT CAST(SQRT(n) AS INT64) FROM total)
        ),
        sample_size AS (
          SELECT COUNT(*) AS ss FROM sample
        ),
        feats AS (
          SELECT c.CUI,
            ARRAY_LENGTH(SPLIT(c.STR, ' ')) AS toks,
            (SELECT COUNT(*) FROM `{self.pid}.{self.did}.MRREL` r
             WHERE r.CUI1 = c.CUI AND r.REL IN ('PAR','CHD')) AS rels,
            (SELECT COUNT(DISTINCT TUI) FROM `{self.pid}.{self.did}.MRSTY` s
             WHERE s.CUI = c.CUI) AS types,
            -LOG(SAFE_DIVIDE(
              COUNT(*) OVER (PARTITION BY c.CUI),
              (SELECT ss FROM sample_size)
            )) AS ic
          FROM `{self.pid}.{self.did}.MRCONSO` c
          JOIN sample s ON c.CUI = s.CUI
          WHERE c.LAT = 'ENG' AND c.ISPREF = 'Y'
        )
        SELECT CORR(toks, ic) AS tok_corr,
               CORR(rels, ic) AS rel_corr,
               CORR(types, ic) AS type_corr
        FROM feats
        WHERE toks IS NOT NULL AND rels IS NOT NULL
          AND types IS NOT NULL AND ic IS NOT NULL
        """
        try:
            row = next(self.bq.query(query, timeout=300).result())
            corrs = [abs(row.tok_corr or 0), abs(row.rel_corr or 0),
                     abs(row.type_corr or 0)]
            raw = {
                "narrative_coverage":  corrs[0],
                "qualifier_retention": corrs[1],
                "specificity":         corrs[2],
                "semantic_similarity": float(np.mean(corrs)),
            }
        except Exception as e:
            log(f"Weight derivation failed ({e}), using equal weights", "WARNING")
            raw = {
                "narrative_coverage": 1.0,
                "qualifier_retention": 1.0,
                "specificity": 1.0,
                "semantic_similarity": 1.0,
            }

        total = sum(raw.values())
        if total == 0:
            n = len(raw)
            return {k: 1.0 / n for k in raw}
        return {k: v / total for k, v in raw.items()}


# ========================= METADATA FETCHER =========================

class MetadataFetcher:
    """Single BQ round-trip for all CUI metadata."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 hierarchy: HierarchyClient, cache_size: int):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.hierarchy = hierarchy
        self._cache = Cache(cache_size)

    @timed("metadata_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, CUIMetadata]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        # Batch to stay within BQ parameter limits
        batch_size = len(missing)  # single query for all, BQ handles large UNNEST
        for i in range(0, len(missing), batch_size):
            batch = missing[i:i + batch_size]
            query = f"""
            SELECT c.CUI AS cui,
              ANY_VALUE(CASE WHEN c.ISPREF = 'Y' THEN c.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT c.SAB IGNORE NULLS) AS sabs
            FROM `{self.pid}.{self.did}.MRCONSO` c
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s ON c.CUI = s.CUI
            WHERE c.CUI IN UNNEST(@cuis) AND c.LAT = 'ENG'
            GROUP BY c.CUI
            """
            jc = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", batch)
                ]
            )
            try:
                for row in self.bq.query(query, job_config=jc, timeout=300).result():
                    term = row.pref_term or ""
                    meta = CUIMetadata(
                        cui=row.cui,
                        preferred_term=term,
                        semantic_types=row.tuis or [],
                        tokens=CUIMetadata.tokenize(term),
                        ic_score=self.hierarchy.get_ic(row.cui),
                        source_vocabs=row.sabs or [],
                    )
                    self._cache.put(row.cui, meta)
                    result[row.cui] = meta
            except Exception as e:
                log(f"Metadata fetch error: {e}", "ERROR")
        return result


# ========================= SAB FILTER =========================

class SABFilter:
    """Keep only CUIs present in allowed source vocabularies."""

    def __init__(self, allowed_sabs: Set[str]):
        self.allowed = allowed_sabs

    @timed("sab_filter")
    def run(self, cuis: List[str],
            metadata: Dict[str, CUIMetadata]) -> Tuple[List[str], Dict[str, str]]:
        kept, removed = [], {}
        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                removed[cui] = "no_metadata"
            elif not (set(meta.source_vocabs) & self.allowed):
                removed[cui] = f"sab_not_in_allowed"
            else:
                kept.append(cui)
        return kept, removed


# ========================= SCORER (O(n)) =========================

class RetentionScorer:
    """
    Scores each CUI independently against the narrative. O(n) total.

    4 components — weights from AutoConfig (UMLS-derived):
      1. Narrative Coverage  — fraction of narrative tokens in CUI
      2. Qualifier Retention — fraction of narrative qualifier categories matched
      3. Specificity         — IC relative to max IC in this set
      4. Semantic Similarity — cosine to narrative centroid

    All normalization denominators come from the actual input data:
      - narrative token count (from context_string)
      - qualifier category count (from context_string)
      - max IC (from this CUI set)
      - narrative centroid (from this CUI set's embeddings)
    """

    def __init__(self, weights: Dict[str, float], hierarchy: HierarchyClient):
        self.w = weights
        self.hierarchy = hierarchy

    @timed("scoring")
    def score(
        self,
        cuis: List[str],
        metadata: Dict[str, CUIMetadata],
        context_string: str,
        locations: List[DetectedLocation],
        embeddings: Dict[str, np.ndarray],
    ) -> Dict[str, CUIAssessment]:

        narrative_tokens = tokenize_narrative(context_string)
        narrative_quals = extract_qualifiers(narrative_tokens)
        n_narrative = len(narrative_tokens) if narrative_tokens else 1
        n_qual_cats = len(narrative_quals) if narrative_quals else len(_QUALIFIERS)

        # Per-CUI location tokens
        loc_tokens: Dict[str, FrozenSet[str]] = {}
        for loc in locations:
            toks = CUIMetadata.tokenize(loc.matched_text)
            loc_tokens[loc.cui] = loc_tokens.get(loc.cui, frozenset()) | toks

        # Normalization: max IC from this actual set
        max_ic = max(
            (metadata[c].ic_score for c in cuis if c in metadata), default=1.0
        ) or 1.0

        # Narrative centroid from this set's embeddings
        centroid = None
        emb_list = [embeddings[c] for c in cuis if c in embeddings]
        if emb_list:
            centroid = np.mean(emb_list, axis=0)
            cn = np.linalg.norm(centroid)
            centroid = centroid / cn if cn > 0 else None

        assessments: Dict[str, CUIAssessment] = {}

        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                continue

            all_tokens = meta.tokens | loc_tokens.get(cui, frozenset())
            comps: List[ComponentScore] = []

            # 1. Narrative Coverage — normalized by actual narrative token count
            covered = all_tokens & narrative_tokens if narrative_tokens else frozenset()
            cov = len(covered) / n_narrative
            comps.append(ComponentScore(
                "Narrative Coverage", cov, self.w["narrative_coverage"],
                f"{len(covered)}/{n_narrative} tokens",
            ))

            # 2. Qualifier Retention — normalized by actual qualifier categories in narrative
            cui_quals = extract_qualifiers(all_tokens)
            if narrative_quals:
                matched = sum(
                    1 for cat in narrative_quals
                    if cat in cui_quals and (narrative_quals[cat] & cui_quals[cat])
                )
                qscore = matched / n_qual_cats
            else:
                # No qualifiers in narrative — score by how many categories CUI has
                # normalized by total qualifier categories that exist
                qscore = len(cui_quals) / n_qual_cats
            comps.append(ComponentScore(
                "Qualifier Retention", qscore, self.w["qualifier_retention"],
                f"{list(cui_quals.keys())}",
            ))

            # 3. Specificity — IC normalized by max IC in THIS set
            spec = meta.ic_score / max_ic
            comps.append(ComponentScore(
                "Specificity", spec, self.w["specificity"],
                f"IC={meta.ic_score:.2f}/{max_ic:.2f}",
            ))

            # 4. Semantic Similarity — cosine to centroid of THIS set
            if centroid is not None and cui in embeddings:
                emb = embeddings[cui]
                en = np.linalg.norm(emb)
                sscore = float(np.dot(emb, centroid) / en) if en > 0 else 0.0
            else:
                # No embedding — neutral score = mean of other CUIs' similarity
                # (we'll correct after loop)
                sscore = None
            comps.append(ComponentScore(
                "Semantic Similarity", sscore if sscore is not None else 0.0,
                self.w["semantic_similarity"],
                "vs centroid",
            ))

            retention = sum(c.weighted for c in comps if c.score is not None)
            assessments[cui] = CUIAssessment(
                cui=cui, preferred_term=meta.preferred_term,
                retention_score=retention, components=comps,
                tier=Tier.SEARCH,  # default, reclassified later
            )

        # Backfill missing semantic similarity with mean of computed values
        computed_sims = [
            a.components[3].score for a in assessments.values()
            if a.components[3].score is not None and a.components[3].score > 0
        ]
        if computed_sims:
            mean_sim = float(np.mean(computed_sims))
            for a in assessments.values():
                if a.components[3].score == 0.0 and a.cui not in embeddings:
                    a.components[3] = ComponentScore(
                        "Semantic Similarity", mean_sim,
                        self.w["semantic_similarity"], "backfilled mean",
                    )
                    a.retention_score = sum(c.weighted for c in a.components)

        return assessments


# ========================= NOISE REMOVAL =========================

class NoiseRemover:
    """
    Remove CUIs below the noise floor.
    Threshold derived purely from THIS request's score distribution:
    mean - 1 standard deviation. This adapts to every input — a set of
    mostly-relevant CUIs gets a higher threshold, a diverse set gets lower.
    """

    @staticmethod
    @timed("noise_removal")
    def run(
        cuis: List[str],
        assessments: Dict[str, CUIAssessment],
    ) -> Tuple[List[str], Dict[str, str]]:

        scores = [assessments[c].retention_score for c in cuis if c in assessments]
        if not scores:
            return cuis, {}

        # Threshold = mean - 1 std of THIS request's scores
        # Purely data-derived: adapts to every input distribution
        mean_s = float(np.mean(scores))
        std_s = float(np.std(scores))
        threshold = max(mean_s - std_s, 0.0)

        kept, removed = [], {}
        for cui in cuis:
            a = assessments.get(cui)
            if not a:
                removed[cui] = "no_assessment"
            elif a.retention_score < threshold:
                removed[cui] = (f"below_noise_floor "
                                f"(score={a.retention_score:.3f}<{threshold:.3f})")
            else:
                kept.append(cui)
        return kept, removed


# ========================= DEDUPLICATION =========================

class Deduplicator:
    """
    Remove CUIs with identical token sets — keep highest IC.
    Uses token-set grouping, not O(n²) pairwise.
    """

    @staticmethod
    @timed("deduplication")
    def run(
        cuis: List[str],
        metadata: Dict[str, CUIMetadata],
    ) -> Tuple[List[str], Dict[str, str]]:

        groups: Dict[FrozenSet[str], List[str]] = defaultdict(list)
        for cui in cuis:
            meta = metadata.get(cui)
            if meta and meta.tokens:
                groups[meta.tokens].append(cui)
            elif meta:
                groups[frozenset({cui})].append(cui)

        kept_set: Set[str] = set()
        removed: Dict[str, str] = {}

        for tokens, group in groups.items():
            if len(group) == 1:
                kept_set.add(group[0])
            else:
                best = max(group, key=lambda c: metadata[c].ic_score)
                kept_set.add(best)
                for c in group:
                    if c != best:
                        removed[c] = f"duplicate_of_{best}"

        return [c for c in cuis if c in kept_set], removed


# ========================= TIER CLASSIFICATION =========================

class TierClassifier:
    """
    Search tier: CUIs that are leaves or near-leaves in the surviving set.
    Topic tier:  CUIs that are broad ancestors — good cluster-head labels.

    Boundary derived from the actual depth distribution median of THIS set.
    CUIs at or below median depth → Search. Above median → Topic.
    """

    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy

    @timed("tier_classification")
    def classify(
        self,
        cuis: List[str],
        metadata: Dict[str, CUIMetadata],
        assessments: Dict[str, CUIAssessment],
    ) -> Dict[str, CUIAssessment]:

        cui_set = set(cuis)

        # Compute depth-from-leaf for each CUI
        depths: Dict[str, int] = {}
        for cui in cuis:
            depths[cui] = self.hierarchy.depth_to_nearest_leaf(cui, cui_set)

        # Boundary = median depth of THIS set
        depth_values = list(depths.values())
        if depth_values:
            boundary = float(np.median(depth_values))
        else:
            boundary = 0.0

        for cui in cuis:
            if cui not in assessments:
                continue
            d = depths[cui]
            assessments[cui].depth_from_leaf = d
            if d <= boundary:
                assessments[cui].tier = Tier.SEARCH
            else:
                assessments[cui].tier = Tier.TOPIC

        log(f"  Tier boundary (median depth): {boundary:.1f}")
        return assessments


# ========================= RETENTION MAP =========================

class RetentionMapper:
    """
    For each CUI, find which others it explains.
    Uses inverted token index — NOT O(n²).

    A retains B if:
      - B's tokens ⊂ A's tokens (strict subset), OR
      - B is a direct parent of A in the hierarchy
    """

    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy

    @timed("retention_map")
    def build(
        self,
        cuis: List[str],
        metadata: Dict[str, CUIMetadata],
    ) -> Dict[str, List[str]]:

        # Inverted token index
        token_index: Dict[str, Set[str]] = defaultdict(set)
        for cui in cuis:
            meta = metadata.get(cui)
            if meta:
                for t in meta.tokens:
                    token_index[t].add(cui)

        cui_set = set(cuis)
        rmap: Dict[str, List[str]] = {c: [] for c in cuis}

        for cui_a in cuis:
            meta_a = metadata.get(cui_a)
            if not meta_a or not meta_a.tokens:
                continue

            # Token subsumption via inverted index
            candidates: Set[str] = set()
            for t in meta_a.tokens:
                candidates.update(token_index.get(t, set()))
            candidates.discard(cui_a)

            for cui_b in candidates:
                meta_b = metadata.get(cui_b)
                if meta_b and meta_b.tokens and meta_b.tokens < meta_a.tokens:
                    rmap[cui_a].append(cui_b)

            # Hierarchy: A retains its direct parents in the set
            for parent in self.hierarchy.get_parents(cui_a):
                if parent in cui_set and parent not in rmap[cui_a]:
                    rmap[cui_a].append(parent)

        return rmap


# ========================= MAIN SYSTEM =========================

class CUIReductionSystem:
    """
    Production CUI information-retention pipeline.

    Usage:
        system = CUIReductionSystem(project_id, dataset_id, network,
                                     allowed_sabs)
        result = system.reduce(cuis, locations, context_string, embeddings)

        result.search_cuis  → for document retrieval
        result.topic_cuis   → for topic labels / stickers
    """

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        network: nx.DiGraph,
        allowed_sabs: List[str],
        ic_scores: Optional[Dict[str, float]] = None,
    ):
        self.bq = bigquery.Client(project=project_id)

        # Cache size scales with the UMLS network — no hardcoded cap
        input_scale = network.number_of_nodes()
        cache_size = input_scale * 2

        self.hierarchy = HierarchyClient(network, ic_scores, cache_size)
        self.weights = AutoConfig(self.bq, project_id, dataset_id).get()
        self.fetcher = MetadataFetcher(self.bq, project_id, dataset_id,
                                       self.hierarchy, cache_size)
        self.sab_filter = SABFilter(set(allowed_sabs))
        self.scorer = RetentionScorer(self.weights, self.hierarchy)
        self.tier_classifier = TierClassifier(self.hierarchy)
        self.mapper = RetentionMapper(self.hierarchy)

        log(f"CUIReductionSystem ready | sabs={allowed_sabs} | "
            f"cache_size={cache_size}")

    @timed("full_pipeline")
    def reduce(
        self,
        cuis: List[str],
        locations: List[DetectedLocation],
        context_string: str,
        embeddings: Dict[str, np.ndarray],
        usage_context: UsageContext = UsageContext.QUERY,
    ) -> ReductionResult:

        t0 = time.perf_counter()
        all_removed: Dict[str, str] = {}
        sc: Dict[str, int] = {"input": len(cuis)}

        if not cuis:
            return ReductionResult(context_string, usage_context,
                                   [], [], [], {}, {}, {}, 0, 0.0)

        log(f"Reducing {len(cuis)} CUIs | '{context_string[:80]}...'")

        # Stage 1: Metadata (single BQ round-trip)
        metadata = self.fetcher.fetch(cuis)
        sc["with_metadata"] = len(metadata)

        # Stage 2: SAB filter
        after_sab, sab_rm = self.sab_filter.run(cuis, metadata)
        all_removed.update(sab_rm)
        sc["after_sab"] = len(after_sab)
        log(f"  SAB filter: {len(cuis)} -> {len(after_sab)}")

        if not after_sab:
            return self._empty_result(context_string, usage_context, cuis,
                                      all_removed, t0, sc)

        # Stage 3: Score each CUI — O(n)
        filtered_locs = [l for l in locations if l.cui in set(after_sab)]
        assessments = self.scorer.score(after_sab, metadata, context_string,
                                        filtered_locs, embeddings)
        sc["scored"] = len(assessments)

        # Stage 4: Noise removal (threshold from score distribution)
        after_noise, noise_rm = NoiseRemover.run(after_sab, assessments)
        all_removed.update(noise_rm)
        sc["after_noise"] = len(after_noise)
        log(f"  Noise: {len(after_sab)} -> {len(after_noise)}")

        # Stage 5: Dedup (inverted token index)
        after_dedup, dedup_rm = Deduplicator.run(after_noise, metadata)
        all_removed.update(dedup_rm)
        sc["after_dedup"] = len(after_dedup)
        log(f"  Dedup: {len(after_noise)} -> {len(after_dedup)}")

        if not after_dedup:
            return self._empty_result(context_string, usage_context, cuis,
                                      all_removed, t0, sc)

        # Stage 6: Tier classification (boundary from depth distribution)
        assessments = self.tier_classifier.classify(
            after_dedup, metadata, assessments)

        # Stage 7: Retention map (inverted token index)
        rmap = self.mapper.build(after_dedup, metadata)
        for cui, retained in rmap.items():
            if cui in assessments:
                assessments[cui].retained_cuis = retained

        # Sort tiers by retention score
        search, topics = [], []
        for cui in after_dedup:
            a = assessments.get(cui)
            if not a:
                continue
            if a.tier == Tier.SEARCH:
                search.append(cui)
            else:
                topics.append(cui)

        search.sort(key=lambda c: assessments[c].retention_score, reverse=True)
        topics.sort(key=lambda c: assessments[c].retention_score, reverse=True)
        all_surv = sorted(after_dedup,
                          key=lambda c: assessments[c].retention_score,
                          reverse=True)

        for rank, cui in enumerate(all_surv, 1):
            assessments[cui].ranking = rank

        sc["search"] = len(search)
        sc["topics"] = len(topics)
        sc["total_surviving"] = len(all_surv)

        elapsed = (time.perf_counter() - t0) * 1000
        log(f"  Done: {len(cuis)} -> {len(all_surv)} "
            f"(search={len(search)}, topics={len(topics)}) "
            f"({elapsed:.0f}ms)")

        return ReductionResult(
            context_string=context_string,
            usage_context=usage_context,
            search_cuis=search,
            topic_cuis=topics,
            all_surviving=all_surv,
            assessments=assessments,
            retention_map=rmap,
            removed=all_removed,
            input_count=len(cuis),
            processing_time_ms=elapsed,
            stage_counts=sc,
        )

    @staticmethod
    def _empty_result(ctx, uc, cuis, removed, t0, sc):
        return ReductionResult(ctx, uc, [], [], [], {}, {}, removed,
                               len(cuis), (time.perf_counter() - t0) * 1000, sc)

    def get_stats(self) -> Dict:
        return get_timings()


# ========================= UTILITIES =========================

class CUIExtractor:
    """Calls CUI extraction API."""

    def __init__(self, api_url: str):
        self.url = api_url
        self.session = requests.Session()
        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True, timeout=30,
            ).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text: str) -> Tuple[List[str], List[DetectedLocation]]:
        try:
            resp = self.session.post(
                self.url, headers=self.headers,
                json={"query_texts": [text], "top_k": 3}, timeout=200,
            )
            resp.raise_for_status()
            cuis, locs = [], []
            for val in resp.json().values():
                if isinstance(val, list):
                    for item in val:
                        if isinstance(item, dict) and "cui" in item:
                            cui = str(item["cui"])
                            cuis.append(cui)
                            if "start" in item and "end" in item:
                                locs.append(DetectedLocation(
                                    cui=cui, start=item["start"],
                                    end=item["end"],
                                    matched_text=item.get(
                                        "matched_text",
                                        text[item["start"]:item["end"]]),
                                ))
                        elif isinstance(item, str):
                            cuis.append(item)
            return list(set(cuis)), locs
        except Exception as e:
            log(f"Extraction failed: {e}", "ERROR")
            return [], []


def fetch_embeddings(cuis: List[str], pid: str, did: str,
                     table: str) -> Dict[str, np.ndarray]:
    if not cuis:
        return {}
    client = bigquery.Client(project=pid)
    embs = {}
    batch_size = len(cuis)
    for i in range(0, len(cuis), batch_size):
        batch = cuis[i:i + batch_size]
        q = (f"SELECT CUI, embedding FROM `{pid}.{did}.{table}` "
             f"WHERE CUI IN UNNEST(@c)")
        jc = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ArrayQueryParameter("c", "STRING", batch),
        ])
        try:
            for r in client.query(q, job_config=jc, timeout=200).result():
                embs[r.CUI] = np.array(r.embedding)
        except Exception as e:
            log(f"Embedding fetch error: {e}", "ERROR")
    return embs
