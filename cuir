# %% Retention v5: fully data-driven, no hardcoded values

from collections import defaultdict

embs = system.emb_fetcher.fetch(result.all_reduced_cuis)
embedded_cuis = [c for c in result.all_reduced_cuis if c in embs]
n = len(embedded_cuis)

if n < 2:
    print("Too few embedded CUIs")
else:
    matrix = np.array([embs[c] for c in embedded_cuis], dtype=np.float32)

    # --- STEP 1: find dense core (data-driven context) ---
    # each CUI's average similarity to all others
    sim_matrix = matrix @ matrix.T
    np.fill_diagonal(sim_matrix, 0)
    avg_sims = sim_matrix.mean(axis=1)

    # top-K = sqrt(n) most central CUIs define the context
    k_core = max(5, int(np.sqrt(n)))
    core_idx = np.argsort(avg_sims)[-k_core:]
    ctx_vec = matrix[core_idx].mean(axis=0)
    ctx_vec = ctx_vec / (np.linalg.norm(ctx_vec) + 1e-10)

    # --- STEP 2: compute all scores from data ---
    # alignment: cosine to dense core (semantic match, handles synonyms)
    alignments = matrix @ ctx_vec

    # specificity: IC percentile WITHIN this result set (not global)
    ic_vals = np.array([ic_precomputed.get(c, 0.0) for c in embedded_cuis])
    ic_min, ic_max = ic_vals.min(), ic_vals.max()
    ic_range = ic_max - ic_min if ic_max > ic_min else 1.0
    ic_norm = (ic_vals - ic_min) / ic_range

    # explanation power: how many nearby CUIs does each one "cover"?
    # use per-CUI count of neighbors above median similarity
    median_sim = np.median(sim_matrix[sim_matrix > 0])
    explain_counts = (sim_matrix > median_sim).sum(axis=1).astype(float)
    ec_max = explain_counts.max() if explain_counts.max() > 0 else 1.0
    explain_norm = explain_counts / ec_max

    # --- STEP 3: data-driven weights from variance ---
    # features that vary more across CUIs carry more signal
    features = np.column_stack([alignments, ic_norm, explain_norm])
    variances = features.var(axis=0)
    variances = variances / (variances.sum() + 1e-10)  # normalize to sum=1
    print(f"Data-derived weights: alignment={variances[0]:.3f}, "
          f"specificity={variances[1]:.3f}, explanation={variances[2]:.3f}")

    # weighted score
    scores = features @ variances

    # --- STEP 4: build output ---
    # audit-based explains map
    explains = defaultdict(list)
    for entry in result.audit_trail:
        if entry.kept_cui and entry.removed_cui:
            explains[entry.kept_cui].append(entry.removed_cui)

    retention = {}
    for i, cui in enumerate(embedded_cuis):
        m = system.fetcher._cache.get(cui)
        if not m:
            continue
        retention[cui] = {
            "score": round(float(scores[i]), 4),
            "alignment": round(float(alignments[i]), 3),
            "specificity": round(float(ic_norm[i]), 3),
            "explain_power": round(float(explain_norm[i]), 3),
            "term": m.preferred_term,
            "semantic_types": m.semantic_types,
            "explains_cuis": explains.get(cui, []),
        }

    sorted_cuis = sorted(retention.keys(),
                          key=lambda c: retention[c]["score"], reverse=True)

    print(f"\nCore context from {k_core} densest CUIs out of {n}")
    print(f"Median similarity threshold: {median_sim:.3f}")
    print(f"\nTop 25 CUIs by retention score:")
    for c in sorted_cuis[:25]:
        r = retention[c]
        stys = ", ".join(r["semantic_types"][:2]) if r["semantic_types"] else "?"
        line = (f"  {c} | score={r['score']} "
                f"(align={r['alignment']}, spec={r['specificity']}, "
                f"expl={r['explain_power']}) "
                f"| [{stys}] {r['term']}")
        if r["explains_cuis"]:
            line += f" | explains {len(r['explains_cuis'])}"
        print(line)
