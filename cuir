"""
CUI Reduction System — Semantic Group + Tree + IC
==================================================
Reduces 20K+ extracted CUIs to a clean set for search and topics.

Pipeline:
  1. CUI extraction API → raw CUIs (count logged)
  2. SAB filter → keep target vocabularies (count logged)
  3. Semantic type grouping → query BQ MRSTY, separate CUIs into
     clinical categories (Diagnosis, Procedure, Finding, etc.)
  4. Per group: build full hierarchy (no depth limit) from
     SAB-filtered CUIs only
  5. Per group: cluster related CUIs via shared ancestry
  6. Per cluster: keep CUIs by IC — remove ancestors subsumed by
     more specific descendants, keep ALL specific CUIs (not just
     one representative)
  7. Ungroup all semantic groups → merge → deduplicate → final set

No tokenization. No embeddings. No hardcoded thresholds.
UMLS hierarchy + IC + semantic types = full signal.

Requirements:
  pip install google-cloud-bigquery networkx numpy requests
"""

import time
import threading
import logging
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque, OrderedDict
from enum import Enum
from functools import wraps

import numpy as np
import networkx as nx
import requests
import subprocess
from google.cloud import bigquery


# ========================= LOGGING =========================

_logger = logging.getLogger("cui_reduction")
if not _logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s"))
    _logger.addHandler(_handler)
    _logger.setLevel(logging.INFO)
    _logger.propagate = False


def log(msg: str, level: str = "INFO"):
    getattr(_logger, level.lower(), _logger.info)(msg)


# ========================= TIMING =========================

_TIMING_BUFFER_SIZE = 1000
_timings: Dict[str, deque] = defaultdict(
    lambda: deque(maxlen=_TIMING_BUFFER_SIZE))
_timing_lock = threading.Lock()


def timed(name: str):
    def dec(fn):
        @wraps(fn)
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                with _timing_lock:
                    _timings[name].append(
                        (time.perf_counter() - t0) * 1000)
        return wrapper
    return dec


def get_timings() -> Dict[str, Dict[str, float]]:
    with _timing_lock:
        out = {}
        for k, v in _timings.items():
            if v:
                vals = list(v)
                out[k] = {
                    "count": len(vals),
                    "mean_ms": float(np.mean(vals)),
                    "p50_ms": float(np.median(vals)),
                    "p99_ms": float(np.percentile(vals, 99)),
                    "max_ms": float(np.max(vals)),
                }
        return out


# ========================= CACHE =========================

_CACHE_HARD_CAP = 200_000
_cache_total = 0
_cache_total_lock = threading.Lock()
_CACHE_GLOBAL_CAP = 1_000_000


class Cache:
    """Thread-safe LRU with hard memory caps."""

    def __init__(self, max_size: int):
        global _cache_total
        self._d: OrderedDict = OrderedDict()
        self._lock = threading.RLock()
        capped = max(1, min(max_size, _CACHE_HARD_CAP))
        with _cache_total_lock:
            remaining = max(1, _CACHE_GLOBAL_CAP - _cache_total)
            self._max = min(capped, remaining)
            _cache_total += self._max

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
            return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# ========================= DATA MODELS =========================

@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]       # e.g. ["Disease or Syndrome"]
    semantic_type_ids: List[str]    # e.g. ["T047"]
    ic_score: float
    source_vocabs: List[str] = field(default_factory=list)


@dataclass
class CUIAssessment:
    cui: str
    preferred_term: str
    specificity: float              # IC / max_IC in set
    semantic_group: str             # which group this came from
    cluster_id: str = ""
    retained_cuis: List[str] = field(default_factory=list)
    explanation: str = ""


@dataclass
class ReductionResult:
    context_string: str
    input_count: int
    after_sab_count: int
    semantic_groups: Dict[str, int]         # group_name → count
    cluster_counts: Dict[str, int]          # group_name → cluster count
    reduced_cuis: List[str]
    assessments: Dict[str, CUIAssessment]
    removed: Dict[str, str]
    processing_time_ms: float


# ========================= SEMANTIC GROUP MAPPING =========================
# Maps TUI prefixes to clinical categories.
# Derived from UMLS Semantic Network — not hardcoded thresholds,
# these are structural ontology categories.

_SEMANTIC_GROUPS = {
    # Disorders
    "T019": "Diagnosis", "T020": "Diagnosis", "T037": "Diagnosis",
    "T046": "Diagnosis", "T047": "Diagnosis", "T048": "Diagnosis",
    "T049": "Diagnosis", "T050": "Diagnosis", "T190": "Diagnosis",
    "T191": "Diagnosis",   # Neoplastic Process (cancer)
    # Procedures
    "T058": "Procedure", "T059": "Procedure", "T060": "Procedure",
    "T061": "Procedure",
    # Findings
    "T033": "Finding", "T034": "Finding", "T184": "Finding",
    # Anatomy
    "T017": "Anatomy", "T021": "Anatomy", "T022": "Anatomy",
    "T023": "Anatomy", "T024": "Anatomy", "T025": "Anatomy",
    "T026": "Anatomy", "T029": "Anatomy", "T030": "Anatomy",
    # Chemicals & Drugs
    "T109": "Substance", "T110": "Substance", "T114": "Substance",
    "T115": "Substance", "T116": "Substance", "T118": "Substance",
    "T119": "Substance", "T121": "Substance", "T122": "Substance",
    "T123": "Substance", "T124": "Substance", "T125": "Substance",
    "T126": "Substance", "T127": "Substance", "T129": "Substance",
    "T130": "Substance", "T131": "Substance", "T195": "Substance",
    "T196": "Substance", "T197": "Substance", "T200": "Substance",
    # Physiology
    "T032": "Physiology", "T039": "Physiology", "T040": "Physiology",
    "T041": "Physiology", "T042": "Physiology", "T043": "Physiology",
    "T044": "Physiology", "T045": "Physiology",
    # Living Beings
    "T004": "Organism", "T005": "Organism", "T007": "Organism",
    "T008": "Organism", "T010": "Organism", "T011": "Organism",
    "T012": "Organism", "T013": "Organism", "T014": "Organism",
    "T015": "Organism", "T016": "Organism",
    # Concepts
    "T078": "Concept", "T079": "Concept", "T080": "Concept",
    "T081": "Concept", "T082": "Concept", "T089": "Concept",
    "T102": "Concept", "T169": "Concept", "T170": "Concept",
    "T171": "Concept", "T185": "Concept",
    # Objects & Devices
    "T073": "Object", "T074": "Object", "T075": "Object",
}


def get_semantic_group(tuis: List[str]) -> str:
    """Map a list of TUIs to a semantic group. First match wins."""
    for tui in tuis:
        if tui in _SEMANTIC_GROUPS:
            return _SEMANTIC_GROUPS[tui]
    return "Other"


# ========================= HIERARCHY CLIENT =========================

class HierarchyClient:
    """Navigates UMLS hierarchy. No depth limits."""

    def __init__(self, network: nx.DiGraph,
                 ic_scores: Optional[Dict[str, float]] = None,
                 cache_size: int = 1):
        self.network = network
        self.ic_scores = ic_scores or {}
        self._ic_cache = Cache(cache_size)
        self._parent_cache = Cache(cache_size)
        self._children_cache = Cache(cache_size)
        log(f"Hierarchy: {network.number_of_nodes()} nodes, "
            f"{network.number_of_edges()} edges")

    def get_ic(self, cui: str) -> float:
        cached = self._ic_cache.get(cui)
        if cached is not None:
            return cached
        if cui in self.ic_scores:
            ic = float(self.ic_scores[cui])
        else:
            ic = float(np.log1p(self._depth_to_root(cui)))
        if not np.isfinite(ic):
            ic = 0.0
        self._ic_cache.put(cui, ic)
        return ic

    def _depth_to_root(self, cui: str) -> int:
        """Full BFS to root — no depth limit."""
        if not self.network.has_node(cui):
            return 0
        visited, q, depth = {cui}, deque([cui]), 0
        while q:
            nxt = []
            for _ in range(len(q)):
                for p in self.get_parents(q.popleft()):
                    if p not in visited:
                        visited.add(p)
                        nxt.append(p)
            if not nxt:
                break
            q.extend(nxt)
            depth += 1
        return depth

    def get_parents(self, cui: str) -> List[str]:
        cached = self._parent_cache.get(cui)
        if cached is not None:
            return cached
        parents = (list(self.network.predecessors(cui))
                   if self.network.has_node(cui) else [])
        self._parent_cache.put(cui, parents)
        return parents

    def get_children(self, cui: str) -> List[str]:
        cached = self._children_cache.get(cui)
        if cached is not None:
            return cached
        children = (list(self.network.successors(cui))
                    if self.network.has_node(cui) else [])
        self._children_cache.put(cui, children)
        return children

    def find_ancestors_in_set(self, cui: str,
                              cui_set: Set[str]) -> List[str]:
        """BFS up — no depth limit — find ancestors in cui_set."""
        found = []
        visited = {cui}
        q = deque()
        for p in self.get_parents(cui):
            if p not in visited:
                visited.add(p)
                q.append(p)
        while q:
            node = q.popleft()
            if node in cui_set:
                found.append(node)
            for p in self.get_parents(node):
                if p not in visited:
                    visited.add(p)
                    q.append(p)
        return found


# ========================= METADATA FETCHER =========================

_BQ_BATCH_SIZE = 5000


class MetadataFetcher:
    """Batched BQ fetch — gets preferred term, semantic types, SABs."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 hierarchy: HierarchyClient, cache_size: int):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.hierarchy = hierarchy
        self._cache = Cache(cache_size)

    @timed("metadata_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, CUIMetadata]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH_SIZE):
            batch = missing[i:i + _BQ_BATCH_SIZE]
            query = f"""
            SELECT c.CUI AS cui,
              ANY_VALUE(CASE WHEN c.ISPREF = 'Y' THEN c.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT s.STY IGNORE NULLS) AS stys,
              ARRAY_AGG(DISTINCT c.SAB IGNORE NULLS) AS sabs
            FROM `{self.pid}.{self.did}.MRCONSO` c
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s ON c.CUI = s.CUI
            WHERE c.CUI IN UNNEST(@cuis) AND c.LAT = 'ENG'
            GROUP BY c.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            try:
                for row in self.bq.query(
                        query, job_config=jc, timeout=300).result():
                    meta = CUIMetadata(
                        cui=row.cui,
                        preferred_term=row.pref_term or "",
                        semantic_types=row.stys or [],
                        semantic_type_ids=row.tuis or [],
                        ic_score=self.hierarchy.get_ic(row.cui),
                        source_vocabs=row.sabs or [],
                    )
                    self._cache.put(row.cui, meta)
                    result[row.cui] = meta
            except Exception as e:
                log(f"Metadata fetch error (batch {i}): {e}", "ERROR")

        return result


# ========================= SAB FILTER =========================

class SABFilter:
    def __init__(self, allowed_sabs: Set[str]):
        self.allowed = allowed_sabs

    @timed("sab_filter")
    def run(self, cuis: List[str],
            metadata: Dict[str, CUIMetadata]
            ) -> Tuple[List[str], Dict[str, str]]:
        kept, removed = [], {}
        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                removed[cui] = "no_metadata"
            elif not (set(meta.source_vocabs) & self.allowed):
                removed[cui] = (f"sab_not_allowed "
                                f"(has {meta.source_vocabs})")
            else:
                kept.append(cui)
        return kept, removed


# ========================= SEMANTIC GROUPER =========================

class SemanticGrouper:
    """Groups CUIs by semantic type category from BQ metadata."""

    @staticmethod
    @timed("semantic_grouping")
    def group(cuis: List[str],
              metadata: Dict[str, CUIMetadata]
              ) -> Dict[str, List[str]]:
        """Returns {group_name: [cuis]}. A CUI goes into its
        primary semantic group based on TUI mapping."""
        groups: Dict[str, List[str]] = defaultdict(list)
        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                continue
            grp = get_semantic_group(meta.semantic_type_ids)
            groups[grp].append(cui)
        return dict(groups)


# ========================= UNION-FIND =========================

class _UnionFind:
    """Path-compressed union-find for clustering."""

    def __init__(self, items):
        self._parent = {x: x for x in items}
        self._rank = {x: 0 for x in items}

    def find(self, x):
        while self._parent[x] != x:
            self._parent[x] = self._parent[self._parent[x]]
            x = self._parent[x]
        return x

    def union(self, a, b):
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return
        if self._rank[ra] < self._rank[rb]:
            ra, rb = rb, ra
        self._parent[rb] = ra
        if self._rank[ra] == self._rank[rb]:
            self._rank[ra] += 1

    def groups(self) -> Dict[str, List[str]]:
        clusters: Dict[str, List[str]] = defaultdict(list)
        for item in self._parent:
            clusters[self.find(item)].append(item)
        return dict(clusters)


# ========================= CLUSTER + IC SELECT =========================

class ClusterAndSelect:
    """
    Per semantic group:
      1. Build clusters via shared ancestry (union-find)
      2. Within each cluster, remove ancestors that are subsumed
         by more specific descendants (higher IC)
      3. Keep ALL surviving CUIs — not just one representative

    "Subsumed" means: if child has IC >= ancestor IC and child
    is a descendant of ancestor in the hierarchy, ancestor is
    redundant within this cluster.
    """

    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy

    @timed("cluster_and_select")
    def run(
        self,
        cuis: List[str],
        metadata: Dict[str, CUIMetadata],
        group_name: str,
    ) -> Tuple[List[str], Dict[str, List[str]], Dict[str, str], int]:
        """
        Returns:
          surviving: CUIs kept from this group
          retention_map: surviving_cui → [ancestors it subsumes]
          removed: removed_cui → explanation
          cluster_count: number of clusters formed
        """
        if not cuis:
            return [], {}, {}, 0

        # --- Step 1: Cluster via hierarchy ---
        cui_set = set(cuis)
        uf = _UnionFind(cuis)

        parent_to_children: Dict[str, List[str]] = defaultdict(list)
        for cui in cuis:
            for p in self.hierarchy.get_parents(cui):
                if p in cui_set:
                    uf.union(cui, p)
                parent_to_children[p].append(cui)

        # Siblings (shared parent in FULL graph) → same cluster
        for p, children in parent_to_children.items():
            if len(children) > 1:
                first = children[0]
                for other in children[1:]:
                    uf.union(first, other)

        clusters = uf.groups()

        # --- Step 2: Within each cluster, IC-based selection ---
        all_surviving: List[str] = []
        retention_map: Dict[str, List[str]] = {}
        all_removed: Dict[str, str] = {}

        for _, members in clusters.items():
            member_set = set(members)
            ancestor_retainers: Dict[str, Set[str]] = defaultdict(set)

            for cui in members:
                meta = metadata.get(cui)
                if not meta:
                    continue
                ancestors = self.hierarchy.find_ancestors_in_set(
                    cui, member_set)
                for anc in ancestors:
                    anc_meta = metadata.get(anc)
                    if anc_meta and meta.ic_score >= anc_meta.ic_score:
                        ancestor_retainers[anc].add(cui)

            to_remove = set(ancestor_retainers.keys())
            surviving = [c for c in members if c not in to_remove]
            surviving_set = set(surviving)

            all_surviving.extend(surviving)

            for cui in surviving:
                retention_map[cui] = []
            for anc, retainers in ancestor_retainers.items():
                for r in retainers:
                    if r in surviving_set:
                        retention_map[r].append(anc)

            for anc, retainers in ancestor_retainers.items():
                best_retainers = [r for r in retainers
                                  if r in surviving_set]
                if best_retainers:
                    best = max(best_retainers,
                               key=lambda c: (metadata[c].ic_score
                                              if c in metadata else 0))
                    b = metadata.get(best)
                    a = metadata.get(anc)
                    all_removed[anc] = (
                        f"[{group_name}] subsumed by {best} "
                        f"'{b.preferred_term if b else '?'}' "
                        f"(IC {b.ic_score:.2f if b else 0} >= "
                        f"{a.ic_score:.2f if a else 0})")
                else:
                    all_removed[anc] = (
                        f"[{group_name}] subsumed by descendant")

        return all_surviving, retention_map, all_removed, len(clusters)


# ========================= MAIN SYSTEM =========================

class CUIReductionSystem:
    """
    Production CUI reduction pipeline.

    Usage:
        system = CUIReductionSystem(project_id, dataset_id,
                                     network, allowed_sabs)
        result = system.reduce(cuis, context_string)
    """

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        network: nx.DiGraph,
        allowed_sabs: List[str],
        ic_scores: Optional[Dict[str, float]] = None,
    ):
        self.bq = bigquery.Client(project=project_id)
        cache_size = network.number_of_nodes() * 2

        self.hierarchy = HierarchyClient(network, ic_scores, cache_size)
        self.fetcher = MetadataFetcher(
            self.bq, project_id, dataset_id,
            self.hierarchy, cache_size)
        self.sab_filter = SABFilter(set(allowed_sabs))
        self.selector = ClusterAndSelect(self.hierarchy)

        log(f"System ready | sabs={allowed_sabs}")

    @timed("full_pipeline")
    def reduce(
        self,
        cuis: List[str],
        context_string: str,
    ) -> ReductionResult:

        t0 = time.perf_counter()
        all_removed: Dict[str, str] = {}

        if not cuis:
            return self._empty(context_string, cuis, t0)

        # ---- Step 1: Metadata ----
        metadata = self.fetcher.fetch(cuis)

        # ---- Step 2: SAB filter ----
        after_sab, sab_rm = self.sab_filter.run(cuis, metadata)
        all_removed.update(sab_rm)
        log(f"  SAB filter: {len(cuis)} -> {len(after_sab)}")

        if not after_sab:
            return self._empty(context_string, cuis, t0,
                               removed=all_removed,
                               after_sab=0)

        # ---- Step 3: Semantic type grouping ----
        sab_meta = {c: metadata[c] for c in after_sab if c in metadata}
        sem_groups = SemanticGrouper.group(after_sab, sab_meta)
        group_counts = {g: len(c) for g, c in sem_groups.items()}
        log(f"  Semantic groups: {group_counts}")

        # ---- Step 4 + 5 + 6: Per group — hierarchy, cluster, IC select ----
        all_surviving: List[str] = []
        all_retention: Dict[str, List[str]] = {}
        cluster_counts: Dict[str, int] = {}
        all_assessments: Dict[str, CUIAssessment] = {}

        for group_name, group_cuis in sem_groups.items():
            surviving, rmap, removed, n_clusters = self.selector.run(
                group_cuis, metadata, group_name)

            all_surviving.extend(surviving)
            all_retention.update(rmap)
            all_removed.update(removed)
            cluster_counts[group_name] = n_clusters

            log(f"    {group_name}: {len(group_cuis)} -> "
                f"{len(surviving)} ({n_clusters} clusters, "
                f"{len(removed)} subsumed)")

            # Build assessments for surviving CUIs from this group
            group_ics = [metadata[c].ic_score
                         for c in surviving if c in metadata]
            max_ic = max(group_ics) if group_ics else 1.0
            if max_ic <= 0 or not np.isfinite(max_ic):
                max_ic = 1.0

            for cui in surviving:
                meta = metadata.get(cui)
                if not meta:
                    continue
                spec = meta.ic_score / max_ic
                all_assessments[cui] = CUIAssessment(
                    cui=cui,
                    preferred_term=meta.preferred_term,
                    specificity=spec if np.isfinite(spec) else 0.0,
                    semantic_group=group_name,
                    retained_cuis=rmap.get(cui, []),
                )

        # ---- Step 7: Ungroup and deduplicate ----
        seen = set()
        reduced: List[str] = []
        for cui in all_surviving:
            if cui not in seen:
                seen.add(cui)
                reduced.append(cui)

        # Sort by specificity
        reduced.sort(
            key=lambda c: (all_assessments[c].specificity
                           if c in all_assessments else 0),
            reverse=True)

        # Generate explanations
        for cui in reduced:
            a = all_assessments.get(cui)
            if not a:
                continue
            meta = metadata.get(cui)
            parts = [
                f"Group: {a.semantic_group}",
                f"IC ratio: {a.specificity:.2f}",
            ]
            if meta:
                parts.append(f"Types: {meta.semantic_types}")
            if a.retained_cuis:
                n = len(a.retained_cuis)
                names = [metadata[r].preferred_term
                         for r in a.retained_cuis[:3]
                         if r in metadata]
                extra = f" +{n-3} more" if n > 3 else ""
                parts.append(f"Subsumes: {names}{extra}")
            a.explanation = " | ".join(parts)

        elapsed = (time.perf_counter() - t0) * 1000
        log(f"  Final: {len(cuis)} -> {len(reduced)} "
            f"({elapsed:.0f}ms)")

        return ReductionResult(
            context_string=context_string,
            input_count=len(cuis),
            after_sab_count=len(after_sab),
            semantic_groups=group_counts,
            cluster_counts=cluster_counts,
            reduced_cuis=reduced,
            assessments={c: all_assessments[c] for c in reduced
                         if c in all_assessments},
            removed=all_removed,
            processing_time_ms=elapsed,
        )

    @staticmethod
    def _empty(ctx, cuis, t0, removed=None, after_sab=0):
        return ReductionResult(
            context_string=ctx,
            input_count=len(cuis),
            after_sab_count=after_sab,
            semantic_groups={},
            cluster_counts={},
            reduced_cuis=[],
            assessments={},
            removed=removed or {},
            processing_time_ms=(time.perf_counter() - t0) * 1000,
        )

    def get_stats(self) -> Dict:
        return get_timings()


# ========================= CUI EXTRACTOR =========================

class CUIExtractor:
    """Calls CUI extraction API. Returns deduplicated CUI list."""

    def __init__(self, api_url: str):
        self.url = api_url
        self.session = requests.Session()
        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30,
            ).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text: str) -> List[str]:
        try:
            resp = self.session.post(
                self.url, headers=self.headers,
                json={"query_texts": [text], "top_k": 3},
                timeout=200,
            )
            resp.raise_for_status()
            data = resp.json()
            if not isinstance(data, dict):
                return []

            cuis = []
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
            return list(set(cuis))
        except Exception as e:
            log(f"Extraction failed: {e}", "ERROR")
            return []


# ========================= MAIN =========================

def main():
    """Run CUI reduction for clinical texts."""
    import pickle

    # ---- Configuration (replace with your values) ----
    PROJECT_ID = "your-project-id"
    DATASET_ID = "your-dataset"
    API_URL = "https://your-api/extract"
    NETWORK_PATH = "networkx_cui_context_v1_1_0.pkl"
    ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM",
                     "SNOMEDCT_US", "LOINC"]

    # ---- Load hierarchy ----
    log("Loading hierarchy...")
    with open(NETWORK_PATH, "rb") as f:
        network = pickle.load(f)
    log(f"Loaded: {network.number_of_nodes()} nodes, "
        f"{network.number_of_edges()} edges")

    # ---- Initialize ----
    system = CUIReductionSystem(
        PROJECT_ID, DATASET_ID, network, ALLOWED_SABS)
    extractor = CUIExtractor(API_URL)

    # ---- Process texts ----
    texts = [
        "Patient has severe pain in left knee with swelling",
        "History of type 2 diabetes mellitus with peripheral neuropathy",
        "Acute exacerbation of chronic obstructive pulmonary disease",
    ]

    for text in texts:
        log(f"\n{'='*60}")
        log(f"TEXT: {text}")

        cuis = extractor.extract(text)
        if not cuis:
            log("  No CUIs extracted")
            continue

        result = system.reduce(cuis, text)

        log(f"  Result: {len(result.reduced_cuis)} reduced CUIs")
        log(f"  Validation: {len(result.removed)} removed, "
            f"{'CLEAN' if len(result.reduced_cuis) > 0 else 'EMPTY'}")

    log(f"\n{'='*60}")
    log("PERFORMANCE:")
    for name, stats in system.get_stats().items():
        log(f"  {name}: mean={stats['mean_ms']:.1f}ms")


if __name__ == "__main__":
    main()
