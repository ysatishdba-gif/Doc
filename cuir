"""
CUI Reduction System — Semantic Group + Tree + IC + Embedding Sub-Clusters
==========================================================================
Reduces 20K+ extracted CUIs and organizes into topics.

Pipeline:
  1. CUI extraction API → raw CUIs
  2. SAB filter → keep target vocabularies
  3. Semantic type grouping → separate Diagnosis, Procedure, Finding, etc.
  4. Per group: cluster related CUIs via shared ancestry (union-find)
  5. Per cluster: remove ancestors subsumed by higher-IC descendants
     ** only two removal points: step 2 and step 5 **
  6. Fetch pre-computed embeddings for surviving CUIs (one BQ query)
  7. Per cluster: sub-cluster survivors by embedding cosine similarity
     using agglomerative clustering (no CUIs removed, purely organizational)
  8. Each sub-cluster = one topic containing its CUIs
  9. Ungroup all → deduplicate → final output

Output:
  topics: {topic_id: [cui1, cui2, ...]}
  All CUIs across all topics = reduced CUIs for search

Requirements:
  pip install google-cloud-bigquery networkx numpy scipy requests
"""

import time
import threading
import logging
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque, OrderedDict
from functools import wraps

import numpy as np
import networkx as nx
import requests
import subprocess
from google.cloud import bigquery
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform


# ========================= LOGGING =========================

_logger = logging.getLogger("cui_reduction")
if not _logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s"))
    _logger.addHandler(_handler)
    _logger.setLevel(logging.INFO)
    _logger.propagate = False


def log(msg: str, level: str = "INFO"):
    getattr(_logger, level.lower(), _logger.info)(msg)


# ========================= TIMING =========================

_TIMING_BUFFER_SIZE = 1000
_timings: Dict[str, deque] = defaultdict(
    lambda: deque(maxlen=_TIMING_BUFFER_SIZE))
_timing_lock = threading.Lock()


def timed(name: str):
    def dec(fn):
        @wraps(fn)
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                with _timing_lock:
                    _timings[name].append(
                        (time.perf_counter() - t0) * 1000)
        return wrapper
    return dec


def get_timings() -> Dict[str, Dict[str, float]]:
    with _timing_lock:
        out = {}
        for k, v in _timings.items():
            if v:
                vals = list(v)
                out[k] = {
                    "count": len(vals),
                    "mean_ms": float(np.mean(vals)),
                    "p50_ms": float(np.median(vals)),
                    "p99_ms": float(np.percentile(vals, 99)),
                    "max_ms": float(np.max(vals)),
                }
        return out


# ========================= CACHE =========================

_CACHE_HARD_CAP = 200_000
_cache_total = 0
_cache_total_lock = threading.Lock()
_CACHE_GLOBAL_CAP = 1_000_000


class Cache:
    """Thread-safe LRU with hard memory caps."""

    def __init__(self, max_size: int):
        global _cache_total
        self._d: OrderedDict = OrderedDict()
        self._lock = threading.RLock()
        capped = max(1, min(max_size, _CACHE_HARD_CAP))
        with _cache_total_lock:
            remaining = max(1, _CACHE_GLOBAL_CAP - _cache_total)
            self._max = min(capped, remaining)
            _cache_total += self._max

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
            return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# ========================= DATA MODELS =========================

@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]       # e.g. ["Disease or Syndrome"]
    semantic_type_ids: List[str]    # e.g. ["T047"]
    ic_score: float
    source_vocabs: List[str] = field(default_factory=list)


@dataclass
class TopicInfo:
    """One topic = one sub-cluster of semantically similar CUIs."""
    topic_id: str
    semantic_group: str
    cuis: List[str]
    cui_terms: Dict[str, str]   # cui → preferred_term


@dataclass
class ReductionResult:
    context_string: str
    input_count: int
    after_sab_count: int
    semantic_groups: Dict[str, int]     # group → count before reduction
    reduced_count: int
    topics: Dict[str, TopicInfo]        # topic_id → TopicInfo
    all_reduced_cuis: List[str]         # flat list, deduplicated
    removed: Dict[str, str]             # cui → removal reason
    processing_time_ms: float


# ========================= HIERARCHY CLIENT =========================

class HierarchyClient:
    """Navigates UMLS hierarchy. No depth limits."""

    def __init__(self, network: nx.DiGraph,
                 ic_scores: Optional[Dict[str, float]] = None,
                 cache_size: int = 1):
        self.network = network
        self.ic_scores = ic_scores or {}
        self._ic_cache = Cache(cache_size)
        self._parent_cache = Cache(cache_size)
        self._children_cache = Cache(cache_size)
        log(f"Hierarchy: {network.number_of_nodes()} nodes, "
            f"{network.number_of_edges()} edges")

    def get_ic(self, cui: str) -> float:
        cached = self._ic_cache.get(cui)
        if cached is not None:
            return cached
        if cui in self.ic_scores:
            ic = float(self.ic_scores[cui])
        else:
            ic = float(np.log1p(self._depth_to_root(cui)))
        if not np.isfinite(ic):
            ic = 0.0
        self._ic_cache.put(cui, ic)
        return ic

    def _depth_to_root(self, cui: str) -> int:
        if not self.network.has_node(cui):
            return 0
        visited, q, depth = {cui}, deque([cui]), 0
        while q:
            nxt = []
            for _ in range(len(q)):
                for p in self.get_parents(q.popleft()):
                    if p not in visited:
                        visited.add(p)
                        nxt.append(p)
            if not nxt:
                break
            q.extend(nxt)
            depth += 1
        return depth

    def get_parents(self, cui: str) -> List[str]:
        cached = self._parent_cache.get(cui)
        if cached is not None:
            return cached
        parents = (list(self.network.predecessors(cui))
                   if self.network.has_node(cui) else [])
        self._parent_cache.put(cui, parents)
        return parents

    def get_children(self, cui: str) -> List[str]:
        cached = self._children_cache.get(cui)
        if cached is not None:
            return cached
        children = (list(self.network.successors(cui))
                    if self.network.has_node(cui) else [])
        self._children_cache.put(cui, children)
        return children

    def find_ancestors_in_set(self, cui: str,
                              cui_set: Set[str]) -> List[str]:
        """BFS up — no depth limit — find ancestors in cui_set."""
        found = []
        visited = {cui}
        q = deque()
        for p in self.get_parents(cui):
            if p not in visited:
                visited.add(p)
                q.append(p)
        while q:
            node = q.popleft()
            if node in cui_set:
                found.append(node)
            for p in self.get_parents(node):
                if p not in visited:
                    visited.add(p)
                    q.append(p)
        return found


# ========================= METADATA FETCHER =========================

_BQ_BATCH_SIZE = 5000


class MetadataFetcher:
    """Batched BQ fetch — preferred term, semantic types, SABs."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 hierarchy: HierarchyClient, cache_size: int):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.hierarchy = hierarchy
        self._cache = Cache(cache_size)

    @timed("metadata_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, CUIMetadata]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH_SIZE):
            batch = missing[i:i + _BQ_BATCH_SIZE]
            query = f"""
            SELECT c.CUI AS cui,
              ANY_VALUE(CASE WHEN c.ISPREF = 'Y' THEN c.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT s.STY IGNORE NULLS) AS stys,
              ARRAY_AGG(DISTINCT c.SAB IGNORE NULLS) AS sabs
            FROM `{self.pid}.{self.did}.MRCONSO` c
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s ON c.CUI = s.CUI
            WHERE c.CUI IN UNNEST(@cuis) AND c.LAT = 'ENG'
            GROUP BY c.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            try:
                for row in self.bq.query(
                        query, job_config=jc, timeout=300).result():
                    meta = CUIMetadata(
                        cui=row.cui,
                        preferred_term=row.pref_term or "",
                        semantic_types=row.stys or [],
                        semantic_type_ids=row.tuis or [],
                        ic_score=self.hierarchy.get_ic(row.cui),
                        source_vocabs=row.sabs or [],
                    )
                    self._cache.put(row.cui, meta)
                    result[row.cui] = meta
            except Exception as e:
                log(f"Metadata fetch error (batch {i}): {e}", "ERROR")

        return result


# ========================= EMBEDDING FETCHER =========================

class EmbeddingFetcher:
    """Fetches pre-computed CUI embeddings from BQ."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 embedding_table: str):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.embedding_table = embedding_table
        self._cache = Cache(50_000)

    @timed("embedding_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, np.ndarray]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH_SIZE):
            batch = missing[i:i + _BQ_BATCH_SIZE]
            query = f"""
            SELECT cui, embedding
            FROM `{self.pid}.{self.did}.{self.embedding_table}`
            WHERE cui IN UNNEST(@cuis)
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            try:
                for row in self.bq.query(
                        query, job_config=jc, timeout=300).result():
                    vec = np.array(row.embedding, dtype=np.float32)
                    norm = np.linalg.norm(vec)
                    if norm > 0:
                        vec = vec / norm
                    self._cache.put(row.cui, vec)
                    result[row.cui] = vec
            except Exception as e:
                log(f"Embedding fetch error (batch {i}): {e}", "ERROR")

        return result


# ========================= SAB FILTER =========================

class SABFilter:
    def __init__(self, allowed_sabs: Set[str]):
        self.allowed = allowed_sabs

    @timed("sab_filter")
    def run(self, cuis: List[str],
            metadata: Dict[str, CUIMetadata]
            ) -> Tuple[List[str], Dict[str, str]]:
        kept, removed = [], {}
        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                removed[cui] = "no_metadata"
            elif not (set(meta.source_vocabs) & self.allowed):
                removed[cui] = (f"sab_not_allowed "
                                f"(has {meta.source_vocabs})")
            else:
                kept.append(cui)
        return kept, removed


# ========================= SEMANTIC GROUPER =============================
# No hardcoded mapping. Semantic type names (STY) come directly from
# BigQuery MRSTY table via metadata. Each CUI's first semantic type
# name becomes its group. Examples from BQ: "Disease or Syndrome",
# "Therapeutic or Preventive Procedure", "Finding", "Body Part, Organ,
# or Organ Component", etc.

class SemanticGrouper:
    @staticmethod
    @timed("semantic_grouping")
    def group(cuis: List[str],
              metadata: Dict[str, CUIMetadata]
              ) -> Dict[str, List[str]]:
        """Groups CUIs by their first semantic type name from BQ MRSTY."""
        groups: Dict[str, List[str]] = defaultdict(list)
        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                continue
            # Use first semantic type name directly from BQ
            grp = (meta.semantic_types[0]
                   if meta.semantic_types else "Unknown")
            groups[grp].append(cui)
        return dict(groups)


# ========================= UNION-FIND =========================

class _UnionFind:
    def __init__(self, items):
        self._parent = {x: x for x in items}
        self._rank = {x: 0 for x in items}

    def find(self, x):
        while self._parent[x] != x:
            self._parent[x] = self._parent[self._parent[x]]
            x = self._parent[x]
        return x

    def union(self, a, b):
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return
        if self._rank[ra] < self._rank[rb]:
            ra, rb = rb, ra
        self._parent[rb] = ra
        if self._rank[ra] == self._rank[rb]:
            self._rank[ra] += 1

    def groups(self) -> Dict[str, List[str]]:
        clusters: Dict[str, List[str]] = defaultdict(list)
        for item in self._parent:
            clusters[self.find(item)].append(item)
        return dict(clusters)


# ========================= HIERARCHY CLUSTER + IC REMOVAL ===========

class HierarchyClusterAndRemove:
    """
    Per semantic group:
      1. Cluster via shared ancestry (union-find)
      2. Within each cluster, remove ancestors subsumed by
         higher-IC descendants
      3. Keep ALL surviving CUIs (not just representative)
    """

    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy

    @timed("hierarchy_cluster_remove")
    def run(
        self,
        cuis: List[str],
        metadata: Dict[str, CUIMetadata],
        group_name: str,
    ) -> Tuple[Dict[str, List[str]], Dict[str, str]]:
        """
        Returns:
          clusters: {cluster_id: [surviving cuis]}
          removed: {cui: reason}
        """
        if not cuis:
            return {}, {}

        cui_set = set(cuis)
        uf = _UnionFind(cuis)

        parent_to_children: Dict[str, List[str]] = defaultdict(list)
        for cui in cuis:
            for p in self.hierarchy.get_parents(cui):
                if p in cui_set:
                    uf.union(cui, p)
                parent_to_children[p].append(cui)

        for p, children in parent_to_children.items():
            if len(children) > 1:
                first = children[0]
                for other in children[1:]:
                    uf.union(first, other)

        raw_clusters = uf.groups()

        # IC-based ancestor removal within each cluster
        final_clusters: Dict[str, List[str]] = {}
        all_removed: Dict[str, str] = {}

        for cid, members in raw_clusters.items():
            member_set = set(members)
            ancestor_retainers: Dict[str, Set[str]] = defaultdict(set)

            for cui in members:
                meta = metadata.get(cui)
                if not meta:
                    continue
                ancestors = self.hierarchy.find_ancestors_in_set(
                    cui, member_set)
                for anc in ancestors:
                    anc_meta = metadata.get(anc)
                    if anc_meta and meta.ic_score >= anc_meta.ic_score:
                        ancestor_retainers[anc].add(cui)

            to_remove = set(ancestor_retainers.keys())
            surviving = [c for c in members if c not in to_remove]

            if surviving:
                final_clusters[f"{group_name}_{cid}"] = surviving

            # Removal explanations
            surviving_set = set(surviving)
            for anc, retainers in ancestor_retainers.items():
                best_retainers = [r for r in retainers
                                  if r in surviving_set]
                if best_retainers:
                    best = max(best_retainers,
                               key=lambda c: (metadata[c].ic_score
                                              if c in metadata else 0))
                    b = metadata.get(best)
                    a = metadata.get(anc)
                    all_removed[anc] = (
                        f"[{group_name}] subsumed by {best} "
                        f"'{b.preferred_term if b else '?'}' "
                        f"(IC {b.ic_score:.2f if b else 0} >= "
                        f"{a.ic_score:.2f if a else 0})")
                else:
                    all_removed[anc] = (
                        f"[{group_name}] subsumed by descendant")

        return final_clusters, all_removed


# ========================= EMBEDDING SUB-CLUSTERING ================

class EmbeddingSubClusterer:
    """
    Takes a cluster of surviving CUIs and sub-clusters them
    by embedding cosine similarity using agglomerative clustering.

    No CUIs are removed — this is purely organizational.

    Distance threshold is derived from the data:
      mean pairwise distance within the cluster.
    """

    def __init__(self, embedding_fetcher: EmbeddingFetcher):
        self.fetcher = embedding_fetcher

    @timed("embedding_subcluster")
    def subcluster_all(
        self,
        clusters: Dict[str, List[str]],
        embeddings: Dict[str, np.ndarray],
    ) -> Dict[str, List[str]]:
        """
        Input:  {cluster_id: [cuis]}
        Output: {topic_id: [cuis]}  (sub-clusters become topics)

        CUIs without embeddings stay in their own singleton topic.
        """
        topics: Dict[str, List[str]] = {}
        topic_counter = 0

        for cid, cuis in clusters.items():
            # Separate CUIs with and without embeddings
            with_emb = [c for c in cuis if c in embeddings]
            without_emb = [c for c in cuis if c not in embeddings]

            if len(with_emb) <= 1:
                # 0 or 1 CUI with embedding — one topic per cluster
                topic_id = f"topic_{topic_counter}"
                topics[topic_id] = cuis
                topic_counter += 1
                continue

            # Build embedding matrix
            emb_matrix = np.array([embeddings[c] for c in with_emb])

            # Cosine similarity → cosine distance
            # embeddings are pre-normalized, so dot product = cosine
            sim_matrix = emb_matrix @ emb_matrix.T
            np.clip(sim_matrix, -1, 1, out=sim_matrix)
            dist_matrix = 1.0 - sim_matrix
            np.fill_diagonal(dist_matrix, 0)
            # Fix floating point noise
            dist_matrix = np.maximum(dist_matrix, 0)

            # Derive threshold from data: mean pairwise distance
            n = len(with_emb)
            upper_tri = dist_matrix[np.triu_indices(n, k=1)]
            if len(upper_tri) == 0 or np.std(upper_tri) < 1e-9:
                # All identical — one topic
                topic_id = f"topic_{topic_counter}"
                topics[topic_id] = cuis
                topic_counter += 1
                continue

            threshold = float(np.mean(upper_tri))

            # Agglomerative clustering
            condensed = squareform(dist_matrix, checks=False)
            Z = linkage(condensed, method="average")
            labels = fcluster(Z, t=threshold, criterion="distance")

            # Group CUIs by sub-cluster label
            sub_groups: Dict[int, List[str]] = defaultdict(list)
            for cui, label in zip(with_emb, labels):
                sub_groups[label].append(cui)

            # Assign each sub-group as a topic
            for sub_cuis in sub_groups.values():
                topic_id = f"topic_{topic_counter}"
                topics[topic_id] = sub_cuis
                topic_counter += 1

            # CUIs without embeddings — each gets its own topic
            for cui in without_emb:
                topic_id = f"topic_{topic_counter}"
                topics[topic_id] = [cui]
                topic_counter += 1

        return topics


# ========================= MAIN SYSTEM =========================

class CUIReductionSystem:
    """
    Production CUI reduction pipeline.

    Usage:
        system = CUIReductionSystem(project_id, dataset_id,
                                     network, allowed_sabs,
                                     embedding_table)
        result = system.reduce(cuis, context_string)

        result.topics         → {topic_id: TopicInfo}
        result.all_reduced_cuis → flat deduplicated list for search
        result.removed        → {cui: reason}
    """

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        network: nx.DiGraph,
        allowed_sabs: List[str],
        embedding_table: str,
        ic_scores: Optional[Dict[str, float]] = None,
    ):
        self.bq = bigquery.Client(project=project_id)
        cache_size = network.number_of_nodes() * 2

        self.hierarchy = HierarchyClient(network, ic_scores, cache_size)
        self.fetcher = MetadataFetcher(
            self.bq, project_id, dataset_id,
            self.hierarchy, cache_size)
        self.sab_filter = SABFilter(set(allowed_sabs))
        self.cluster_remover = HierarchyClusterAndRemove(self.hierarchy)
        self.emb_fetcher = EmbeddingFetcher(
            self.bq, project_id, dataset_id, embedding_table)
        self.sub_clusterer = EmbeddingSubClusterer(self.emb_fetcher)

        log(f"System ready | sabs={allowed_sabs} | "
            f"embeddings={embedding_table}")

    @timed("full_pipeline")
    def reduce(
        self,
        cuis: List[str],
        context_string: str,
    ) -> ReductionResult:

        t0 = time.perf_counter()
        all_removed: Dict[str, str] = {}

        if not cuis:
            return self._empty(context_string, cuis, t0)

        # ---- Step 1: Metadata ----
        metadata = self.fetcher.fetch(cuis)

        # ---- Step 2: SAB filter ----
        after_sab, sab_rm = self.sab_filter.run(cuis, metadata)
        all_removed.update(sab_rm)
        log(f"  SAB filter: {len(cuis)} -> {len(after_sab)}")

        if not after_sab:
            return self._empty(context_string, cuis, t0,
                               removed=all_removed,
                               after_sab=0)

        # ---- Step 3: Semantic type grouping ----
        sab_meta = {c: metadata[c] for c in after_sab if c in metadata}
        sem_groups = SemanticGrouper.group(after_sab, sab_meta)
        group_counts = {g: len(c) for g, c in sem_groups.items()}
        log(f"  Semantic groups: {group_counts}")

        # ---- Step 4 + 5: Per group — hierarchy cluster + IC removal ----
        all_clusters: Dict[str, List[str]] = {}
        total_removed = 0

        for group_name, group_cuis in sem_groups.items():
            clusters, removed = self.cluster_remover.run(
                group_cuis, metadata, group_name)
            all_removed.update(removed)
            total_removed += len(removed)

            surviving = sum(len(c) for c in clusters.values())
            log(f"    {group_name}: {len(group_cuis)} -> "
                f"{surviving} ({len(clusters)} clusters, "
                f"{len(removed)} subsumed)")

            all_clusters.update(clusters)

        # Collect all surviving CUIs
        all_surviving = []
        for cluster_cuis in all_clusters.values():
            all_surviving.extend(cluster_cuis)
        log(f"  After IC removal: {len(after_sab)} -> "
            f"{len(all_surviving)}")

        if not all_surviving:
            return self._empty(context_string, cuis, t0,
                               removed=all_removed,
                               after_sab=len(after_sab))

        # ---- Step 6: Fetch embeddings for survivors only ----
        embeddings = self.emb_fetcher.fetch(all_surviving)
        emb_coverage = len(embeddings) / len(all_surviving)
        log(f"  Embeddings: {len(embeddings)}/{len(all_surviving)} "
            f"({emb_coverage:.0%} coverage)")

        # ---- Step 7: Sub-cluster by embedding similarity ----
        topic_cuis = self.sub_clusterer.subcluster_all(
            all_clusters, embeddings)
        log(f"  Sub-clusters: {len(all_clusters)} clusters -> "
            f"{len(topic_cuis)} topics")

        # ---- Step 8 + 9: Build topics and deduplicate ----
        # Map clusters back to semantic groups
        cluster_to_group: Dict[str, str] = {}
        for group_name in sem_groups:
            for cid in all_clusters:
                if cid.startswith(group_name + "_"):
                    cluster_to_group[cid] = group_name

        topics: Dict[str, TopicInfo] = {}
        seen: Set[str] = set()
        all_reduced: List[str] = []

        for topic_id, t_cuis in topic_cuis.items():
            cui_terms = {}
            for c in t_cuis:
                m = metadata.get(c)
                cui_terms[c] = m.preferred_term if m else c
                if c not in seen:
                    seen.add(c)
                    all_reduced.append(c)

            # Semantic group from BQ metadata directly
            sem_grp = "Unknown"
            for c in t_cuis:
                m = metadata.get(c)
                if m and m.semantic_types:
                    sem_grp = m.semantic_types[0]
                    break

            topics[topic_id] = TopicInfo(
                topic_id=topic_id,
                semantic_group=sem_grp,
                cuis=t_cuis,
                cui_terms=cui_terms,
            )

        elapsed = (time.perf_counter() - t0) * 1000
        log(f"  Final: {len(cuis)} -> {len(all_reduced)} CUIs "
            f"in {len(topics)} topics ({elapsed:.0f}ms)")

        return ReductionResult(
            context_string=context_string,
            input_count=len(cuis),
            after_sab_count=len(after_sab),
            semantic_groups=group_counts,
            reduced_count=len(all_reduced),
            topics=topics,
            all_reduced_cuis=all_reduced,
            removed=all_removed,
            processing_time_ms=elapsed,
        )

    @staticmethod
    def _empty(ctx, cuis, t0, removed=None, after_sab=0):
        return ReductionResult(
            context_string=ctx,
            input_count=len(cuis),
            after_sab_count=after_sab,
            semantic_groups={},
            reduced_count=0,
            topics={},
            all_reduced_cuis=[],
            removed=removed or {},
            processing_time_ms=(time.perf_counter() - t0) * 1000,
        )

    def get_stats(self) -> Dict:
        return get_timings()


# ========================= CUI EXTRACTOR =========================

class CUIExtractor:
    """Calls CUI extraction API. Returns deduplicated CUI list."""

    def __init__(self, api_url: str):
        self.url = api_url
        self.session = requests.Session()
        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30,
            ).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text: str) -> List[str]:
        try:
            resp = self.session.post(
                self.url, headers=self.headers,
                json={"query_texts": [text], "top_k": 3},
                timeout=200,
            )
            resp.raise_for_status()
            data = resp.json()
            if not isinstance(data, dict):
                return []

            cuis = []
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
            return list(set(cuis))
        except Exception as e:
            log(f"Extraction failed: {e}", "ERROR")
            return []


# ========================= MAIN =========================

def main():
    """Run CUI reduction for clinical texts."""
    import pickle

    # ---- Configuration (replace with your values) ----
    PROJECT_ID = "your-project-id"
    DATASET_ID = "your-dataset"
    API_URL = "https://your-api/extract"
    NETWORK_PATH = "networkx_cui_context_v1_1_0.pkl"
    EMBEDDING_TABLE = "cui_embeddings"
    ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM",
                     "SNOMEDCT_US", "LOINC"]

    # ---- Load hierarchy ----
    log("Loading hierarchy...")
    with open(NETWORK_PATH, "rb") as f:
        network = pickle.load(f)
    log(f"Loaded: {network.number_of_nodes()} nodes, "
        f"{network.number_of_edges()} edges")

    # ---- Initialize ----
    system = CUIReductionSystem(
        PROJECT_ID, DATASET_ID, network, ALLOWED_SABS,
        EMBEDDING_TABLE)
    extractor = CUIExtractor(API_URL)

    # ---- Process texts ----
    texts = [
        "Patient has severe pain in left knee with swelling",
        "History of type 2 diabetes mellitus with peripheral neuropathy",
        "Acute exacerbation of chronic obstructive pulmonary disease",
    ]

    for text in texts:
        log(f"\n{'='*60}")
        log(f"TEXT: {text}")

        cuis = extractor.extract(text)
        if not cuis:
            log("  No CUIs extracted")
            continue

        result = system.reduce(cuis, text)
        log(f"  {len(result.topics)} topics, "
            f"{result.reduced_count} total CUIs")

    log(f"\n{'='*60}")
    log("PERFORMANCE:")
    for name, stats in system.get_stats().items():
        log(f"  {name}: mean={stats['mean_ms']:.1f}ms")


if __name__ == "__main__":
    main()
