"""
CUI Reduction System
====================
Reduces 20K+ extracted CUIs and organizes into topics.

Pipeline:
  1. CUI extraction API → raw CUIs
  2. SAB filter → keep target vocabularies
  3. Build sub-graph from NetworkX — edges only between SAB-filtered
     CUIs. No new CUIs introduced. Sparse tree reflecting real
     parent-child relationships within the extracted set.
  4. IC ancestor removal on sub-graph — if child IC >= parent IC,
     parent is redundant. Removes genuine hierarchy duplicates.
  5. Fetch pre-computed embeddings for survivors. Agglomerative
     clustering (average linkage, data-driven threshold) → topics.
  6. Within each topic: tight sub-clustering to find redundancy
     groups (CUIs that mean the same thing). Keep highest-IC
     member from each redundancy group.

Three removal points:
  Step 2: wrong vocabulary (SAB)
  Step 4: redundant ancestor (IC on sub-graph)
  Step 6: semantic duplicate (embedding + IC)

Output:
  topics: {topic_id: [cui1, cui2, ...]}
  All CUIs across all topics = reduced CUIs for search

Requirements:
  pip install google-cloud-bigquery networkx numpy scipy requests
"""

import time
import threading
import logging
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque, OrderedDict
from functools import wraps

import numpy as np
import networkx as nx
import requests
import subprocess
from google.cloud import bigquery
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform


# ========================= LOGGING =========================

_logger = logging.getLogger("cui_reduction")
if not _logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s"))
    _logger.addHandler(_handler)
    _logger.setLevel(logging.INFO)
    _logger.propagate = False


def log(msg: str, level: str = "INFO"):
    getattr(_logger, level.lower(), _logger.info)(msg)


# ========================= TIMING =========================

_TIMING_BUFFER_SIZE = 1000
_timings: Dict[str, deque] = defaultdict(
    lambda: deque(maxlen=_TIMING_BUFFER_SIZE))
_timing_lock = threading.Lock()


def timed(name: str):
    def dec(fn):
        @wraps(fn)
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                with _timing_lock:
                    _timings[name].append(
                        (time.perf_counter() - t0) * 1000)
        return wrapper
    return dec


def get_timings() -> Dict[str, Dict[str, float]]:
    with _timing_lock:
        out = {}
        for k, v in _timings.items():
            if v:
                vals = list(v)
                out[k] = {
                    "count": len(vals),
                    "mean_ms": float(np.mean(vals)),
                    "p50_ms": float(np.median(vals)),
                    "p99_ms": float(np.percentile(vals, 99)),
                    "max_ms": float(np.max(vals)),
                }
        return out


# ========================= CACHE =========================

_CACHE_HARD_CAP = 200_000
_cache_total = 0
_cache_total_lock = threading.Lock()
_CACHE_GLOBAL_CAP = 1_000_000


class Cache:
    """Thread-safe LRU with hard memory caps."""

    def __init__(self, max_size: int):
        global _cache_total
        self._d: OrderedDict = OrderedDict()
        self._lock = threading.RLock()
        capped = max(1, min(max_size, _CACHE_HARD_CAP))
        with _cache_total_lock:
            remaining = max(1, _CACHE_GLOBAL_CAP - _cache_total)
            self._max = min(capped, remaining)
            _cache_total += self._max

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
            return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# ========================= DATA MODELS =========================

@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]       # STY names from BQ MRSTY
    semantic_type_ids: List[str]    # TUI codes from BQ MRSTY
    ic_score: float
    source_vocabs: List[str] = field(default_factory=list)


@dataclass
class TopicInfo:
    """One topic = one embedding cluster of related CUIs."""
    topic_id: str
    cuis: List[str]
    cui_terms: Dict[str, str]       # cui → preferred_term


@dataclass
class ReductionResult:
    context_string: str
    input_count: int
    after_sab_count: int
    after_ic_count: int
    after_dedup_count: int
    topics: Dict[str, TopicInfo]
    all_reduced_cuis: List[str]
    removed: Dict[str, str]         # cui → removal reason
    processing_time_ms: float


# ========================= METADATA FETCHER =========================

_BQ_BATCH_SIZE = 5000


class MetadataFetcher:
    """Batched BQ fetch — preferred term, semantic types, SABs."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 cache_size: int):
        self.bq = bq
        self.pid = pid
        self.did = did
        self._cache = Cache(cache_size)

    @timed("metadata_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, CUIMetadata]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH_SIZE):
            batch = missing[i:i + _BQ_BATCH_SIZE]
            query = f"""
            SELECT c.CUI AS cui,
              ANY_VALUE(CASE WHEN c.ISPREF = 'Y' THEN c.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT s.STY IGNORE NULLS) AS stys,
              ARRAY_AGG(DISTINCT c.SAB IGNORE NULLS) AS sabs
            FROM `{self.pid}.{self.did}.MRCONSO` c
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s ON c.CUI = s.CUI
            WHERE c.CUI IN UNNEST(@cuis) AND c.LAT = 'ENG'
            GROUP BY c.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            try:
                for row in self.bq.query(
                        query, job_config=jc, timeout=300).result():
                    meta = CUIMetadata(
                        cui=row.cui,
                        preferred_term=row.pref_term or "",
                        semantic_types=row.stys or [],
                        semantic_type_ids=row.tuis or [],
                        ic_score=0.0,       # filled by IC step
                        source_vocabs=row.sabs or [],
                    )
                    self._cache.put(row.cui, meta)
                    result[row.cui] = meta
            except Exception as e:
                log(f"Metadata fetch error (batch {i}): {e}", "ERROR")

        return result


# ========================= EMBEDDING FETCHER =========================

class EmbeddingFetcher:
    """Fetches pre-computed CUI embeddings from BQ."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 embedding_table: str):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.embedding_table = embedding_table
        self._cache = Cache(50_000)

    @timed("embedding_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, np.ndarray]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH_SIZE):
            batch = missing[i:i + _BQ_BATCH_SIZE]
            query = f"""
            SELECT cui, embedding
            FROM `{self.pid}.{self.did}.{self.embedding_table}`
            WHERE cui IN UNNEST(@cuis)
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            try:
                for row in self.bq.query(
                        query, job_config=jc, timeout=300).result():
                    vec = np.array(row.embedding, dtype=np.float32)
                    norm = np.linalg.norm(vec)
                    if norm > 0:
                        vec = vec / norm
                    self._cache.put(row.cui, vec)
                    result[row.cui] = vec
            except Exception as e:
                log(f"Embedding fetch error (batch {i}): {e}", "ERROR")

        return result


# ========================= SAB FILTER =========================

class SABFilter:
    def __init__(self, allowed_sabs: Set[str]):
        self.allowed = allowed_sabs

    @timed("sab_filter")
    def run(self, cuis: List[str],
            metadata: Dict[str, CUIMetadata]
            ) -> Tuple[List[str], Dict[str, str]]:
        kept, removed = [], {}
        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                removed[cui] = "no_metadata"
            elif not (set(meta.source_vocabs) & self.allowed):
                removed[cui] = (f"sab_not_allowed "
                                f"(has {meta.source_vocabs})")
            else:
                kept.append(cui)
        return kept, removed


# ========================= SUB-GRAPH BUILDER ===========================

class SubGraphBuilder:
    """
    Extracts the induced sub-graph from the full NetworkX hierarchy
    containing only the given CUI set. No new CUIs introduced.

    Also computes IC for each CUI based on its depth in the FULL
    graph (not the sub-graph — full graph gives true specificity).
    """

    def __init__(self, full_graph: nx.DiGraph,
                 ic_scores: Optional[Dict[str, float]] = None):
        self.full_graph = full_graph
        self.ic_scores = ic_scores or {}

    @timed("subgraph_build")
    def build(self, cuis: List[str],
              metadata: Dict[str, CUIMetadata]
              ) -> Tuple[nx.DiGraph, Dict[str, float]]:
        """
        Returns:
          sub_graph: DiGraph with edges only between CUIs in the set
          ic_map: {cui: ic_score} computed from full graph
        """
        cui_set = set(cuis)

        # Build sub-graph: only edges where both endpoints are in set
        sub = nx.DiGraph()
        sub.add_nodes_from(cuis)

        edges_found = 0
        for cui in cuis:
            if not self.full_graph.has_node(cui):
                continue
            # Check parents in full graph — keep edge if parent in set
            for parent in self.full_graph.predecessors(cui):
                if parent in cui_set:
                    sub.add_edge(parent, cui)
                    edges_found += 1
            # Check children in full graph — keep edge if child in set
            for child in self.full_graph.successors(cui):
                if child in cui_set:
                    sub.add_edge(cui, child)
                    edges_found += 1

        # Deduplicate edges (added from both directions)
        edges_found = sub.number_of_edges()

        # Compute IC from full graph depth
        ic_map: Dict[str, float] = {}
        for cui in cuis:
            if cui in self.ic_scores:
                ic_map[cui] = float(self.ic_scores[cui])
            else:
                depth = self._depth_in_full_graph(cui)
                ic_map[cui] = float(np.log1p(depth))
            # Update metadata
            if cui in metadata:
                metadata[cui].ic_score = ic_map[cui]

        connected = sum(1 for c in cuis
                        if sub.in_degree(c) > 0 or sub.out_degree(c) > 0)
        isolated = len(cuis) - connected

        log(f"  Sub-graph: {len(cuis)} nodes, {edges_found} edges, "
            f"{connected} connected, {isolated} isolated")

        return sub, ic_map

    def _depth_in_full_graph(self, cui: str) -> int:
        """BFS to root in the full 25M graph. No depth limit."""
        if not self.full_graph.has_node(cui):
            return 0
        visited, q, depth = {cui}, deque([cui]), 0
        while q:
            nxt = []
            for _ in range(len(q)):
                node = q.popleft()
                for p in self.full_graph.predecessors(node):
                    if p not in visited:
                        visited.add(p)
                        nxt.append(p)
            if not nxt:
                break
            q.extend(nxt)
            depth += 1
        return depth


# ========================= IC ANCESTOR REMOVAL ==========================

class ICAncestorRemover:
    """
    On the sub-graph (edges only within the CUI set):
    if child IC >= parent IC, parent is redundant → remove it.
    """

    @staticmethod
    @timed("ic_ancestor_removal")
    def run(sub_graph: nx.DiGraph,
            ic_map: Dict[str, float],
            metadata: Dict[str, CUIMetadata]
            ) -> Tuple[List[str], Dict[str, str]]:
        """
        Returns:
          surviving: CUIs that were not subsumed
          removed: {cui: reason}
        """
        to_remove: Dict[str, str] = {}

        # For each edge parent→child in sub-graph
        for parent, child in sub_graph.edges():
            parent_ic = ic_map.get(parent, 0)
            child_ic = ic_map.get(child, 0)

            if child_ic >= parent_ic and parent not in to_remove:
                c_meta = metadata.get(child)
                p_meta = metadata.get(parent)
                to_remove[parent] = (
                    f"subsumed by {child} "
                    f"'{c_meta.preferred_term if c_meta else '?'}' "
                    f"(IC {child_ic:.2f} >= {parent_ic:.2f})")

        remove_set = set(to_remove.keys())

        # Check: a removed parent might be the only one subsuming
        # another parent. Walk transitively to ensure consistency.
        # If A is removed by B, and B is removed by C, that's fine —
        # C is the true representative.
        surviving = [c for c in sub_graph.nodes()
                     if c not in remove_set]

        return surviving, to_remove


# ========================= TOPIC CLUSTERING =============================
# Agglomerative clustering on embedding cosine distance.
# Threshold derived from data: mean pairwise distance.

class TopicClusterer:
    """
    Clusters CUIs into topics by embedding similarity.
    No CUIs removed — purely organizational.
    """

    @staticmethod
    @timed("topic_clustering")
    def cluster(cuis: List[str],
                embeddings: Dict[str, np.ndarray]
                ) -> Dict[str, List[str]]:
        """Returns {topic_id: [cuis]}"""

        with_emb = [c for c in cuis if c in embeddings]
        without_emb = [c for c in cuis if c not in embeddings]

        topics: Dict[str, List[str]] = {}
        tid = 0

        if len(with_emb) <= 1:
            # Everything in one topic
            topics[f"topic_{tid}"] = cuis
            return topics

        # Build matrix and compute cosine distance
        matrix = np.array([embeddings[c] for c in with_emb])
        sim = matrix @ matrix.T
        np.clip(sim, -1, 1, out=sim)
        dist = 1.0 - sim
        np.fill_diagonal(dist, 0)
        dist = np.maximum(dist, 0)

        # Threshold from data
        n = len(with_emb)
        upper = dist[np.triu_indices(n, k=1)]
        if len(upper) == 0 or np.std(upper) < 1e-9:
            topics[f"topic_{tid}"] = cuis
            return topics

        threshold = float(np.mean(upper))

        # Agglomerative clustering
        condensed = squareform(dist, checks=False)
        Z = linkage(condensed, method="average")
        labels = fcluster(Z, t=threshold, criterion="distance")

        groups: Dict[int, List[str]] = defaultdict(list)
        for cui, label in zip(with_emb, labels):
            groups[label].append(cui)

        for group_cuis in groups.values():
            topics[f"topic_{tid}"] = group_cuis
            tid += 1

        # CUIs without embeddings — each its own topic
        for cui in without_emb:
            topics[f"topic_{tid}"] = [cui]
            tid += 1

        return topics


# ========================= REDUNDANCY REMOVAL ===========================
# Within each topic: tight sub-clustering to find CUIs that mean
# the same thing. Keep highest-IC from each redundancy group.

class RedundancyRemover:
    """
    Within each topic, finds redundancy groups — CUIs so similar
    in embedding space they're essentially the same concept.

    Uses agglomerative clustering with a TIGHT threshold
    (derived from data: mean - 1 std of pairwise distances,
    i.e. only very close pairs are considered redundant).

    From each redundancy group, keeps the highest-IC member.
    Others are removed with explanation.
    """

    @staticmethod
    @timed("redundancy_removal")
    def run(topics: Dict[str, List[str]],
            embeddings: Dict[str, np.ndarray],
            ic_map: Dict[str, float],
            metadata: Dict[str, CUIMetadata],
            ) -> Tuple[Dict[str, List[str]], Dict[str, str]]:
        """
        Returns:
          cleaned_topics: {topic_id: [surviving cuis]}
          removed: {cui: reason}
        """
        cleaned: Dict[str, List[str]] = {}
        all_removed: Dict[str, str] = {}

        for tid, cuis in topics.items():
            with_emb = [c for c in cuis if c in embeddings]
            without_emb = [c for c in cuis if c not in embeddings]

            if len(with_emb) <= 1:
                # Nothing to dedup
                cleaned[tid] = cuis
                continue

            # Build distance matrix
            matrix = np.array([embeddings[c] for c in with_emb])
            sim = matrix @ matrix.T
            np.clip(sim, -1, 1, out=sim)
            dist = 1.0 - sim
            np.fill_diagonal(dist, 0)
            dist = np.maximum(dist, 0)

            n = len(with_emb)
            upper = dist[np.triu_indices(n, k=1)]

            if len(upper) == 0 or np.std(upper) < 1e-9:
                # All identical — keep highest IC
                best = max(with_emb,
                           key=lambda c: ic_map.get(c, 0))
                for c in with_emb:
                    if c != best:
                        m = metadata.get(best)
                        all_removed[c] = (
                            f"redundant with {best} "
                            f"'{m.preferred_term if m else '?'}'")
                cleaned[tid] = [best] + without_emb
                continue

            # Tight threshold: only very similar pairs are redundant
            # mean - 1*std → captures pairs well below average distance
            tight_t = float(np.mean(upper) - np.std(upper))
            tight_t = max(tight_t, 0.01)  # floor to avoid 0

            condensed = squareform(dist, checks=False)
            Z = linkage(condensed, method="average")
            labels = fcluster(Z, t=tight_t, criterion="distance")

            # Within each redundancy group, keep highest IC
            groups: Dict[int, List[str]] = defaultdict(list)
            for cui, label in zip(with_emb, labels):
                groups[label].append(cui)

            surviving = []
            for group_cuis in groups.values():
                if len(group_cuis) == 1:
                    surviving.append(group_cuis[0])
                else:
                    best = max(group_cuis,
                               key=lambda c: ic_map.get(c, 0))
                    surviving.append(best)
                    b_meta = metadata.get(best)
                    for c in group_cuis:
                        if c != best:
                            all_removed[c] = (
                                f"redundant with {best} "
                                f"'{b_meta.preferred_term if b_meta else '?'}' "
                                f"(IC {ic_map.get(best, 0):.2f} vs "
                                f"{ic_map.get(c, 0):.2f})")

            cleaned[tid] = surviving + without_emb

        return cleaned, all_removed


# ========================= MAIN SYSTEM =========================

class CUIReductionSystem:
    """
    Production CUI reduction pipeline.

    Usage:
        system = CUIReductionSystem(project_id, dataset_id,
                                     full_network, allowed_sabs,
                                     embedding_table)
        result = system.reduce(cuis, context_string)

        result.topics             → {topic_id: TopicInfo}
        result.all_reduced_cuis   → flat deduplicated list
        result.removed            → {cui: reason}
    """

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        full_network: nx.DiGraph,
        allowed_sabs: List[str],
        embedding_table: str,
        ic_scores: Optional[Dict[str, float]] = None,
    ):
        self.bq = bigquery.Client(project=project_id)

        self.fetcher = MetadataFetcher(
            self.bq, project_id, dataset_id, 50_000)
        self.sab_filter = SABFilter(set(allowed_sabs))
        self.subgraph_builder = SubGraphBuilder(full_network, ic_scores)
        self.emb_fetcher = EmbeddingFetcher(
            self.bq, project_id, dataset_id, embedding_table)

        log(f"System ready | sabs={allowed_sabs} | "
            f"graph={full_network.number_of_nodes()} nodes | "
            f"embeddings={embedding_table}")

    @timed("full_pipeline")
    def reduce(
        self,
        cuis: List[str],
        context_string: str,
    ) -> ReductionResult:

        t0 = time.perf_counter()
        all_removed: Dict[str, str] = {}

        if not cuis:
            return self._empty(context_string, cuis, t0)

        # ---- Step 1: Metadata from BQ ----
        metadata = self.fetcher.fetch(cuis)
        log(f"  API: {len(cuis)} CUIs")

        # ---- Step 2: SAB filter ----
        after_sab, sab_rm = self.sab_filter.run(cuis, metadata)
        all_removed.update(sab_rm)
        log(f"  SAB filter: {len(cuis)} -> {len(after_sab)}")

        if not after_sab:
            return self._empty(context_string, cuis, t0,
                               removed=all_removed,
                               after_sab=0)

        # ---- Step 3: Build sub-graph within SAB CUIs ----
        sub_graph, ic_map = self.subgraph_builder.build(
            after_sab, metadata)

        # ---- Step 4: IC ancestor removal on sub-graph ----
        after_ic, ic_rm = ICAncestorRemover.run(
            sub_graph, ic_map, metadata)
        all_removed.update(ic_rm)
        log(f"  IC removal: {len(after_sab)} -> {len(after_ic)} "
            f"({len(ic_rm)} ancestors removed)")

        if not after_ic:
            return self._empty(context_string, cuis, t0,
                               removed=all_removed,
                               after_sab=len(after_sab),
                               after_ic=0)

        # ---- Step 5: Fetch embeddings + topic clustering ----
        embeddings = self.emb_fetcher.fetch(after_ic)
        emb_cov = len(embeddings) / len(after_ic) if after_ic else 0
        log(f"  Embeddings: {len(embeddings)}/{len(after_ic)} "
            f"({emb_cov:.0%} coverage)")

        topics_raw = TopicClusterer.cluster(after_ic, embeddings)
        log(f"  Topics: {len(topics_raw)} clusters")

        # ---- Step 6: Redundancy removal within topics ----
        topics_clean, dedup_rm = RedundancyRemover.run(
            topics_raw, embeddings, ic_map, metadata)
        all_removed.update(dedup_rm)

        total_after_dedup = sum(len(c) for c in topics_clean.values())
        log(f"  Redundancy: {len(after_ic)} -> {total_after_dedup} "
            f"({len(dedup_rm)} duplicates removed)")

        # ---- Build output ----
        seen: Set[str] = set()
        all_reduced: List[str] = []
        topics_out: Dict[str, TopicInfo] = {}

        for tid, t_cuis in topics_clean.items():
            # Skip empty topics
            if not t_cuis:
                continue
            cui_terms = {}
            for c in t_cuis:
                m = metadata.get(c)
                cui_terms[c] = m.preferred_term if m else c
                if c not in seen:
                    seen.add(c)
                    all_reduced.append(c)

            topics_out[tid] = TopicInfo(
                topic_id=tid,
                cuis=t_cuis,
                cui_terms=cui_terms,
            )

        elapsed = (time.perf_counter() - t0) * 1000
        log(f"  Final: {len(cuis)} -> {len(all_reduced)} CUIs "
            f"in {len(topics_out)} topics ({elapsed:.0f}ms)")

        return ReductionResult(
            context_string=context_string,
            input_count=len(cuis),
            after_sab_count=len(after_sab),
            after_ic_count=len(after_ic),
            after_dedup_count=total_after_dedup,
            topics=topics_out,
            all_reduced_cuis=all_reduced,
            removed=all_removed,
            processing_time_ms=elapsed,
        )

    @staticmethod
    def _empty(ctx, cuis, t0, removed=None,
               after_sab=0, after_ic=0):
        return ReductionResult(
            context_string=ctx,
            input_count=len(cuis),
            after_sab_count=after_sab,
            after_ic_count=after_ic,
            after_dedup_count=0,
            topics={},
            all_reduced_cuis=[],
            removed=removed or {},
            processing_time_ms=(time.perf_counter() - t0) * 1000,
        )

    def get_stats(self) -> Dict:
        return get_timings()


# ========================= CUI EXTRACTOR =========================

class CUIExtractor:
    """Calls CUI extraction API. Returns deduplicated CUI list."""

    def __init__(self, api_url: str):
        self.url = api_url
        self.session = requests.Session()
        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30,
            ).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text: str) -> List[str]:
        try:
            resp = self.session.post(
                self.url, headers=self.headers,
                json={"query_texts": [text], "top_k": 3},
                timeout=200,
            )
            resp.raise_for_status()
            data = resp.json()
            if not isinstance(data, dict):
                return []

            cuis = []
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
            return list(set(cuis))
        except Exception as e:
            log(f"Extraction failed: {e}", "ERROR")
            return []


# ========================= MAIN =========================

def main():
    """Run CUI reduction for clinical texts."""
    import pickle

    # ---- Configuration (replace with your values) ----
    PROJECT_ID = "your-project-id"
    DATASET_ID = "your-dataset"
    API_URL = "https://your-api/extract"
    NETWORK_PATH = "networkx_cui_context_v1_1_0.pkl"
    EMBEDDING_TABLE = "cui_embeddings"
    ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM",
                     "SNOMEDCT_US", "LOINC"]

    # ---- Load hierarchy ----
    log("Loading hierarchy...")
    with open(NETWORK_PATH, "rb") as f:
        network = pickle.load(f)
    log(f"Loaded: {network.number_of_nodes()} nodes, "
        f"{network.number_of_edges()} edges")

    # ---- Initialize ----
    system = CUIReductionSystem(
        PROJECT_ID, DATASET_ID, network, ALLOWED_SABS,
        EMBEDDING_TABLE)
    extractor = CUIExtractor(API_URL)

    # ---- Process texts ----
    texts = [
        "Patient has severe pain in left knee with swelling",
        "History of type 2 diabetes mellitus with peripheral neuropathy",
        "Acute exacerbation of chronic obstructive pulmonary disease",
    ]

    for text in texts:
        log(f"\n{'='*60}")
        log(f"TEXT: {text}")

        cuis = extractor.extract(text)
        if not cuis:
            log("  No CUIs extracted")
            continue

        result = system.reduce(cuis, text)
        log(f"  {len(result.topics)} topics, "
            f"{result.after_dedup_count} CUIs")

    log(f"\n{'='*60}")
    log("PERFORMANCE:")
    for name, stats in system.get_stats().items():
        log(f"  {name}: mean={stats['mean_ms']:.1f}ms")


if __name__ == "__main__":
    main()
