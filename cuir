# %% Retention v2: embedding-based alignment + IC percentile

# precompute IC percentile lookup (run once)
all_ic_vals = np.array(list(ic_precomputed.values()))
ic_p25 = np.percentile(all_ic_vals, 25)
ic_p75 = np.percentile(all_ic_vals, 75)
print(f"IC range: min={all_ic_vals.min():.2f}, p25={ic_p25:.2f}, p75={ic_p75:.2f}, max={all_ic_vals.max():.2f}")

# get embeddings for surviving CUIs (already cached from pipeline run)
embs = system.emb_fetcher.fetch(result.all_reduced_cuis)

# context vector = mean of all surviving CUI embeddings
if embs:
    ctx_vec = np.mean([embs[c] for c in embs], axis=0)
    ctx_vec = ctx_vec / (np.linalg.norm(ctx_vec) + 1e-10)

retention = {}
for cui in result.all_reduced_cuis:
    m = system.fetcher._cache.get(cui)
    if not m:
        continue

    # IC: percentile-based (0-1 scale using IQR)
    raw_ic = m.ic_score
    ic_norm = np.clip((raw_ic - ic_p25) / (ic_p75 - ic_p25 + 1e-10), 0, 1)

    # context alignment: cosine similarity of CUI embedding to context vector
    if cui in embs:
        alignment = float(np.dot(embs[cui], ctx_vec))
        alignment = max(0.0, alignment)  # clamp negatives
    else:
        alignment = 0.0

    # combined
    retention[cui] = {
        "score": 0.4 * ic_norm + 0.6 * alignment,
        "ic_norm": round(ic_norm, 3),
        "alignment": round(alignment, 3),
        "term": m.preferred_term,
    }

# explains map
explains = defaultdict(list)
for entry in result.audit_trail:
    if entry.kept_cui and entry.removed_cui:
        explains[entry.kept_cui].append(entry.removed_cui)

sorted_cuis = sorted(retention.keys(), key=lambda c: retention[c]["score"], reverse=True)

print(f"\nTop 20 CUIs by retention score:")
for c in sorted_cuis[:20]:
    r = retention[c]
    explained = explains.get(c, [])
    line = f"  {c} | score={r['score']:.3f} (IC={r['ic_norm']}, align={r['alignment']}) | {r['term']}"
    if explained:
        line += f" | explains {len(explained)} CUIs"
    print(line)
