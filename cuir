"""
CUI Reduction System — Tree & IC Approach
==========================================
Reduces 20K+ extracted CUIs to two actionable tiers:

  Search CUIs — specific leaf concepts for document retrieval
  Topic CUIs  — broad cluster-head concepts for topic labels

Signal chain:
  Extraction API → CUIs
  UMLS hierarchy → tree structure (parent/child edges)
  UMLS hierarchy → IC (information content = specificity)
  Tree structure → clusters (shared ancestry groups related concepts)
  IC → ranking (higher IC = more specific = better representative)

No tokenization. No stopwords. No text matching. No embeddings.
The extraction API bridges text→CUI.
The UMLS hierarchy provides everything else.

Stages:
  1. Metadata fetch (batched BQ)
  2. SAB filter (keep target vocabularies)
  3. Tree clustering (group related CUIs by shared ancestry)
  4. Hierarchy retention (children retain ancestors, IC validated)
  5. Tier classification (depth from leaf in hierarchy)

Every CUI — surviving or removed — gets a traceable explanation.

Requirements:
  pip install google-cloud-bigquery networkx numpy requests
"""

import time
import threading
import logging
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque, OrderedDict
from enum import Enum
from functools import wraps

import numpy as np
import networkx as nx
import requests
import subprocess
from google.cloud import bigquery


# ========================= LOGGING =========================

_logger = logging.getLogger("cui_reduction")
if not _logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s"))
    _logger.addHandler(_handler)
    _logger.setLevel(logging.INFO)
    _logger.propagate = False


def log(msg: str, level: str = "INFO"):
    getattr(_logger, level.lower(), _logger.info)(msg)


# ========================= TIMING =========================

_TIMING_BUFFER_SIZE = 1000
_timings: Dict[str, deque] = defaultdict(
    lambda: deque(maxlen=_TIMING_BUFFER_SIZE))
_timing_lock = threading.Lock()


def timed(name: str):
    def dec(fn):
        @wraps(fn)
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                with _timing_lock:
                    _timings[name].append(
                        (time.perf_counter() - t0) * 1000)
        return wrapper
    return dec


def get_timings() -> Dict[str, Dict[str, float]]:
    with _timing_lock:
        out = {}
        for k, v in _timings.items():
            if v:
                vals = list(v)
                out[k] = {
                    "count": len(vals),
                    "mean_ms": float(np.mean(vals)),
                    "p50_ms": float(np.median(vals)),
                    "p99_ms": float(np.percentile(vals, 99)),
                    "max_ms": float(np.max(vals)),
                }
        return out


# ========================= CACHE =========================

_CACHE_HARD_CAP = 200_000
_cache_total = 0
_cache_total_lock = threading.Lock()
_CACHE_GLOBAL_CAP = 1_000_000


class Cache:
    """Thread-safe LRU with hard memory caps."""

    def __init__(self, max_size: int):
        global _cache_total
        self._d: OrderedDict = OrderedDict()
        self._lock = threading.RLock()
        capped = max(1, min(max_size, _CACHE_HARD_CAP))
        with _cache_total_lock:
            remaining = max(1, _CACHE_GLOBAL_CAP - _cache_total)
            self._max = min(capped, remaining)
            _cache_total += self._max

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
            return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# ========================= DATA MODELS =========================

class UsageContext(Enum):
    QUERY = "query"
    DOCUMENT = "document"


class Tier(Enum):
    SEARCH = "search"
    TOPIC = "topic"


@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]
    ic_score: float
    source_vocabs: List[str] = field(default_factory=list)


@dataclass
class CUIAssessment:
    """Full assessment with explainability."""
    cui: str
    preferred_term: str
    specificity: float          # IC / max_IC in set, 0-1
    tier: Tier = Tier.SEARCH
    depth_from_leaf: int = 0
    cluster_id: str = ""
    cluster_label: str = ""
    retained_cuis: List[str] = field(default_factory=list)
    explanation: str = ""
    ranking: int = 0


@dataclass
class ClusterInfo:
    """Describes one semantic cluster."""
    cluster_id: str
    label: str
    member_count: int
    surviving_count: int
    retained_count: int
    ic_range: Tuple[float, float] = (0.0, 0.0)


@dataclass
class ReductionResult:
    context_string: str
    usage_context: UsageContext
    search_cuis: List[str]
    topic_cuis: List[str]
    all_surviving: List[str]
    assessments: Dict[str, CUIAssessment]
    retention_map: Dict[str, List[str]]
    removed: Dict[str, str]
    clusters: Dict[str, ClusterInfo]
    input_count: int
    processing_time_ms: float
    stage_counts: Dict[str, int] = field(default_factory=dict)


# ========================= HIERARCHY CLIENT =========================

class HierarchyClient:
    """Navigates UMLS hierarchy with deque BFS and depth caps."""

    def __init__(self, network: nx.DiGraph,
                 ic_scores: Optional[Dict[str, float]] = None,
                 cache_size: int = 1):
        self.network = network
        self.ic_scores = ic_scores or {}
        self._ic_cache = Cache(cache_size)
        self._parent_cache = Cache(cache_size)
        self._children_cache = Cache(cache_size)

        n = network.number_of_nodes()
        e = network.number_of_edges()
        avg_deg = (e / n) if n > 0 else 1
        self._max_depth = max(
            int(np.log(max(n, 1)) / np.log(max(avg_deg, 2))), 3)
        log(f"Hierarchy: {n} nodes, {e} edges, max_depth={self._max_depth}")

    def get_ic(self, cui: str) -> float:
        cached = self._ic_cache.get(cui)
        if cached is not None:
            return cached
        if cui in self.ic_scores:
            ic = float(self.ic_scores[cui])
        else:
            ic = float(np.log1p(self._depth_to_root(cui)))
        if not np.isfinite(ic):
            ic = 0.0
        self._ic_cache.put(cui, ic)
        return ic

    def _depth_to_root(self, cui: str) -> int:
        if not self.network.has_node(cui):
            return 0
        visited, q, depth = {cui}, deque([cui]), 0
        while q and depth < self._max_depth:
            nxt = []
            for _ in range(len(q)):
                for p in self.get_parents(q.popleft()):
                    if p not in visited:
                        visited.add(p)
                        nxt.append(p)
            if not nxt:
                break
            q.extend(nxt)
            depth += 1
        return depth

    def get_parents(self, cui: str) -> List[str]:
        cached = self._parent_cache.get(cui)
        if cached is not None:
            return cached
        parents = (list(self.network.predecessors(cui))
                   if self.network.has_node(cui) else [])
        self._parent_cache.put(cui, parents)
        return parents

    def get_children(self, cui: str) -> List[str]:
        cached = self._children_cache.get(cui)
        if cached is not None:
            return cached
        children = (list(self.network.successors(cui))
                    if self.network.has_node(cui) else [])
        self._children_cache.put(cui, children)
        return children

    def depth_to_nearest_leaf(self, cui: str, cui_set: Set[str]) -> int:
        """Hops down to most specific descendant in cui_set. 0 = leaf."""
        if not self.network.has_node(cui):
            return 0
        visited = {cui}
        q: deque = deque([(cui, 0)])
        max_d = 0
        while q:
            node, d = q.popleft()
            if d >= self._max_depth:
                continue
            for child in self.get_children(node):
                if child not in visited:
                    visited.add(child)
                    if child in cui_set:
                        max_d = max(max_d, d + 1)
                    q.append((child, d + 1))
        return max_d

    def find_ancestors_in_set(self, cui: str,
                              cui_set: Set[str]) -> List[str]:
        """BFS up — find all ancestors of cui that are in cui_set."""
        found = []
        visited = {cui}
        q = deque()
        for p in self.get_parents(cui):
            if p not in visited:
                visited.add(p)
                q.append((p, 1))
        while q:
            node, depth = q.popleft()
            if depth > self._max_depth:
                continue
            if node in cui_set:
                found.append(node)
            for p in self.get_parents(node):
                if p not in visited:
                    visited.add(p)
                    q.append((p, depth + 1))
        return found


# ========================= METADATA FETCHER =========================

_BQ_BATCH_SIZE = 5000


class MetadataFetcher:
    """Batched BQ fetch for CUI metadata."""

    def __init__(self, bq: bigquery.Client, pid: str, did: str,
                 hierarchy: HierarchyClient, cache_size: int):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.hierarchy = hierarchy
        self._cache = Cache(cache_size)

    @timed("metadata_fetch")
    def fetch(self, cuis: List[str]) -> Dict[str, CUIMetadata]:
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH_SIZE):
            batch = missing[i:i + _BQ_BATCH_SIZE]
            query = f"""
            SELECT c.CUI AS cui,
              ANY_VALUE(CASE WHEN c.ISPREF = 'Y' THEN c.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT c.SAB IGNORE NULLS) AS sabs
            FROM `{self.pid}.{self.did}.MRCONSO` c
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s ON c.CUI = s.CUI
            WHERE c.CUI IN UNNEST(@cuis) AND c.LAT = 'ENG'
            GROUP BY c.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            try:
                for row in self.bq.query(
                        query, job_config=jc, timeout=300).result():
                    meta = CUIMetadata(
                        cui=row.cui,
                        preferred_term=row.pref_term or "",
                        semantic_types=row.tuis or [],
                        ic_score=self.hierarchy.get_ic(row.cui),
                        source_vocabs=row.sabs or [],
                    )
                    self._cache.put(row.cui, meta)
                    result[row.cui] = meta
            except Exception as e:
                log(f"Metadata fetch error (batch {i}): {e}", "ERROR")

        fetched = len(result) - (len(cuis) - len(missing))
        if fetched < len(missing):
            log(f"  Metadata partial: got {fetched}/{len(missing)}",
                "WARNING")
        return result


# ========================= SAB FILTER =========================

class SABFilter:
    def __init__(self, allowed_sabs: Set[str]):
        self.allowed = allowed_sabs

    @timed("sab_filter")
    def run(self, cuis: List[str],
            metadata: Dict[str, CUIMetadata]
            ) -> Tuple[List[str], Dict[str, str]]:
        kept, removed = [], {}
        for cui in cuis:
            meta = metadata.get(cui)
            if not meta:
                removed[cui] = "no_metadata"
            elif not (set(meta.source_vocabs) & self.allowed):
                removed[cui] = (f"sab_not_allowed "
                                f"(has {meta.source_vocabs})")
            else:
                kept.append(cui)
        return kept, removed


# ========================= TREE CLUSTERING =========================

class _UnionFind:
    """Path-compressed union-find for clustering."""

    def __init__(self, items):
        self._parent = {x: x for x in items}
        self._rank = {x: 0 for x in items}

    def find(self, x):
        while self._parent[x] != x:
            self._parent[x] = self._parent[self._parent[x]]
            x = self._parent[x]
        return x

    def union(self, a, b):
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return
        if self._rank[ra] < self._rank[rb]:
            ra, rb = rb, ra
        self._parent[rb] = ra
        if self._rank[ra] == self._rank[rb]:
            self._rank[ra] += 1

    def groups(self) -> Dict[str, List[str]]:
        clusters: Dict[str, List[str]] = defaultdict(list)
        for item in self._parent:
            clusters[self.find(item)].append(item)
        return dict(clusters)


class TreeClusterer:
    """
    Groups CUIs into semantic clusters using UMLS hierarchy.

    Algorithm:
      1. For each CUI, get parents from the FULL graph
      2. If a parent is also in the CUI set -> union(CUI, parent)
      3. If two CUIs share a parent (even if parent NOT in set)
         -> union them (siblings = same cluster)
      4. Connected components = clusters
    """

    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy

    @timed("tree_clustering")
    def cluster(self, cuis: List[str],
                metadata: Dict[str, CUIMetadata]
                ) -> Tuple[Dict[str, List[str]], Dict[str, ClusterInfo]]:

        if not cuis:
            return {}, {}

        cui_set = set(cuis)
        uf = _UnionFind(cuis)

        parent_to_children: Dict[str, List[str]] = defaultdict(list)

        for cui in cuis:
            for p in self.hierarchy.get_parents(cui):
                if p in cui_set:
                    uf.union(cui, p)
                parent_to_children[p].append(cui)

        for p, children in parent_to_children.items():
            if len(children) > 1:
                first = children[0]
                for other in children[1:]:
                    uf.union(first, other)

        groups = uf.groups()
        clusters: Dict[str, List[str]] = {}
        infos: Dict[str, ClusterInfo] = {}

        for root_id, members in groups.items():
            best = max(members,
                       key=lambda c: (metadata[c].ic_score
                                      if c in metadata else 0))
            best_meta = metadata.get(best)
            label = best_meta.preferred_term if best_meta else best

            ic_scores = [metadata[c].ic_score
                         for c in members if c in metadata]
            ic_range = ((min(ic_scores), max(ic_scores))
                        if ic_scores else (0.0, 0.0))

            cid = f"cluster_{root_id}"
            clusters[cid] = members
            infos[cid] = ClusterInfo(
                cluster_id=cid,
                label=label,
                member_count=len(members),
                surviving_count=0,
                retained_count=0,
                ic_range=ic_range,
            )

        log(f"  Clusters: {len(clusters)} groups from {len(cuis)} CUIs")

        return clusters, infos


# ========================= HIERARCHY RETENTION =========================

class HierarchyRetainer:
    """
    Within each cluster, removes redundant ancestors.
    A specific child retains (explains away) its less-specific ancestors.
    """

    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy

    @timed("hierarchy_retention")
    def run(
        self,
        clusters: Dict[str, List[str]],
        cluster_infos: Dict[str, ClusterInfo],
        metadata: Dict[str, CUIMetadata],
    ) -> Tuple[List[str], Dict[str, List[str]], Dict[str, str]]:

        all_surviving: List[str] = []
        retention_map: Dict[str, List[str]] = {}
        all_removed: Dict[str, str] = {}

        for cid, members in clusters.items():
            member_set = set(members)
            ancestor_retainers: Dict[str, Set[str]] = defaultdict(set)

            for cui in members:
                meta = metadata.get(cui)
                if not meta:
                    continue
                ancestors = self.hierarchy.find_ancestors_in_set(
                    cui, member_set)
                for anc in ancestors:
                    anc_meta = metadata.get(anc)
                    if anc_meta and meta.ic_score >= anc_meta.ic_score:
                        ancestor_retainers[anc].add(cui)

            to_remove = set(ancestor_retainers.keys())
            surviving = [c for c in members if c not in to_remove]
            surviving_set = set(surviving)

            all_surviving.extend(surviving)

            for cui in surviving:
                retention_map[cui] = []
            for anc, retainers in ancestor_retainers.items():
                for r in retainers:
                    if r in surviving_set:
                        retention_map[r].append(anc)

            for anc, retainers in ancestor_retainers.items():
                surviving_retainers = [r for r in retainers
                                       if r in surviving_set]
                if surviving_retainers:
                    best = max(surviving_retainers,
                               key=lambda c: (metadata[c].ic_score
                                              if c in metadata else 0))
                    b_meta = metadata.get(best)
                    a_meta = metadata.get(anc)
                    all_removed[anc] = (
                        f"retained by {best} "
                        f"'{b_meta.preferred_term if b_meta else '?'}' "
                        f"(IC {b_meta.ic_score:.2f if b_meta else 0} >= "
                        f"{a_meta.ic_score:.2f if a_meta else 0})")
                else:
                    all_removed[anc] = "retained by more specific descendant"

            info = cluster_infos.get(cid)
            if info:
                info.surviving_count = len(surviving)
                info.retained_count = len(to_remove)

        return all_surviving, retention_map, all_removed


# ========================= TIER CLASSIFICATION =========================

class TierClassifier:
    """
    Search: leaves / near-leaves (depth <= median).
    Topic:  broad ancestors (depth > median).
    """

    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy

    @timed("tier_classification")
    def classify(self, cuis: List[str],
                 assessments: Dict[str, CUIAssessment]
                 ) -> Dict[str, CUIAssessment]:

        cui_set = set(cuis)
        depths = {cui: self.hierarchy.depth_to_nearest_leaf(cui, cui_set)
                  for cui in cuis}

        distinct = set(depths.values())
        if len(distinct) < 2:
            for cui in cuis:
                if cui in assessments:
                    assessments[cui].depth_from_leaf = depths.get(cui, 0)
                    assessments[cui].tier = Tier.SEARCH
            return assessments

        boundary = float(np.median(list(depths.values())))
        for cui in cuis:
            if cui in assessments:
                d = depths.get(cui, 0)
                assessments[cui].depth_from_leaf = d
                assessments[cui].tier = (
                    Tier.SEARCH if d <= boundary else Tier.TOPIC)

        log(f"  Tier boundary (median depth): {boundary:.1f}")
        return assessments


# ========================= EXPLANATION GENERATOR =========================

def generate_explanation(
    a: CUIAssessment,
    metadata: Dict[str, CUIMetadata],
) -> str:
    """Builds human-readable explanation for a surviving CUI."""
    parts = []

    if a.specificity >= 0.8:
        parts.append(f"Highly specific (IC ratio={a.specificity:.2f})")
    elif a.specificity >= 0.5:
        parts.append(f"Moderately specific (IC ratio={a.specificity:.2f})")
    else:
        parts.append(f"Broad concept (IC ratio={a.specificity:.2f})")

    if a.cluster_label:
        parts.append(f"Cluster: {a.cluster_label}")

    if a.retained_cuis:
        names = []
        for r in a.retained_cuis[:5]:
            m = metadata.get(r)
            names.append(f"'{m.preferred_term}'" if m else r)
        retains_str = ", ".join(names)
        if len(a.retained_cuis) > 5:
            retains_str += f" (+{len(a.retained_cuis) - 5} more)"
        parts.append(f"Retains: {retains_str}")

    tier_desc = ("leaf/near-leaf -> document retrieval"
                 if a.tier == Tier.SEARCH
                 else "broad ancestor -> topic label")
    parts.append(f"Tier: {a.tier.value} "
                 f"(depth={a.depth_from_leaf}, {tier_desc})")

    return " | ".join(parts)


# ========================= DYNAMIC VALIDATION =========================

@dataclass
class ValidationReport:
    """Self-assessment of reduction quality — no ground truth needed."""
    reduction_ratio: float
    is_healthy: bool
    specificity_gain: float
    specificity_favored: bool
    residual_parent_child_pairs: int
    hierarchy_clean: bool
    search_count: int
    topic_count: int
    tier_degenerate: bool
    cluster_count: int
    singleton_clusters: int
    issues: List[str] = field(default_factory=list)

    def summary(self) -> str:
        status = "HEALTHY" if self.is_healthy else "ISSUES DETECTED"
        lines = [
            f"Validation: {status}",
            f"  Reduction: {self.reduction_ratio:.1%} survived",
            f"  Specificity gain: {self.specificity_gain:+.3f} "
            f"({'favored' if self.specificity_favored else 'NOT favored'})",
            f"  Hierarchy: {self.residual_parent_child_pairs} residual pairs "
            f"({'clean' if self.hierarchy_clean else 'DIRTY'})",
            f"  Tiers: {self.search_count} search, {self.topic_count} topic "
            f"({'balanced' if not self.tier_degenerate else 'DEGENERATE'})",
            f"  Clusters: {self.cluster_count} total, "
            f"{self.singleton_clusters} singletons",
        ]
        if self.issues:
            lines.append("  Issues:")
            for issue in self.issues:
                lines.append(f"    - {issue}")
        return "\n".join(lines)


class DynamicValidator:
    """Validates reduction quality using internal consistency."""

    @staticmethod
    def validate(
        result: ReductionResult,
        metadata: Dict[str, CUIMetadata],
        hierarchy: HierarchyClient,
    ) -> ValidationReport:

        issues: List[str] = []
        surviving_set = set(result.all_surviving)

        # Reduction ratio
        ratio = (len(result.all_surviving) / result.input_count
                 if result.input_count > 0 else 0)
        if ratio > 0.5:
            issues.append(
                f"High survival rate ({ratio:.0%}) — "
                f"may not be reducing enough")
        elif ratio < 0.01 and result.input_count > 50:
            issues.append(
                f"Very low survival ({ratio:.0%}) — "
                f"too aggressive")

        # Specificity: are survivors more specific than input average?
        surv_spec = [result.assessments[c].specificity
                     for c in result.all_surviving
                     if c in result.assessments]
        mean_surv = float(np.mean(surv_spec)) if surv_spec else 0

        all_ic = [metadata[c].ic_score for c in metadata]
        max_ic = max(all_ic) if all_ic else 1.0
        mean_all_norm = ((float(np.mean(all_ic)) / max_ic)
                         if max_ic > 0 else 0)
        spec_gain = mean_surv - mean_all_norm
        spec_favored = spec_gain > 0
        if not spec_favored:
            issues.append(
                f"Survivors not more specific than input "
                f"(gain={spec_gain:+.3f})")

        # Residual parent-child pairs (should be 0)
        residual = 0
        for cui in result.all_surviving:
            for p in hierarchy.get_parents(cui):
                if p in surviving_set:
                    residual += 1
        if residual > 0:
            issues.append(
                f"{residual} parent-child pairs remain — "
                f"hierarchy retention has gaps")

        # Tier balance
        n_s = len(result.search_cuis)
        n_t = len(result.topic_cuis)
        tier_degen = ((n_s == 0 or n_t == 0)
                      and (n_s + n_t) > 5)
        if tier_degen:
            issues.append(
                f"Degenerate tiers: {n_s} search, {n_t} topic")

        # Cluster health
        n_clusters = len(result.clusters)
        singletons = sum(1 for c in result.clusters.values()
                         if c.surviving_count <= 1)
        if n_clusters > 0 and singletons / n_clusters > 0.8:
            issues.append(
                f"Mostly singleton clusters "
                f"({singletons}/{n_clusters})")

        return ValidationReport(
            reduction_ratio=ratio,
            is_healthy=len(issues) == 0,
            specificity_gain=spec_gain,
            specificity_favored=spec_favored,
            residual_parent_child_pairs=residual,
            hierarchy_clean=residual == 0,
            search_count=n_s,
            topic_count=n_t,
            tier_degenerate=tier_degen,
            cluster_count=n_clusters,
            singleton_clusters=singletons,
            issues=issues,
        )


# ========================= MAIN SYSTEM =========================

class CUIReductionSystem:
    """
    Production CUI reduction pipeline using IC + tree structure.

    Usage:
        system = CUIReductionSystem(project_id, dataset_id,
                                     network, allowed_sabs)
        result = system.reduce(cuis, context_string)
    """

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        network: nx.DiGraph,
        allowed_sabs: List[str],
        ic_scores: Optional[Dict[str, float]] = None,
    ):
        self.bq = bigquery.Client(project=project_id)
        cache_size = network.number_of_nodes() * 2

        self.hierarchy = HierarchyClient(network, ic_scores, cache_size)
        self.fetcher = MetadataFetcher(
            self.bq, project_id, dataset_id,
            self.hierarchy, cache_size)
        self.sab_filter = SABFilter(set(allowed_sabs))
        self.clusterer = TreeClusterer(self.hierarchy)
        self.retainer = HierarchyRetainer(self.hierarchy)
        self.tier_classifier = TierClassifier(self.hierarchy)

        log(f"CUIReductionSystem ready | sabs={allowed_sabs}")

    @timed("full_pipeline")
    def reduce(
        self,
        cuis: List[str],
        context_string: str,
        usage_context: UsageContext = UsageContext.QUERY,
    ) -> ReductionResult:

        t0 = time.perf_counter()
        all_removed: Dict[str, str] = {}
        sc: Dict[str, int] = {"input": len(cuis)}

        if not cuis:
            return self._empty(context_string, usage_context, cuis,
                               all_removed, t0, sc)

        log(f"Reducing {len(cuis)} CUIs | "
            f"'{context_string[:80]}...'")

        # Stage 1: Metadata
        metadata = self.fetcher.fetch(cuis)
        sc["with_metadata"] = len(metadata)

        # Stage 2: SAB filter
        after_sab, sab_rm = self.sab_filter.run(cuis, metadata)
        all_removed.update(sab_rm)
        sc["after_sab"] = len(after_sab)
        log(f"  SAB: {len(cuis)} -> {len(after_sab)}")

        if not after_sab:
            return self._empty(context_string, usage_context, cuis,
                               all_removed, t0, sc)

        # Stage 3: Tree clustering
        sab_meta = {c: metadata[c] for c in after_sab if c in metadata}
        clusters, cluster_infos = self.clusterer.cluster(
            after_sab, sab_meta)
        sc["clusters"] = len(clusters)

        # Stage 4: Hierarchy retention within clusters
        surviving, rmap, hier_rm = self.retainer.run(
            clusters, cluster_infos, metadata)
        all_removed.update(hier_rm)
        sc["after_retention"] = len(surviving)
        log(f"  Retention: {len(after_sab)} -> {len(surviving)} "
            f"({len(hier_rm)} ancestors removed)")

        if not surviving:
            return self._empty(context_string, usage_context, cuis,
                               all_removed, t0, sc)

        # Build assessments
        ic_vals = [metadata[c].ic_score
                   for c in surviving if c in metadata]
        max_ic = max(ic_vals) if ic_vals else 1.0
        if max_ic <= 0 or not np.isfinite(max_ic):
            max_ic = 1.0

        cui_to_cluster: Dict[str, str] = {}
        for cid, members in clusters.items():
            for m in members:
                cui_to_cluster[m] = cid

        assessments: Dict[str, CUIAssessment] = {}
        for cui in surviving:
            meta = metadata.get(cui)
            if not meta:
                continue
            spec = meta.ic_score / max_ic
            if not np.isfinite(spec):
                spec = 0.0

            cid = cui_to_cluster.get(cui, "")
            cinfo = cluster_infos.get(cid)

            assessments[cui] = CUIAssessment(
                cui=cui,
                preferred_term=meta.preferred_term,
                specificity=spec,
                cluster_id=cid,
                cluster_label=cinfo.label if cinfo else "",
                retained_cuis=rmap.get(cui, []),
            )

        # Stage 5: Tier classification
        assessments = self.tier_classifier.classify(
            surviving, assessments)

        # Generate explanations
        for cui in surviving:
            if cui in assessments:
                assessments[cui].explanation = generate_explanation(
                    assessments[cui], metadata)

        # Sort by specificity within tiers
        search, topics = [], []
        for cui in surviving:
            a = assessments.get(cui)
            if not a:
                continue
            (search if a.tier == Tier.SEARCH else topics).append(cui)

        search.sort(key=lambda c: assessments[c].specificity,
                    reverse=True)
        topics.sort(key=lambda c: assessments[c].specificity,
                    reverse=True)
        all_surv = sorted(
            surviving,
            key=lambda c: (assessments[c].specificity
                           if c in assessments else 0),
            reverse=True)

        for rank, cui in enumerate(all_surv, 1):
            if cui in assessments:
                assessments[cui].ranking = rank

        sc.update({"search": len(search), "topics": len(topics),
                   "total_surviving": len(all_surv)})

        elapsed = (time.perf_counter() - t0) * 1000
        log(f"  Done: {len(cuis)} -> {len(all_surv)} "
            f"(search={len(search)}, topics={len(topics)}) "
            f"({elapsed:.0f}ms)")

        return ReductionResult(
            context_string=context_string,
            usage_context=usage_context,
            search_cuis=search,
            topic_cuis=topics,
            all_surviving=all_surv,
            assessments={c: assessments[c] for c in all_surv
                         if c in assessments},
            retention_map=rmap,
            removed=all_removed,
            clusters=cluster_infos,
            input_count=len(cuis),
            processing_time_ms=elapsed,
            stage_counts=sc,
        )

    @staticmethod
    def _empty(ctx, uc, cuis, removed, t0, sc):
        return ReductionResult(
            ctx, uc, [], [], [], {}, {}, removed, {},
            len(cuis), (time.perf_counter() - t0) * 1000, sc,
        )

    def validate(self, result: ReductionResult) -> ValidationReport:
        """Run dynamic validation on a reduction result."""
        metadata = self.fetcher.fetch(
            list(set(result.all_surviving)
                 | set(result.removed.keys())))
        return DynamicValidator.validate(
            result, metadata, self.hierarchy)

    def get_stats(self) -> Dict:
        return get_timings()


# ========================= CUI EXTRACTOR =========================

class CUIExtractor:
    """Calls CUI extraction API. Returns list of CUI strings."""

    def __init__(self, api_url: str):
        self.url = api_url
        self.session = requests.Session()
        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30,
            ).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text: str) -> List[str]:
        """Returns deduplicated list of CUI strings."""
        try:
            resp = self.session.post(
                self.url, headers=self.headers,
                json={"query_texts": [text], "top_k": 3},
                timeout=200,
            )
            resp.raise_for_status()
            data = resp.json()
            if not isinstance(data, dict):
                return []

            cuis = []
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
            return list(set(cuis))
        except Exception as e:
            log(f"Extraction failed: {e}", "ERROR")
            return []


# ========================= MAIN =========================

def main():
    """Run CUI reduction for clinical texts."""
    import pickle

    # ---- Configuration (replace with your values) ----
    PROJECT_ID = "your-project-id"
    DATASET_ID = "your-dataset"
    API_URL = "https://your-api/extract"
    NETWORK_PATH = "networkx_cui_context_v1_1_0.pkl"
    ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM",
                     "SNOMEDCT_US", "LOINC"]

    # ---- Load hierarchy ----
    log("Loading hierarchy...")
    with open(NETWORK_PATH, "rb") as f:
        network = pickle.load(f)
    log(f"Loaded: {network.number_of_nodes()} nodes, "
        f"{network.number_of_edges()} edges")

    # ---- Initialize ----
    system = CUIReductionSystem(
        PROJECT_ID, DATASET_ID, network, ALLOWED_SABS)
    extractor = CUIExtractor(API_URL)

    # ---- Process texts ----
    texts = [
        "Patient has severe pain in left knee with swelling",
        "History of type 2 diabetes mellitus with peripheral neuropathy",
        "Acute exacerbation of chronic obstructive pulmonary disease",
    ]

    for text in texts:
        log(f"\n{'='*60}")
        log(f"TEXT: {text}")

        cuis = extractor.extract(text)
        if not cuis:
            log("  No CUIs extracted")
            continue

        result = system.reduce(cuis, text)
        report = system.validate(result)

        log(f"  {result.input_count} -> {len(result.all_surviving)} "
            f"(search={len(result.search_cuis)}, "
            f"topic={len(result.topic_cuis)}, "
            f"clusters={len(result.clusters)}) "
            f"[{result.processing_time_ms:.0f}ms]")
        log(f"  Validation: {'HEALTHY' if report.is_healthy else 'ISSUES'}")
        if report.issues:
            for issue in report.issues:
                log(f"    - {issue}")

    log(f"\n{'='*60}")
    log("PERFORMANCE:")
    for name, stats in system.get_stats().items():
        log(f"  {name}: mean={stats['mean_ms']:.1f}ms")


if __name__ == "__main__":
    main()
