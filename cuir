# %% Quick test: build retention output from existing result

# 1. Context alignment: token overlap between CUI term and input text
context_tokens = tokenize_term(normalize_term(text))

retention = {}
for cui in result.all_reduced_cuis:
    m = system.fetcher._cache.get(cui)
    if not m:
        continue
    
    # IC component (0-1 normalized)
    max_ic = max(ic_precomputed.values()) if ic_precomputed else 1.0
    ic_norm = m.ic_score / max_ic
    
    # context alignment: what fraction of CUI's tokens appear in the input
    if m.term_tokens and context_tokens:
        alignment = len(m.term_tokens & context_tokens) / len(m.term_tokens)
    else:
        alignment = 0.0
    
    # combined retention score
    retention[cui] = {
        "score": 0.5 * ic_norm + 0.5 * alignment,
        "ic_norm": ic_norm,
        "alignment": alignment,
        "term": m.preferred_term,
    }

# 2. Build explains-map from audit trail
explains = defaultdict(list)
for entry in result.audit_trail:
    if entry.kept_cui and entry.removed_cui:
        explains[entry.kept_cui].append(entry.removed_cui)

# 3. Sorted output
sorted_cuis = sorted(retention.keys(), key=lambda c: retention[c]["score"], reverse=True)

print(f"Top 15 CUIs by retention score:")
for c in sorted_cuis[:15]:
    r = retention[c]
    explained = explains.get(c, [])
    print(f"  {c} | score={r['score']:.3f} (IC={r['ic_norm']:.3f}, align={r['alignment']:.3f}) | {r['term']}")
    if explained:
        print(f"       explains {len(explained)} CUIs: {explained[:5]}")
