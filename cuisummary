"""
Production-Grade CUI Reduction System
- No recursive SQL
- Explicit error handling
- No runtime gcloud dependencies
- Strict API/schema contracts
- Full observability
"""

import time
import json
import logging
import numpy as np
from typing import List, Set, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field, asdict
from collections import defaultdict, deque
from datetime import datetime
from enum import Enum
import traceback
from abc import ABC, abstractmethod

from google.cloud import bigquery
from google.cloud import monitoring_v3
from google.cloud import logging as cloud_logging
from sklearn.cluster import AgglomerativeClustering
import requests
from prometheus_client import Counter, Histogram, Gauge, Summary
import structlog

# ============================================================
# OBSERVABILITY SETUP
# ============================================================

# Structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Prometheus metrics
METRICS = {
    'cui_extraction_total': Counter('cui_extraction_total', 'Total CUI extractions', ['status']),
    'cui_extraction_duration': Histogram('cui_extraction_duration_seconds', 'CUI extraction duration'),
    'reduction_rate': Gauge('cui_reduction_rate', 'CUI reduction rate'),
    'hierarchy_depth': Histogram('cui_hierarchy_depth', 'CUI hierarchy depth'),
    'batch_processing_duration': Histogram('batch_processing_duration_seconds', 'Batch processing duration', ['operation']),
    'api_calls': Counter('api_calls_total', 'Total API calls', ['endpoint', 'status']),
    'bigquery_queries': Counter('bigquery_queries_total', 'Total BigQuery queries', ['table', 'status']),
    'errors': Counter('errors_total', 'Total errors', ['error_type', 'operation']),
    'cache_operations': Counter('cache_operations_total', 'Cache operations', ['operation', 'hit_miss'])
}

# ============================================================
# STRICT CONTRACTS AND SCHEMAS
# ============================================================

@dataclass
class TableSchema:
    """Strict schema definition for BigQuery tables"""
    
    @dataclass
    class MRREL:
        cui1: str  # Source CUI
        cui2: str  # Target CUI  
        rel: str   # Relationship type (PAR, CHD, RB, RN)
        rela: Optional[str] = None  # Additional relationship attribute
        sab: Optional[str] = None  # Source abbreviation
        
    @dataclass
    class MRCONSO:
        cui: str   # Concept Unique Identifier
        lat: str   # Language
        ts: str    # Term status
        lui: str   # Lexical Unique Identifier
        stt: str   # String type
        sui: str   # String Unique Identifier
        ispref: str  # Preferred term indicator
        aui: str   # Atom Unique Identifier  
        saui: Optional[str] = None
        scui: Optional[str] = None
        sdui: Optional[str] = None
        sab: str   # Source abbreviation
        tty: str   # Term type
        code: str  # Source code
        str: str   # String/term
        
    @dataclass
    class CUIEmbeddings:
        cui: str
        embedding: List[float]  # Vector embedding
        model_version: str
        created_at: datetime
        
    @dataclass 
    class CUIDescriptions:
        cui: str
        definition: str
        source: str
        
    # Required columns for each table
    REQUIRED_COLUMNS = {
        'mrrel': ['cui1', 'cui2', 'rel'],
        'mrconso': ['cui', 'sab', 'str'],
        'cui_embeddings': ['cui', 'embedding'],
        'cui_descriptions': ['cui', 'definition']
    }


@dataclass
class APIContract:
    """Strict API contract definitions"""
    
    @dataclass
    class CUIExtractionRequest:
        query_texts: List[str]
        top_k: int = 3
        model_version: Optional[str] = "latest"
        include_scores: bool = False
        
        def validate(self) -> None:
            if not self.query_texts:
                raise ValueError("query_texts cannot be empty")
            if not 1 <= self.top_k <= 10:
                raise ValueError("top_k must be between 1 and 10")
            if len(self.query_texts) > 100:
                raise ValueError("Maximum 100 texts per request")
    
    @dataclass
    class CUIExtractionResponse:
        results: Dict[str, List[str]]  # text -> [CUIs]
        scores: Optional[Dict[str, List[float]]] = None
        model_version: str = "latest"
        processing_time_ms: int = 0
        
        def validate(self) -> None:
            if not isinstance(self.results, dict):
                raise ValueError("results must be a dictionary")


@dataclass 
class ReductionConfig:
    """Validated configuration with strict typing"""
    # Required BigQuery settings
    project_id: str
    dataset_id: str
    
    # Required table names  
    mrrel_table: str
    mrconso_table: str
    cui_embeddings_table: str
    cui_descriptions_table: str
    
    # Required API settings
    api_url: str
    api_key: str  # Use service account key, not runtime gcloud
    
    # Required reduction parameters
    allowed_sab: List[str]
    target_reduction: float
    ic_percentile: float
    semantic_threshold: float
    duplicate_threshold: float
    max_hierarchy_depth: int
    min_cluster_retention: float
    
    # Required performance settings
    batch_size: int
    query_timeout: int
    max_query_results: int
    cache_enabled: bool
    
    # Optional monitoring
    enable_metrics: bool = True
    enable_cloud_logging: bool = True
    metrics_port: int = 8000
    
    def validate(self) -> None:
        """Validate configuration values"""
        errors = []
        
        if not self.project_id:
            errors.append("project_id is required")
        if not self.dataset_id:
            errors.append("dataset_id is required")
        if not self.api_url.startswith(('http://', 'https://')):
            errors.append("api_url must be valid HTTP(S) URL")
        if not 0 < self.target_reduction < 1:
            errors.append("target_reduction must be between 0 and 1")
        if not 0 <= self.ic_percentile <= 100:
            errors.append("ic_percentile must be between 0 and 100")
        if not 0 < self.semantic_threshold < 1:
            errors.append("semantic_threshold must be between 0 and 1")
        if not 0 < self.duplicate_threshold <= 1:
            errors.append("duplicate_threshold must be between 0 and 1")
        if not 1 <= self.max_hierarchy_depth <= 10:
            errors.append("max_hierarchy_depth must be between 1 and 10")
        if not 0 < self.min_cluster_retention <= 1:
            errors.append("min_cluster_retention must be between 0 and 1")
        if self.batch_size < 1:
            errors.append("batch_size must be positive")
        if self.query_timeout < 1:
            errors.append("query_timeout must be positive")
        
        if errors:
            raise ValueError(f"Configuration validation failed: {'; '.join(errors)}")


# ============================================================
# ERROR HANDLING
# ============================================================

class CUIReductionError(Exception):
    """Base exception for CUI reduction errors"""
    pass

class DataValidationError(CUIReductionError):
    """Data validation error"""
    pass

class BigQueryError(CUIReductionError):
    """BigQuery operation error"""
    pass

class APIError(CUIReductionError):
    """API call error"""
    pass

class ConfigurationError(CUIReductionError):
    """Configuration error"""
    pass


# ============================================================
# BIGQUERY OPERATIONS (NO RECURSIVE SQL)
# ============================================================

class BigQueryOperations:
    """BigQuery operations with proper error handling and no recursive queries"""
    
    def __init__(self, config: ReductionConfig):
        self.config = config
        self.client = bigquery.Client(project=config.project_id)
        self.logger = logger.bind(component="bigquery")
        
    def validate_table_schema(self, table_name: str, required_columns: List[str]) -> None:
        """Validate table exists and has required columns"""
        try:
            table_ref = f"{self.config.project_id}.{self.config.dataset_id}.{table_name}"
            table = self.client.get_table(table_ref)
            
            existing_columns = {field.name for field in table.schema}
            missing_columns = set(required_columns) - existing_columns
            
            if missing_columns:
                raise DataValidationError(
                    f"Table {table_name} missing required columns: {missing_columns}"
                )
                
            self.logger.info(
                "table_schema_validated",
                table=table_name,
                columns=required_columns
            )
            
        except Exception as e:
            METRICS['errors'].labels(error_type='schema_validation', operation='validate_table').inc()
            self.logger.error(
                "table_schema_validation_failed",
                table=table_name,
                error=str(e),
                traceback=traceback.format_exc()
            )
            raise BigQueryError(f"Schema validation failed for {table_name}: {e}")
    
    def get_hierarchy_iterative(self, cui_list: List[str]) -> Dict[str, Any]:
        """
        Build hierarchy using iterative queries instead of recursive SQL
        """
        start_time = time.time()
        
        try:
            child_to_parents = defaultdict(list)
            parent_to_children = defaultdict(list)
            all_cuis = set(cui_list)
            visited = set()
            frontier = set(cui_list)
            
            for depth in range(self.config.max_hierarchy_depth):
                if not frontier:
                    break
                    
                self.logger.info(
                    "hierarchy_iteration",
                    depth=depth,
                    frontier_size=len(frontier)
                )
                
                # Batch process frontier
                batch_results = []
                for batch_start in range(0, len(list(frontier)), self.config.batch_size):
                    batch = list(frontier)[batch_start:batch_start + self.config.batch_size]
                    
                    # Simple non-recursive query
                    query = f"""
                    SELECT DISTINCT cui1, cui2, rel
                    FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.mrrel_table}`
                    WHERE cui1 IN UNNEST(@cuis) 
                       OR cui2 IN UNNEST(@cuis)
                    AND rel IN ('PAR', 'CHD')
                    """
                    
                    job_config = bigquery.QueryJobConfig(
                        query_parameters=[
                            bigquery.ArrayQueryParameter("cuis", "STRING", batch)
                        ]
                    )
                    
                    with METRICS['bigquery_queries'].labels(table='mrrel', status='pending').count_exceptions() as counter:
                        counter.inc()
                        query_job = self.client.query(query, job_config=job_config)
                        df = query_job.result(timeout=self.config.query_timeout).to_dataframe()
                        batch_results.append(df)
                
                # Process results
                import pandas as pd
                if batch_results:
                    combined_df = pd.concat(batch_results, ignore_index=True)
                    
                    next_frontier = set()
                    for _, row in combined_df.iterrows():
                        cui1, cui2, rel = str(row['cui1']), str(row['cui2']), str(row['rel'])
                        
                        # Build relationships
                        if rel == 'PAR':
                            parent_to_children[cui1].append(cui2)
                            child_to_parents[cui2].append(cui1)
                        elif rel == 'CHD':
                            parent_to_children[cui2].append(cui1)
                            child_to_parents[cui1].append(cui2)
                        
                        all_cuis.update([cui1, cui2])
                        
                        # Add unvisited CUIs to next frontier
                        for cui in [cui1, cui2]:
                            if cui not in visited:
                                next_frontier.add(cui)
                    
                    visited.update(frontier)
                    frontier = next_frontier - visited
                    
                    METRICS['hierarchy_depth'].observe(depth + 1)
            
            hierarchy = {
                'child_to_parents': dict(child_to_parents),
                'parent_to_children': dict(parent_to_children),
                'all_cuis': all_cuis
            }
            
            duration = time.time() - start_time
            METRICS['batch_processing_duration'].labels(operation='hierarchy_build').observe(duration)
            
            self.logger.info(
                "hierarchy_built",
                total_cuis=len(all_cuis),
                depth_reached=depth,
                duration_seconds=duration
            )
            
            return hierarchy
            
        except Exception as e:
            METRICS['errors'].labels(error_type='hierarchy_build', operation='get_hierarchy').inc()
            self.logger.error(
                "hierarchy_build_failed",
                error=str(e),
                traceback=traceback.format_exc()
            )
            raise BigQueryError(f"Failed to build hierarchy: {e}")
    
    def filter_by_sab(self, cuis: List[str]) -> List[str]:
        """Filter CUIs by allowed SAB codes"""
        try:
            filtered_cuis = []
            
            for batch_start in range(0, len(cuis), self.config.batch_size):
                batch = cuis[batch_start:batch_start + self.config.batch_size]
                
                query = f"""
                SELECT DISTINCT CUI
                FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.mrconso_table}`
                WHERE CUI IN UNNEST(@cuis)
                  AND SAB IN UNNEST(@sabs)
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("cuis", "STRING", batch),
                        bigquery.ArrayQueryParameter("sabs", "STRING", self.config.allowed_sab)
                    ]
                )
                
                with METRICS['bigquery_queries'].labels(table='mrconso', status='pending').count_exceptions() as counter:
                    counter.inc()
                    query_job = self.client.query(query, job_config=job_config)
                    df = query_job.result(timeout=self.config.query_timeout).to_dataframe()
                    filtered_cuis.extend(df['CUI'].tolist())
            
            self.logger.info(
                "sab_filtering_complete",
                input_count=len(cuis),
                output_count=len(filtered_cuis),
                reduction_pct=(1 - len(filtered_cuis)/len(cuis))*100 if cuis else 0
            )
            
            return filtered_cuis
            
        except Exception as e:
            METRICS['errors'].labels(error_type='sab_filter', operation='filter_by_sab').inc()
            self.logger.error(
                "sab_filtering_failed",
                error=str(e),
                traceback=traceback.format_exc()
            )
            raise BigQueryError(f"SAB filtering failed: {e}")
    
    def get_embeddings(self, cuis: List[str]) -> Dict[str, np.ndarray]:
        """Get embeddings for CUIs"""
        try:
            embeddings = {}
            
            for batch_start in range(0, len(cuis), self.config.batch_size):
                batch = cuis[batch_start:batch_start + self.config.batch_size]
                
                query = f"""
                SELECT cui, embedding
                FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.cui_embeddings_table}`
                WHERE cui IN UNNEST(@cuis)
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("cuis", "STRING", batch)
                    ]
                )
                
                with METRICS['bigquery_queries'].labels(table='embeddings', status='pending').count_exceptions() as counter:
                    counter.inc()
                    query_job = self.client.query(query, job_config=job_config)
                    df = query_job.result(timeout=self.config.query_timeout).to_dataframe()
                    
                    for _, row in df.iterrows():
                        embeddings[row['cui']] = np.array(row['embedding'])
            
            self.logger.info(
                "embeddings_retrieved",
                requested=len(cuis),
                retrieved=len(embeddings)
            )
            
            return embeddings
            
        except Exception as e:
            METRICS['errors'].labels(error_type='embeddings', operation='get_embeddings').inc()
            self.logger.error(
                "embeddings_retrieval_failed",
                error=str(e),
                traceback=traceback.format_exc()
            )
            raise BigQueryError(f"Failed to retrieve embeddings: {e}")


# ============================================================
# API CLIENT (NO RUNTIME GCLOUD)
# ============================================================

class CUIExtractionClient:
    """API client using service account key instead of runtime gcloud"""
    
    def __init__(self, config: ReductionConfig):
        self.config = config
        self.session = requests.Session()
        self.logger = logger.bind(component="api_client")
        
        # Use service account key for authentication
        self.headers = {
            "Authorization": f"Bearer {config.api_key}",
            "Content-Type": "application/json"
        }
    
    def extract_cuis(self, texts: List[str]) -> Dict[str, List[str]]:
        """Extract CUIs from texts using API"""
        start_time = time.time()
        
        try:
            # Validate request
            request = APIContract.CUIExtractionRequest(
                query_texts=texts,
                top_k=3
            )
            request.validate()
            
            # Make API call
            with METRICS['api_calls'].labels(endpoint='extract', status='pending').count_exceptions() as counter:
                counter.inc()
                
                response = self.session.post(
                    self.config.api_url,
                    json=asdict(request),
                    headers=self.headers,
                    timeout=30
                )
                
                response.raise_for_status()
                
                # Validate response
                response_data = response.json()
                api_response = APIContract.CUIExtractionResponse(**response_data)
                api_response.validate()
                
                duration = time.time() - start_time
                METRICS['cui_extraction_duration'].observe(duration)
                
                self.logger.info(
                    "cuis_extracted",
                    text_count=len(texts),
                    duration_seconds=duration,
                    model_version=api_response.model_version
                )
                
                return api_response.results
                
        except requests.RequestException as e:
            METRICS['errors'].labels(error_type='api_request', operation='extract_cuis').inc()
            self.logger.error(
                "api_request_failed",
                error=str(e),
                traceback=traceback.format_exc()
            )
            raise APIError(f"API request failed: {e}")
        except Exception as e:
            METRICS['errors'].labels(error_type='api_validation', operation='extract_cuis').inc()
            self.logger.error(
                "api_validation_failed",
                error=str(e),
                traceback=traceback.format_exc()
            )
            raise APIError(f"API validation failed: {e}")


# ============================================================
# CUI REDUCER WITH OBSERVABILITY
# ============================================================

class ProductionCUIReducer:
    """Production CUI reducer with full observability"""
    
    def __init__(self, config: ReductionConfig):
        # Validate configuration
        config.validate()
        self.config = config
        
        # Initialize components
        self.bq_ops = BigQueryOperations(config)
        self.api_client = CUIExtractionClient(config)
        self.logger = logger.bind(component="reducer")
        
        # Validate table schemas on initialization
        self._validate_all_schemas()
        
        # Initialize cache if enabled
        if config.cache_enabled:
            self.cache = defaultdict(dict)
        else:
            self.cache = None
    
    def _validate_all_schemas(self) -> None:
        """Validate all required table schemas"""
        self.logger.info("validating_table_schemas")
        
        validations = [
            (self.config.mrrel_table, TableSchema.REQUIRED_COLUMNS['mrrel']),
            (self.config.mrconso_table, TableSchema.REQUIRED_COLUMNS['mrconso']),
            (self.config.cui_embeddings_table, TableSchema.REQUIRED_COLUMNS['cui_embeddings']),
            (self.config.cui_descriptions_table, TableSchema.REQUIRED_COLUMNS['cui_descriptions'])
        ]
        
        for table_name, required_columns in validations:
            self.bq_ops.validate_table_schema(table_name, required_columns)
    
    def reduce(self, texts: List[str]) -> Dict[str, Any]:
        """Main reduction pipeline with full observability"""
        
        pipeline_start = time.time()
        pipeline_id = f"pipeline_{int(time.time()*1000)}"
        
        self.logger.info(
            "pipeline_started",
            pipeline_id=pipeline_id,
            text_count=len(texts)
        )
        
        try:
            # Step 1: Extract CUIs
            self.logger.info("step_1_extract_cuis", pipeline_id=pipeline_id)
            text_cui_mapping = self.api_client.extract_cuis(texts)
            all_cuis = set()
            for cuis in text_cui_mapping.values():
                all_cuis.update(cuis)
            initial_count = len(all_cuis)
            
            METRICS['cui_extraction_total'].labels(status='success').inc()
            
            # Step 2: Filter by SAB
            self.logger.info("step_2_filter_sab", pipeline_id=pipeline_id)
            filtered_cuis = self.bq_ops.filter_by_sab(list(all_cuis))
            after_filter = len(filtered_cuis)
            
            # Step 3: Build hierarchy
            self.logger.info("step_3_build_hierarchy", pipeline_id=pipeline_id)
            hierarchy = self.bq_ops.get_hierarchy_iterative(filtered_cuis)
            
            # Step 4: Compute IC scores
            self.logger.info("step_4_compute_ic", pipeline_id=pipeline_id)
            ic_scores = self._compute_ic_scores(hierarchy)
            
            # Step 5: Semantic rollup
            self.logger.info("step_5_semantic_rollup", pipeline_id=pipeline_id)
            ic_threshold = np.percentile(list(ic_scores.values()), self.config.ic_percentile)
            rolled_up = self._semantic_rollup(filtered_cuis, hierarchy, ic_scores, ic_threshold)
            after_rollup = len(rolled_up)
            
            # Step 6: Semantic clustering
            self.logger.info("step_6_clustering", pipeline_id=pipeline_id)
            final_cuis = self._semantic_clustering(rolled_up)
            final_count = len(final_cuis)
            
            # Calculate metrics
            reduction_rate = (initial_count - final_count) / initial_count if initial_count else 0
            METRICS['reduction_rate'].set(reduction_rate)
            
            pipeline_duration = time.time() - pipeline_start
            
            results = {
                'pipeline_id': pipeline_id,
                'initial_count': initial_count,
                'after_filter': after_filter,
                'after_rollup': after_rollup,
                'final_count': final_count,
                'reduction_rate': reduction_rate,
                'duration_seconds': pipeline_duration,
                'final_cuis': final_cuis,
                'text_cui_mapping': text_cui_mapping
            }
            
            self.logger.info(
                "pipeline_completed",
                pipeline_id=pipeline_id,
                initial_count=initial_count,
                final_count=final_count,
                reduction_rate=reduction_rate,
                duration=pipeline_duration
            )
            
            return results
            
        except Exception as e:
            METRICS['errors'].labels(error_type='pipeline', operation='reduce').inc()
            self.logger.error(
                "pipeline_failed",
                pipeline_id=pipeline_id,
                error=str(e),
                traceback=traceback.format_exc()
            )
            raise CUIReductionError(f"Pipeline failed: {e}")
    
    def _compute_ic_scores(self, hierarchy: Dict) -> Dict[str, float]:
        """Compute information content scores"""
        parent_to_children = hierarchy.get('parent_to_children', {})
        all_cuis = hierarchy.get('all_cuis', set())
        total = len(all_cuis)
        
        if total == 0:
            return {}
        
        # Count descendants for each CUI
        descendant_counts = {}
        for cui in all_cuis:
            count = self._count_descendants_iterative(cui, parent_to_children)
            descendant_counts[cui] = count
        
        # Compute IC scores
        ic_scores = {}
        for cui, count in descendant_counts.items():
            ic = -np.log((count + 1) / total)
            ic_scores[cui] = max(0.0, ic)
        
        return ic_scores
    
    def _count_descendants_iterative(self, cui: str, parent_to_children: Dict) -> int:
        """Count descendants iteratively (not recursively)"""
        visited = set()
        stack = [cui]
        count = 0
        
        while stack:
            current = stack.pop()
            if current in visited:
                continue
            visited.add(current)
            
            children = parent_to_children.get(current, [])
            count += len(children)
            stack.extend(children)
        
        return count
    
    def _semantic_rollup(self, cuis: List[str], hierarchy: Dict, ic_scores: Dict, threshold: float) -> List[str]:
        """Perform semantic rollup"""
        child_to_parents = hierarchy.get('child_to_parents', {})
        rolled_up = {}
        
        for cui in cuis:
            # Find ancestors iteratively
            ancestors = self._get_ancestors_iterative(cui, child_to_parents)
            
            # Find lowest informative ancestor
            candidates = [cui] + ancestors
            valid = [c for c in candidates if ic_scores.get(c, 0) >= threshold]
            
            if valid:
                rolled_up[cui] = min(valid, key=lambda c: ic_scores.get(c, float('inf')))
            else:
                rolled_up[cui] = cui
        
        return list(set(rolled_up.values()))
    
    def _get_ancestors_iterative(self, cui: str, child_to_parents: Dict) -> List[str]:
        """Get ancestors iteratively (not recursively)"""
        ancestors = []
        visited = set()
        queue = deque([cui])
        
        while queue and len(visited) < 100:  # Limit to prevent infinite loops
            current = queue.popleft()
            if current in visited:
                continue
            visited.add(current)
            
            parents = child_to_parents.get(current, [])
            ancestors.extend(parents)
            queue.extend(parents)
        
        return ancestors
    
    def _semantic_clustering(self, cuis: List[str]) -> List[str]:
        """Perform semantic clustering"""
        if len(cuis) <= 1:
            return cuis
        
        # Get embeddings
        embeddings_dict = self.bq_ops.get_embeddings(cuis)
        
        if len(embeddings_dict) < 2:
            return cuis
        
        # Prepare for clustering
        cuis_with_embeddings = list(embeddings_dict.keys())
        embeddings_matrix = np.vstack([embeddings_dict[cui] for cui in cuis_with_embeddings])
        
        # Perform clustering
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1 - self.config.duplicate_threshold,
            metric='cosine',
            linkage='average'
        )
        
        labels = clustering.fit_predict(embeddings_matrix)
        
        # Process clusters
        final_cuis = []
        for cluster_id in np.unique(labels):
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_cuis = [cuis_with_embeddings[i] for i in cluster_indices]
            
            if len(cluster_cuis) == 1:
                final_cuis.append(cluster_cuis[0])
            else:
                # Keep diverse CUIs from cluster
                unique_cuis = self._select_diverse_cuis(cluster_cuis, embeddings_matrix[cluster_indices])
                final_cuis.extend(unique_cuis)
        
        # Add back CUIs without embeddings
        cuis_without_embeddings = set(cuis) - set(cuis_with_embeddings)
        final_cuis.extend(cuis_without_embeddings)
        
        return final_cuis
    
    def _select_diverse_cuis(self, cluster_cuis: List[str], embeddings: np.ndarray) -> List[str]:
        """Select diverse CUIs from cluster"""
        n = len(cluster_cuis)
        if n <= 1:
            return cluster_cuis
        
        # Keep CUIs that are sufficiently different
        to_keep = [cluster_cuis[0]]
        
        for i in range(1, n):
            is_duplicate = False
            for j in range(len(to_keep)):
                kept_idx = cluster_cuis.index(to_keep[j])
                similarity = np.dot(embeddings[i], embeddings[kept_idx]) / (
                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[kept_idx])
                )
                if similarity > self.config.duplicate_threshold + 0.03:  # Stricter threshold
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                to_keep.append(cluster_cuis[i])
        
        # Ensure minimum retention
        min_keep = max(1, int(len(cluster_cuis) * self.config.min_cluster_retention))
        if len(to_keep) < min_keep:
            remaining = [c for c in cluster_cuis if c not in to_keep]
            to_keep.extend(remaining[:min_keep - len(to_keep)])
        
        return to_keep


# ============================================================
# MAIN EXECUTION
# ============================================================

def main():
    """Main execution with proper configuration and error handling"""
    
    # Load configuration from environment or file
    import os
    import yaml
    
    config_path = os.getenv('CUI_CONFIG_PATH', 'config.yaml')
    
    try:
        with open(config_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        config = ReductionConfig(**config_dict)
        
        # Initialize reducer
        reducer = ProductionCUIReducer(config)
        
        # Example texts
        texts = [
            "Patient with brain tumor and hypertension",
            "Diabetes mellitus type 2 with complications"
        ]
        
        # Run reduction
        results = reducer.reduce(texts)
        
        print(json.dumps(results, indent=2))
        
    except FileNotFoundError:
        logger.error(f"Configuration file not found: {config_path}")
        raise ConfigurationError(f"Configuration file not found: {config_path}")
    except Exception as e:
        logger.error(f"Failed to run CUI reduction: {e}", traceback=traceback.format_exc())
        raise


if __name__ == "__main__":
    main()
