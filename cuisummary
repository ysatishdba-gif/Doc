"""
Enhanced CUI Reduction System with Information Content (IC) based Semantic Rollup
Achieves 80-90% reduction using:
  Stage 1: IC-based Semantic Rollup to Lowest Informative Ancestor (50-70%)
  Stage 2: Semantic Clustering of similar concepts (additional 20-30%)
"""

import os
import time
import numpy as np
import logging
import subprocess
import json
from typing import List, Set, Dict, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ReductionStats:
    """Statistics for the reduction process"""
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float


class CUIAPIClient:
    """Client for GCP-based CUI extraction API with retry logic"""
    
    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3):
        """
        Initialize CUI API client with GCP authentication
        
        Args:
            api_base_url: Base URL for the NER API
            timeout: Request timeout in seconds
            top_k: Number of top CUIs to retrieve per mention
        """
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.headers = None
        
        # Configure session with retry logic
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Authenticate with GCP
        self._update_gcp_token()
    
    def _update_gcp_token(self):
        """Get GCP identity token and update headers"""
        try:
            result = subprocess.run(
                ['gcloud', 'auth', 'print-identity-token'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,
                timeout=10
            )
            
            if result.returncode != 0:
                raise Exception(f"Cannot get GCP access token: {result.stderr}")
            
            identity_token = result.stdout.strip()
            self.headers = {
                "Authorization": f"Bearer {identity_token}",
                "Content-Type": "application/json"
            }
            logger.info("GCP authentication token updated successfully")
            
        except subprocess.TimeoutExpired:
            raise Exception("GCP authentication timed out")
        except FileNotFoundError:
            raise Exception("gcloud CLI not found. Please install Google Cloud SDK")
        except Exception as e:
            raise Exception(f"Failed to authenticate with GCP: {str(e)}")
    
    def extract_cuis(self, text: str) -> Set[str]:
        """Extract CUIs from a single text"""
        return self.extract_cuis_batch([text])
    
    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        """
        Extract CUIs from multiple texts in a single batch request
        
        Args:
            texts: List of text strings to process
            retry_auth: Whether to retry with fresh token on auth failure
        
        Returns:
            Set of unique CUIs extracted from all texts
        """
        if not texts:
            return set()
        
        payload = {
            "query_texts": texts,
            "top_k": self.top_k
        }
        
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=self.headers,
                timeout=self.timeout
            )
            
            # Handle token expiration
            if response.status_code == 401 and retry_auth:
                logger.warning("GCP token expired, refreshing...")
                self._update_gcp_token()
                return self.extract_cuis_batch(texts, retry_auth=False)
            
            response.raise_for_status()
            data = response.json()
            
            # Extract all CUIs from response
            all_cuis = []
            for text in texts:
                cuis = data.get(text, [])
                all_cuis.extend(cuis)
            
            # Deduplicate
            unique_cuis = set(all_cuis)
            logger.info(f"Extracted {len(unique_cuis)} unique CUIs from {len(texts)} texts")
            
            return unique_cuis
            
        except requests.exceptions.HTTPError as e:
            logger.error(f"API HTTP error: {e}")
            if hasattr(e.response, 'text'):
                logger.error(f"Response: {e.response.text}")
            return set()
        except requests.exceptions.RequestException as e:
            logger.error(f"API request failed: {e}")
            return set()
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            return set()
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            return set()
    
    def extract_cuis_parallel(
        self, 
        texts: List[str], 
        batch_size: int = 50,
        max_workers: int = 5
    ) -> Set[str]:
        """
        Extract CUIs from large text collections using parallel batch processing
        
        Args:
            texts: List of texts to process
            batch_size: Number of texts per batch
            max_workers: Number of parallel workers
        
        Returns:
            Set of unique CUIs
        """
        if not texts:
            return set()
        
        # Split texts into batches
        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]
        logger.info(f"Processing {len(texts)} texts in {len(batches)} batches with {max_workers} workers")
        
        all_cuis = set()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_batch = {
                executor.submit(self.extract_cuis_batch, batch): batch 
                for batch in batches
            }
            
            for future in as_completed(future_to_batch):
                try:
                    cuis = future.result()
                    all_cuis.update(cuis)
                except Exception as e:
                    batch = future_to_batch[future]
                    logger.error(f"Failed to process batch of {len(batch)} texts: {str(e)}")
        
        logger.info(f"Total unique CUIs extracted: {len(all_cuis)}")
        return all_cuis


class EnhancedCUIReducer:
    """
    Enhanced CUI reduction engine using IC-based semantic rollup + semantic clustering
    
    Two-stage approach:
    1. IC-based Semantic Rollup: Roll up to Lowest Informative Ancestor
    2. Semantic Clustering: Cluster and merge similar mid-level concepts
    """
    
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        mrrel_table: str = "MRREL",
        cui_description_table: str = "cui_descriptions",
        cui_embeddings_table: str = "cui_embeddings",
        cui_narrower_table: str = "cui_narrower_concepts"
    ):
        """
        Initialize the enhanced CUI reducer
        
        Args:
            project_id: GCP project ID
            dataset_id: BigQuery dataset ID
            mrrel_table: MRREL table name
            cui_description_table: CUI description table name
            cui_embeddings_table: CUI embeddings table name
            cui_narrower_table: CUI narrower concepts table name
        """
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.mrrel_table = mrrel_table
        self.cui_description_table = cui_description_table
        self.cui_embeddings_table = cui_embeddings_table
        self.cui_narrower_table = cui_narrower_table
        
        # Cache for hierarchy and IC scores
        self._hierarchy_cache = None
        self._ic_scores_cache = None
        
        logger.info(f"Initialized EnhancedCUIReducer for project: {project_id}, dataset: {dataset_id}")
    
    def reduce(
        self,
        input_cuis: List[str],
        target_reduction: float = 0.85,
        ic_threshold: Optional[float] = None,
        ic_percentile: float = 50.0,
        semantic_threshold: float = 0.88,
        use_semantic_clustering: bool = True,
        adaptive_threshold: bool = False
    ) -> Tuple[List[str], ReductionStats]:
        """
        Main reduction pipeline with IC-based rollup + semantic clustering
        
        Args:
            input_cuis: List of CUIs to reduce
            target_reduction: Target reduction percentage (0.85 = 85%)
            ic_threshold: Explicit IC threshold (if None, computed from percentile)
            ic_percentile: Percentile for IC threshold (50 = median)
            semantic_threshold: Cosine similarity threshold for clustering
            use_semantic_clustering: Whether to use Stage 2 clustering
            adaptive_threshold: Dynamically adjust IC threshold to reach target
        
        Returns:
            Tuple of (reduced CUI list, statistics)
        """
        start_time = time.time()
        initial_count = len(input_cuis)
        
        logger.info(f"Starting enhanced reduction for {initial_count} CUIs")
        logger.info(f"Target reduction: {target_reduction*100:.1f}%")
        
        # Build hierarchy and compute IC scores (cached)
        logger.info("Building hierarchy and computing IC scores...")
        hierarchy = self._build_hierarchy(input_cuis)
        ic_scores = self._compute_ic_scores(hierarchy)
        
        # Determine IC threshold
        if ic_threshold is None:
            ic_values = list(ic_scores.values())
            if adaptive_threshold:
                ic_threshold = self._find_adaptive_threshold(
                    input_cuis, hierarchy, ic_scores, target_reduction
                )
            else:
                ic_threshold = np.percentile(ic_values, ic_percentile)
        
        logger.info(f"Using IC threshold: {ic_threshold:.3f}")
        
        # Stage 1: IC-based Semantic Rollup
        logger.info("Stage 1: IC-based Semantic Rollup to Lowest Informative Ancestor...")
        rolled_up_cuis = self._semantic_rollup_with_ic(
            input_cuis, hierarchy, ic_scores, ic_threshold
        )
        after_rollup = len(rolled_up_cuis)
        rollup_reduction = 1 - (after_rollup / initial_count)
        
        logger.info(f"After IC rollup: {after_rollup} CUIs ({rollup_reduction*100:.1f}% reduction)")
        
        # Stage 2: Semantic Clustering (if needed and enabled)
        clustering_reduction = 0.0
        if use_semantic_clustering and rollup_reduction < target_reduction:
            logger.info("Stage 2: Semantic Clustering of similar concepts...")
            final_cuis = self._semantic_clustering(
                rolled_up_cuis, ic_scores, semantic_threshold
            )
            final_count = len(final_cuis)
            clustering_reduction = (after_rollup - final_count) / initial_count
            logger.info(f"After clustering: {final_count} CUIs ({clustering_reduction*100:.1f}% additional reduction)")
        else:
            final_cuis = rolled_up_cuis
            final_count = after_rollup
            if not use_semantic_clustering:
                logger.info("Semantic clustering disabled")
            else:
                logger.info(f"Target reduction achieved, skipping clustering stage")
        
        # Calculate statistics
        total_reduction = 1 - (final_count / initial_count)
        processing_time = time.time() - start_time
        
        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=after_rollup,
            final_count=final_count,
            ic_rollup_reduction_pct=rollup_reduction * 100,
            semantic_clustering_reduction_pct=clustering_reduction * 100,
            total_reduction_pct=total_reduction * 100,
            processing_time=processing_time,
            ic_threshold_used=ic_threshold
        )
        
        logger.info(f"Reduction complete: {initial_count} → {final_count} CUIs")
        logger.info(f"Total reduction: {total_reduction*100:.1f}%")
        logger.info(f"Processing time: {processing_time:.2f}s")
        
        return final_cuis, stats
    
    def _build_hierarchy(self, relevant_cuis: List[str]) -> Dict[str, Dict]:
        """
        Build parent-child hierarchy from MRREL table
        
        Returns:
            Dict with structure:
            {
                'child_to_parents': {cui: [parent_cuis]},
                'parent_to_children': {cui: [child_cuis]},
                'all_cuis': set of all CUIs in hierarchy
            }
        """
        if self._hierarchy_cache is not None:
            return self._hierarchy_cache
        
        # Query MRREL for parent-child relationships
        query = f"""
        WITH input_cuis AS (
          SELECT DISTINCT cui FROM UNNEST(@cui_list) AS cui
        ),
        -- Get all relationships where input CUIs are involved
        relevant_relations AS (
          SELECT DISTINCT cui1, cui2, rel
          FROM `{self.project_id}.{self.dataset_id}.{self.mrrel_table}`
          WHERE (cui1 IN (SELECT cui FROM input_cuis) OR cui2 IN (SELECT cui FROM input_cuis))
            AND rel IN ('PAR', 'CHD')
        ),
        -- Expand to get full ancestor chains
        expanded_relations AS (
          SELECT DISTINCT m.cui1, m.cui2, m.rel
          FROM `{self.project_id}.{self.dataset_id}.{self.mrrel_table}` m
          WHERE m.rel IN ('PAR', 'CHD')
            AND (
              m.cui1 IN (SELECT cui1 FROM relevant_relations) OR
              m.cui1 IN (SELECT cui2 FROM relevant_relations) OR
              m.cui2 IN (SELECT cui1 FROM relevant_relations) OR
              m.cui2 IN (SELECT cui2 FROM relevant_relations)
            )
        )
        SELECT cui1, cui2, rel FROM expanded_relations
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cui_list", "STRING", relevant_cuis)
            ]
        )
        
        try:
            df = self.client.query(query, job_config=job_config).to_dataframe()
            
            child_to_parents = defaultdict(list)
            parent_to_children = defaultdict(list)
            all_cuis = set()
            
            for _, row in df.iterrows():
                cui1, cui2, rel = row['cui1'], row['cui2'], row['rel']
                
                if rel == 'PAR':  # cui1 is parent of cui2
                    parent_to_children[cui1].append(cui2)
                    child_to_parents[cui2].append(cui1)
                    all_cuis.add(cui1)
                    all_cuis.add(cui2)
                elif rel == 'CHD':  # cui1 is child of cui2
                    parent_to_children[cui2].append(cui1)
                    child_to_parents[cui1].append(cui2)
                    all_cuis.add(cui1)
                    all_cuis.add(cui2)
            
            hierarchy = {
                'child_to_parents': dict(child_to_parents),
                'parent_to_children': dict(parent_to_children),
                'all_cuis': all_cuis
            }
            
            self._hierarchy_cache = hierarchy
            logger.info(f"Built hierarchy with {len(all_cuis)} total CUIs")
            
            return hierarchy
            
        except Exception as e:
            logger.error(f"Failed to build hierarchy: {str(e)}")
            return {
                'child_to_parents': {},
                'parent_to_children': {},
                'all_cuis': set(relevant_cuis)
            }
    
    def _compute_ic_scores(self, hierarchy: Dict) -> Dict[str, float]:
        """
        Compute structural Information Content (IC) for all CUIs
        
        IC(cui) = -log((# descendants + 1) / # all CUIs)
        
        Higher IC = more specific (fewer descendants)
        Lower IC = more general (many descendants)
        """
        if self._ic_scores_cache is not None:
            return self._ic_scores_cache
        
        parent_to_children = hierarchy['parent_to_children']
        all_cuis = hierarchy['all_cuis']
        total_cuis = len(all_cuis)
        
        if total_cuis == 0:
            return {}
        
        # Count descendants for each CUI
        descendant_counts = {}
        
        def count_descendants(cui: str, visited: Set[str] = None) -> int:
            """Recursively count all descendants"""
            if visited is None:
                visited = set()
            
            if cui in visited:
                return 0
            visited.add(cui)
            
            if cui in descendant_counts:
                return descendant_counts[cui]
            
            children = parent_to_children.get(cui, [])
            count = len(children)
            
            for child in children:
                count += count_descendants(child, visited)
            
            descendant_counts[cui] = count
            return count
        
        # Compute descendant counts for all CUIs
        logger.info("Computing descendant counts...")
        for cui in all_cuis:
            if cui not in descendant_counts:
                count_descendants(cui)
        
        # Compute IC scores
        ic_scores = {}
        for cui in all_cuis:
            desc_count = descendant_counts.get(cui, 0)
            ic = -np.log((desc_count + 1) / total_cuis)
            ic_scores[cui] = ic
        
        self._ic_scores_cache = ic_scores
        
        # Log statistics
        ic_values = list(ic_scores.values())
        logger.info(f"Computed IC scores for {len(ic_scores)} CUIs")
        logger.info(f"IC range: [{min(ic_values):.3f}, {max(ic_values):.3f}]")
        logger.info(f"IC median: {np.median(ic_values):.3f}")
        
        return ic_scores
    
    def _get_ancestors(self, cui: str, hierarchy: Dict) -> List[str]:
        """Get all ancestors of a CUI (breadth-first traversal)"""
        child_to_parents = hierarchy['child_to_parents']
        ancestors = []
        visited = set()
        queue = deque([cui])
        
        while queue:
            current = queue.popleft()
            if current in visited:
                continue
            visited.add(current)
            
            parents = child_to_parents.get(current, [])
            for parent in parents:
                if parent not in visited:
                    ancestors.append(parent)
                    queue.append(parent)
        
        return ancestors
    
    def _semantic_rollup_with_ic(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        ic_threshold: float
    ) -> List[str]:
        """
        Roll up CUIs to their Lowest Informative Ancestor (LIA)
        
        For each CUI:
        1. Get all ancestors
        2. Filter ancestors with IC >= threshold
        3. Select ancestor with lowest IC above threshold (most general while still informative)
        4. If no valid ancestor, keep original CUI
        """
        rolled_up = {}
        
        for cui in cui_list:
            # Get all ancestors
            ancestors = self._get_ancestors(cui, hierarchy)
            
            # Add the CUI itself as a candidate
            candidates = [cui] + ancestors
            
            # Filter by IC threshold
            valid_candidates = [
                c for c in candidates 
                if ic_scores.get(c, 0) >= ic_threshold
            ]
            
            if valid_candidates:
                # Select the one with lowest IC (most general while still informative)
                lia = min(valid_candidates, key=lambda c: ic_scores.get(c, float('inf')))
                rolled_up[cui] = lia
            else:
                # No valid ancestor, keep original
                rolled_up[cui] = cui
        
        # Return unique rolled-up CUIs
        unique_rolled_up = list(set(rolled_up.values()))
        
        # Log rollup statistics
        rollup_groups = defaultdict(list)
        for original, rolled in rolled_up.items():
            rollup_groups[rolled].append(original)
        
        merged_groups = [g for g in rollup_groups.values() if len(g) > 1]
        if merged_groups:
            logger.info(f"Rolled up {len(cui_list)} CUIs into {len(unique_rolled_up)} groups")
            logger.info(f"Largest rollup group: {max(len(g) for g in merged_groups)} CUIs")
        
        return unique_rolled_up
    
    def _semantic_clustering(
        self,
        cui_list: List[str],
        ic_scores: Dict[str, float],
        similarity_threshold: float = 0.88
    ) -> List[str]:
        """
        Cluster semantically similar CUIs and keep one representative per cluster
        Prefers CUIs with lower IC (more general) as cluster representatives
        """
        if len(cui_list) <= 1:
            return cui_list
        
        logger.info(f"Fetching embeddings for {len(cui_list)} CUIs...")
        
        # Fetch embeddings from BigQuery
        query = f"""
        SELECT cui, embedding
        FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
        WHERE cui IN UNNEST(@cui_list)
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cui_list", "STRING", cui_list)
            ]
        )
        
        try:
            df = self.client.query(query, job_config=job_config).to_dataframe()
            
            if len(df) == 0:
                logger.warning("No embeddings found, skipping semantic clustering")
                return cui_list
            
            # Convert embeddings to numpy array
            embeddings = np.vstack(df['embedding'].values)
            cuis = df['cui'].values
            
            logger.info(f"Retrieved embeddings for {len(cuis)} CUIs")
            
            # Perform clustering
            logger.info("Clustering similar CUIs...")
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - similarity_threshold,
                metric='cosine',
                linkage='average'
            )
            
            labels = clustering.fit_predict(embeddings)
            n_clusters = len(np.unique(labels))
            
            logger.info(f"Found {n_clusters} clusters from {len(cuis)} CUIs")
            
            # Keep one representative per cluster (prefer lower IC = more general)
            final_cuis = []
            for cluster_id in np.unique(labels):
                cluster_mask = labels == cluster_id
                cluster_cuis = cuis[cluster_mask].tolist()
                
                if len(cluster_cuis) == 1:
                    final_cuis.append(cluster_cuis[0])
                else:
                    # Pick the one with lowest IC (most general in cluster)
                    representative = min(
                        cluster_cuis,
                        key=lambda c: ic_scores.get(c, float('inf'))
                    )
                    final_cuis.append(representative)
                    
                    logger.debug(f"Cluster {cluster_id}: {len(cluster_cuis)} CUIs → {representative}")
            
            return final_cuis
            
        except Exception as e:
            logger.error(f"Semantic clustering failed: {str(e)}")
            return cui_list
    
    def _find_adaptive_threshold(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        target_reduction: float
    ) -> float:
        """
        Dynamically find IC threshold to achieve target reduction
        """
        logger.info("Finding adaptive IC threshold...")
        
        ic_values = sorted(ic_scores.values())
        
        # Try different percentiles
        for percentile in [50, 40, 30, 25, 20, 15, 10]:
            threshold = np.percentile(ic_values, percentile)
            rolled_up = self._semantic_rollup_with_ic(cui_list, hierarchy, ic_scores, threshold)
            reduction = 1 - len(rolled_up) / len(cui_list)
            
            logger.debug(f"Percentile {percentile} (threshold={threshold:.3f}): {reduction*100:.1f}% reduction")
            
            if reduction >= target_reduction * 0.9:  # Within 90% of target
                logger.info(f"Selected IC threshold: {threshold:.3f} (percentile {percentile})")
                return threshold
        
        # If still not enough, use lowest percentile
        threshold = np.percentile(ic_values, 5)
        logger.info(f"Using minimum IC threshold: {threshold:.3f}")
        return threshold
    
    def get_cui_descriptions(self, cui_list: List[str]) -> Dict[str, str]:
        """Retrieve descriptions for a list of CUIs"""
        if not cui_list:
            return {}
        
        query = f"""
        SELECT cui, description
        FROM `{self.project_id}.{self.dataset_id}.{self.cui_description_table}`
        WHERE cui IN UNNEST(@cui_list)
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cui_list", "STRING", cui_list)
            ]
        )
        
        try:
            df = self.client.query(query, job_config=job_config).to_dataframe()
            return dict(zip(df['cui'], df['description']))
        except Exception as e:
            logger.error(f"Failed to fetch descriptions: {str(e)}")
            return {}
    
    def get_ic_scores(self, cui_list: List[str]) -> Dict[str, float]:
        """Get IC scores for a list of CUIs"""
        if self._ic_scores_cache is None:
            hierarchy = self._build_hierarchy(cui_list)
            self._compute_ic_scores(hierarchy)
        
        return {cui: self._ic_scores_cache.get(cui, 0.0) for cui in cui_list}


class CUIReductionPipeline:
    """End-to-end pipeline: text extraction → reduction → results"""
    
    def __init__(
        self,
        api_client: CUIAPIClient,
        cui_reducer: EnhancedCUIReducer
    ):
        self.api_client = api_client
        self.cui_reducer = cui_reducer
    
    def process_texts(
        self,
        texts: List[str],
        target_reduction: float = 0.85,
        ic_percentile: float = 50.0,
        semantic_threshold: float = 0.88,
        use_semantic_clustering: bool = True,
        adaptive_threshold: bool = False,
        batch_size: int = 50,
        max_workers: int = 5
    ) -> Tuple[List[str], Dict[str, str], ReductionStats]:
        """
        Complete pipeline: extract CUIs from texts and reduce them
        
        Args:
            texts: List of text strings to process
            target_reduction: Target reduction percentage
            ic_percentile: IC percentile for threshold
            semantic_threshold: Similarity threshold for clustering
            use_semantic_clustering: Enable Stage 2
            adaptive_threshold: Auto-find IC threshold
            batch_size: Texts per API batch
            max_workers: Parallel workers for API calls
        
        Returns:
            Tuple of (reduced CUIs, CUI descriptions, statistics)
        """
        # Step 1: Extract CUIs from texts
        logger.info(f"Processing {len(texts)} input texts...")
        
        if len(texts) > batch_size:
            # Use parallel processing for large text collections
            initial_cuis = self.api_client.extract_cuis_parallel(
                texts, 
                batch_size=batch_size,
                max_workers=max_workers
            )
        else:
            # Single batch for small collections
            initial_cuis = self.api_client.extract_cuis_batch(texts)
        
        if not initial_cuis:
            logger.warning("No CUIs extracted from texts")
            return [], {}, None
        
        # Step 2: Reduce CUIs
        reduced_cuis, stats = self.cui_reducer.reduce(
            list(initial_cuis),
            target_reduction=target_reduction,
            ic_percentile=ic_percentile,
            semantic_threshold=semantic_threshold,
            use_semantic_clustering=use_semantic_clustering,
            adaptive_threshold=adaptive_threshold
        )
        
        # Step 3: Get descriptions for final CUIs
        logger.info("Fetching descriptions for reduced CUIs...")
        descriptions = self.cui_reducer.get_cui_descriptions(reduced_cuis)
        
        return reduced_cuis, descriptions, stats


def main():
    """Example usage"""
    
    # Configuration
    PROJECT_ID = "your-gcp-project-id"
    DATASET_ID = "your-dataset-id"
    NER_API_URL = "https://your-ner-api-url.com/endpoint"
    
    # Initialize components
    api_client = CUIAPIClient(
        api_base_url=NER_API_URL,
        timeout=60,
        top_k=3
    )
    
    cui_reducer = EnhancedCUIReducer(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        mrrel_table="MRREL",
        cui_description_table="cui_descriptions",
        cui_embeddings_table="cui_embeddings",
        cui_narrower_table="cui_narrower_concepts"
    )
    
    # Create pipeline
    pipeline = CUIReductionPipeline(api_client, cui_reducer)
    
    # Example texts
    texts = [
        "Patient presents with severe chest pain and shortness of breath.",
        "History of Type 2 Diabetes Mellitus and hypertension.",
        "Prescribed metformin for blood sugar control."
    ]
    
    # Process with enhanced reduction
    reduced_cuis, descriptions, stats = pipeline.process_texts(
        texts,
        target_reduction=0.85,
        ic_percentile=50.0,
        semantic_threshold=0.88,
        use_semantic_clustering=True,
        adaptive_threshold=False,
        batch_size=50,
        max_workers=5
    )
    
    # Display results
    print("\n" + "="*80)
    print("ENHANCED REDUCTION RESULTS")
    print("="*80)
    print(f"Initial CUIs: {stats.initial_count}")
    print(f"After IC Rollup: {stats.after_ic_rollup} ({stats.ic_rollup_reduction_pct:.1f}% reduction)")
    print(f"Final CUIs: {stats.final_count} ({stats.total_reduction_pct:.1f}% total reduction)")
    print(f"IC Threshold Used: {stats.ic_threshold_used:.3f}")
    print(f"Processing Time: {stats.processing_time:.2f}s")
    print("\n" + "="*80)
    print("REDUCED CUIs WITH IC SCORES")
    print("="*80)
    
    ic_scores = cui_reducer.get_ic_scores(reduced_cuis)
    
    for cui in reduced_cuis[:10]:
        desc = descriptions.get(cui, "No description available")
        ic = ic_scores.get(cui, 0.0)
        print(f"{cui} (IC={ic:.3f}): {desc}")
    
    if len(reduced_cuis) > 10:
        print(f"... and {len(reduced_cuis) - 10} more")


if __name__ == "__main__":
    main()
