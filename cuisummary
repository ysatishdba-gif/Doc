"""
Fully Configurable CUI Reduction System - Production Ready
No hardcoded values - everything configurable through parameters
"""

import time
import numpy as np
import logging
from typing import List, Set, Dict, Tuple, Optional, Union
from dataclasses import dataclass, asdict, field
from collections import defaultdict, deque
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import subprocess
import threading
from functools import lru_cache
import hashlib

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ReductionConfig:
    """Complete configuration - ALL parameters must be provided"""
    
    # Required BigQuery configuration
    project_id: str
    dataset_id: str
    
    # Required table names
    mrrel_table: str
    mrconso_table: str
    cui_descriptions_table: str
    cui_embeddings_table: str
    
    # Required API configuration
    api_url: str
    api_timeout: int
    api_top_k: int
    
    # Required terminology systems
    allowed_sab: List[str]
    
    # Required reduction parameters
    target_reduction: float
    ic_percentile: float
    semantic_threshold: float
    duplicate_threshold: float
    use_semantic_clustering: bool
    max_hierarchy_depth: int
    min_cluster_retention: float
    
    # Required performance parameters
    query_timeout: int
    max_query_results: int
    batch_size: int
    cache_enabled: bool
    
    # Optional parameters with defaults
    mrsty_table: str = None
    api_batch_size: int = None
    adaptive_threshold: bool = False
    parallel_processing: bool = False
    max_workers: int = None
    preserve_critical_cuis: List[str] = field(default_factory=list)
    min_acceptable_reduction: float = None
    max_acceptable_reduction: float = None
    require_validation: bool = False
    max_ancestors_per_cui: int = None
    token_expiry_seconds: int = None
    retry_count: int = None
    retry_backoff_factor: float = None
    http_pool_connections: int = None
    http_pool_maxsize: int = None
    hierarchy_relation_types: List[str] = None
    clustering_linkage_method: str = None
    clustering_metric: str = None
    embedding_vector_size: int = None
    log_level: str = None
    
    def __post_init__(self):
        """Validate required fields are not None"""
        required_fields = [
            'project_id', 'dataset_id', 'mrrel_table', 'mrconso_table',
            'cui_descriptions_table', 'cui_embeddings_table', 'api_url',
            'api_timeout', 'api_top_k', 'allowed_sab', 'target_reduction',
            'ic_percentile', 'semantic_threshold', 'duplicate_threshold',
            'use_semantic_clustering', 'max_hierarchy_depth', 'min_cluster_retention',
            'query_timeout', 'max_query_results', 'batch_size', 'cache_enabled'
        ]
        
        for field_name in required_fields:
            if getattr(self, field_name) is None:
                raise ValueError(f"Required configuration field '{field_name}' is not set")


@dataclass
class ReductionStats:
    """Statistics for the reduction process"""
    initial_count: int
    after_filter: int
    after_ic_rollup: int
    final_count: int
    filter_reduction_pct: float
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0
    validation_passed: bool = True
    warnings: List[str] = field(default_factory=list)
    
    def to_dict(self):
        return asdict(self)


class CUIAPIClient:
    """API client for CUI extraction - fully configurable"""
    
    def __init__(self, config: ReductionConfig):
        self.config = config
        self.api_base_url = config.api_url.rstrip('/')
        self.timeout = config.api_timeout
        self.top_k = config.api_top_k
        self.batch_size = config.api_batch_size or config.batch_size
        self.token_expiry_seconds = config.token_expiry_seconds or 3300
        self.session = self._setup_session()
        self._token = None
        self._token_expiry = 0
        self._token_lock = threading.Lock()
        self._cache = {} if config.cache_enabled else None
    
    def _setup_session(self):
        """Setup HTTP session with configurable parameters"""
        session = requests.Session()
        retry = Retry(
            total=self.config.retry_count or 3,
            backoff_factor=self.config.retry_backoff_factor or 1.0,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        adapter = HTTPAdapter(
            max_retries=retry,
            pool_connections=self.config.http_pool_connections or 10,
            pool_maxsize=self.config.http_pool_maxsize or 20
        )
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session
    
    def _get_token(self):
        """Get GCP authentication token"""
        with self._token_lock:
            if time.time() < self._token_expiry:
                return self._token
            
            try:
                result = subprocess.run(
                    ['gcloud', 'auth', 'print-identity-token'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    universal_newlines=True,
                    timeout=self.timeout
                )
                if result.returncode == 0:
                    self._token = result.stdout.strip()
                    self._token_expiry = time.time() + self.token_expiry_seconds
                    logger.info("GCP token refreshed")
                    return self._token
            except Exception as e:
                logger.error(f"Token refresh failed: {e}")
            return None
    
    def extract_cuis(self, texts: List[str]) -> Set[str]:
        """Extract CUIs from texts"""
        if not texts:
            return set()
        
        all_cuis = set()
        
        # Check cache if enabled
        uncached_texts = []
        if self._cache is not None:
            for text in texts:
                if text in self._cache:
                    all_cuis.update(self._cache[text])
                else:
                    uncached_texts.append(text)
        else:
            uncached_texts = texts
        
        if not uncached_texts:
            return all_cuis
        
        # Process uncached texts
        token = self._get_token()
        if not token:
            logger.error("No valid authentication token")
            return all_cuis
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        # Process in batches
        for i in range(0, len(uncached_texts), self.batch_size):
            batch = uncached_texts[i:i + self.batch_size]
            try:
                response = self.session.post(
                    self.api_base_url,
                    json={"query_texts": batch, "top_k": self.top_k},
                    headers=headers,
                    timeout=self.timeout
                )
                response.raise_for_status()
                
                data = response.json()
                for text in batch:
                    cuis = data.get(text, [])
                    if isinstance(cuis, list):
                        cui_set = set(str(c) for c in cuis if c)
                        all_cuis.update(cui_set)
                        if self._cache is not None:
                            self._cache[text] = cui_set
                
            except Exception as e:
                logger.error(f"Batch CUI extraction failed: {e}")
        
        logger.info(f"Extracted {len(all_cuis)} CUIs from {len(texts)} texts")
        return all_cuis


class CUIReducer:
    """CUI reducer - fully configurable"""
    
    def __init__(self, config: ReductionConfig):
        self.config = config
        self.client = bigquery.Client(project=config.project_id)
        
        # Initialize caches if enabled
        if config.cache_enabled:
            self._hierarchy_cache = {}
            self._ic_scores_cache = {}
            self._descriptions_cache = {}
            self._embeddings_cache = {}
            self._sab_cache = {}
        else:
            self._hierarchy_cache = None
            self._ic_scores_cache = None
            self._descriptions_cache = None
            self._embeddings_cache = None
            self._sab_cache = None
        
        self.cache_hits = 0
        self.cache_misses = 0
    
    def filter_to_allowed_sab(self, cuis: Set[str]) -> List[str]:
        """Filter CUIs to allowed SAB codes"""
        if not cuis:
            return []
        
        # Check cache if enabled
        if self._sab_cache is not None:
            uncached = [c for c in cuis if c not in self._sab_cache]
            if uncached:
                self.cache_misses += len(uncached)
            else:
                self.cache_hits += len(cuis)
        else:
            uncached = list(cuis)
        
        # Query for uncached CUIs
        if uncached:
            try:
                # Process in batches
                for i in range(0, len(uncached), self.config.batch_size):
                    batch = uncached[i:i + self.config.batch_size]
                    
                    query = f"""
                    SELECT DISTINCT CUI
                    FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.mrconso_table}`
                    WHERE CUI IN UNNEST(@cuis)
                      AND SAB IN UNNEST(@sabs)
                    LIMIT {self.config.max_query_results}
                    """
                    
                    job_config = bigquery.QueryJobConfig(
                        query_parameters=[
                            bigquery.ArrayQueryParameter("cuis", "STRING", batch),
                            bigquery.ArrayQueryParameter("sabs", "STRING", self.config.allowed_sab)
                        ]
                    )
                    
                    query_job = self.client.query(query, job_config=job_config)
                    df = query_job.result(timeout=self.config.query_timeout).to_dataframe()
                    
                    # Update cache if enabled
                    if self._sab_cache is not None:
                        for cui in df['CUI'].tolist():
                            self._sab_cache[cui] = True
                        for cui in batch:
                            if cui not in self._sab_cache:
                                self._sab_cache[cui] = False
                
            except Exception as e:
                logger.error(f"SAB filtering failed: {e}")
                return list(cuis)
        
        # Build result
        if self._sab_cache is not None:
            filtered_cuis = [cui for cui in cuis if self._sab_cache.get(cui, False)]
        else:
            filtered_cuis = list(cuis)
        
        logger.info(f"SAB filter: {len(cuis)} → {len(filtered_cuis)} CUIs")
        return filtered_cuis
    
    def build_hierarchy(self, relevant_cuis: List[str]) -> Dict:
        """Build hierarchy with configurable parameters"""
        # Check cache if enabled
        if self._hierarchy_cache is not None:
            cache_key = hashlib.md5(','.join(sorted(relevant_cuis)).encode()).hexdigest()
            if cache_key in self._hierarchy_cache:
                self.cache_hits += 1
                return self._hierarchy_cache[cache_key]
            self.cache_misses += 1
        else:
            cache_key = None
        
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(relevant_cuis)
        
        # Get relation types to use
        relation_types = self.config.hierarchy_relation_types or ['PAR', 'CHD']
        
        try:
            visited = set()
            frontier = set(relevant_cuis)
            
            for depth in range(self.config.max_hierarchy_depth):
                if not frontier:
                    break
                
                logger.info(f"Building hierarchy depth {depth + 1}: {len(frontier)} CUIs")
                
                # Process in batches
                for i in range(0, len(list(frontier)), self.config.batch_size):
                    batch = list(frontier)[i:i + self.config.batch_size]
                    
                    query = f"""
                    SELECT DISTINCT cui1, cui2, rel
                    FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.mrrel_table}`
                    WHERE (cui1 IN UNNEST(@batch) OR cui2 IN UNNEST(@batch))
                      AND rel IN UNNEST(@relations)
                      AND cui1 != cui2
                    LIMIT {self.config.max_query_results}
                    """
                    
                    job_config = bigquery.QueryJobConfig(
                        query_parameters=[
                            bigquery.ArrayQueryParameter("batch", "STRING", batch),
                            bigquery.ArrayQueryParameter("relations", "STRING", relation_types)
                        ]
                    )
                    
                    try:
                        query_job = self.client.query(query, job_config=job_config)
                        df = query_job.result(timeout=self.config.query_timeout).to_dataframe()
                        
                        for _, row in df.iterrows():
                            cui1, cui2, rel = str(row['cui1']), str(row['cui2']), str(row['rel'])
                            
                            if rel == 'PAR':
                                parent_to_children[cui1].append(cui2)
                                child_to_parents[cui2].append(cui1)
                            elif rel == 'CHD':
                                parent_to_children[cui2].append(cui1)
                                child_to_parents[cui1].append(cui2)
                            
                            all_cuis.update([cui1, cui2])
                        
                    except Exception as e:
                        logger.warning(f"Batch hierarchy query failed: {e}")
                
                # Update frontier
                next_frontier = set()
                for cui in frontier:
                    for parent in child_to_parents.get(cui, []):
                        if parent not in visited:
                            next_frontier.add(parent)
                    for child in parent_to_children.get(cui, []):
                        if child not in visited:
                            next_frontier.add(child)
                
                visited.update(frontier)
                frontier = next_frontier - visited
                
        except Exception as e:
            logger.error(f"Hierarchy building failed: {e}")
        
        hierarchy = {
            'child_to_parents': dict(child_to_parents),
            'parent_to_children': dict(parent_to_children),
            'all_cuis': all_cuis
        }
        
        # Cache if enabled
        if self._hierarchy_cache is not None and cache_key:
            self._hierarchy_cache[cache_key] = hierarchy
        
        logger.info(f"Built hierarchy with {len(all_cuis)} CUIs")
        return hierarchy
    
    def compute_ic_scores(self, hierarchy: Dict) -> Dict[str, float]:
        """Compute IC scores"""
        # Check cache if enabled
        if self._ic_scores_cache is not None:
            cache_key = str(len(hierarchy.get('all_cuis', set())))
            if cache_key in self._ic_scores_cache:
                return self._ic_scores_cache[cache_key]
        else:
            cache_key = None
        
        parent_to_children = hierarchy.get('parent_to_children', {})
        all_cuis = hierarchy.get('all_cuis', set())
        total = len(all_cuis)
        
        if total == 0:
            return {}
        
        # Count descendants
        descendant_counts = {}
        
        def count_descendants(cui: str, visited: Set[str] = None) -> int:
            if visited is None:
                visited = set()
            if cui in visited or cui in descendant_counts:
                return descendant_counts.get(cui, 0)
            
            visited.add(cui)
            children = parent_to_children.get(cui, [])
            count = len(children)
            
            for child in children:
                count += count_descendants(child, visited)
            
            descendant_counts[cui] = count
            return count
        
        # Compute counts
        for cui in all_cuis:
            if cui not in descendant_counts:
                try:
                    count_descendants(cui)
                except RecursionError:
                    descendant_counts[cui] = 0
        
        # Compute IC scores
        ic_scores = {}
        for cui in all_cuis:
            desc_count = descendant_counts.get(cui, 0)
            ic = -np.log((desc_count + 1) / total)
            ic_scores[cui] = max(0.0, ic)
        
        # Cache if enabled
        if self._ic_scores_cache is not None and cache_key:
            self._ic_scores_cache[cache_key] = ic_scores
        
        return ic_scores
    
    def semantic_rollup(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        ic_threshold: float
    ) -> List[str]:
        """Perform semantic rollup"""
        child_to_parents = hierarchy.get('child_to_parents', {})
        rolled_up = {}
        
        max_ancestors = self.config.max_ancestors_per_cui or 100
        
        for cui in cui_list:
            # Get ancestors
            ancestors = []
            visited = set()
            queue = deque([cui])
            
            while queue and len(visited) < max_ancestors:
                current = queue.popleft()
                if current in visited:
                    continue
                visited.add(current)
                
                for parent in child_to_parents.get(current, []):
                    if parent not in visited:
                        ancestors.append(parent)
                        queue.append(parent)
            
            # Find lowest informative ancestor
            candidates = [cui] + ancestors
            valid = [c for c in candidates if ic_scores.get(c, 0) >= ic_threshold]
            
            if valid:
                rolled_up[cui] = min(valid, key=lambda c: ic_scores.get(c, float('inf')))
            else:
                rolled_up[cui] = cui
        
        final_list = list(set(rolled_up.values()))
        
        # Add critical CUIs if specified
        if self.config.preserve_critical_cuis:
            for critical_cui in self.config.preserve_critical_cuis:
                if critical_cui in cui_list and critical_cui not in final_list:
                    final_list.append(critical_cui)
        
        return final_list
    
    def semantic_clustering(
        self,
        cui_list: List[str],
        similarity_threshold: float
    ) -> List[str]:
        """Cluster similar CUIs"""
        if len(cui_list) <= 1:
            return cui_list
        
        try:
            # Get embeddings
            embeddings_dict = {}
            
            for i in range(0, len(cui_list), self.config.batch_size):
                batch = cui_list[i:i + self.config.batch_size]
                
                query = f"""
                SELECT REF_CUI as cui, REF_Embedding as embedding
                FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.cui_embeddings_table}`
                WHERE REF_CUI IN UNNEST(@cuis)
                LIMIT {self.config.max_query_results}
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("cuis", "STRING", batch)
                    ]
                )
                
                query_job = self.client.query(query, job_config=job_config)
                df = query_job.result(timeout=self.config.query_timeout).to_dataframe()
                
                for _, row in df.iterrows():
                    embeddings_dict[row['cui']] = np.array(row['embedding'])
            
            if len(embeddings_dict) < 2:
                return cui_list
            
            # Prepare for clustering
            cuis_with_embeddings = list(embeddings_dict.keys())
            embeddings_matrix = np.vstack([embeddings_dict[cui] for cui in cuis_with_embeddings])
            
            # Perform clustering
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - self.config.duplicate_threshold,
                metric=self.config.clustering_metric or 'cosine',
                linkage=self.config.clustering_linkage_method or 'average'
            )
            
            labels = clustering.fit_predict(embeddings_matrix)
            
            # Process clusters
            final_cuis = []
            for cluster_id in np.unique(labels):
                cluster_indices = np.where(labels == cluster_id)[0]
                cluster_cuis = [cuis_with_embeddings[i] for i in cluster_indices]
                
                if len(cluster_cuis) == 1:
                    final_cuis.append(cluster_cuis[0])
                else:
                    # Keep all except true duplicates
                    unique_cuis = self._filter_duplicates(
                        cluster_cuis,
                        embeddings_matrix[cluster_indices]
                    )
                    final_cuis.extend(unique_cuis)
            
            # Add back CUIs without embeddings
            cuis_without_embeddings = set(cui_list) - set(cuis_with_embeddings)
            final_cuis.extend(cuis_without_embeddings)
            
            return final_cuis
            
        except Exception as e:
            logger.error(f"Clustering failed: {e}")
            return cui_list
    
    def _filter_duplicates(
        self,
        cluster_cuis: List[str],
        cluster_embeddings: np.ndarray
    ) -> List[str]:
        """Filter true duplicates from cluster"""
        if len(cluster_cuis) <= 1:
            return cluster_cuis
        
        # Calculate similarities
        n = len(cluster_cuis)
        to_keep = [cluster_cuis[0]]
        
        duplicate_sim_threshold = self.config.duplicate_threshold + \
                                 (1 - self.config.duplicate_threshold) * 0.3  # Even stricter
        
        for i in range(1, n):
            is_duplicate = False
            for j in range(i):
                if cluster_cuis[j] in to_keep:
                    sim = np.dot(cluster_embeddings[i], cluster_embeddings[j]) / (
                        np.linalg.norm(cluster_embeddings[i]) * 
                        np.linalg.norm(cluster_embeddings[j])
                    )
                    if sim > duplicate_sim_threshold:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                to_keep.append(cluster_cuis[i])
        
        # Ensure minimum retention
        min_keep = max(1, int(len(cluster_cuis) * self.config.min_cluster_retention))
        if len(to_keep) < min_keep:
            remaining = [c for c in cluster_cuis if c not in to_keep]
            to_keep.extend(remaining[:min_keep - len(to_keep)])
        
        return to_keep
    
    def validate_reduction(self, original: List[str], reduced: List[str]) -> Tuple[bool, List[str]]:
        """Validate reduction if configured"""
        if not self.config.require_validation:
            return True, []
        
        warnings = []
        original_set = set(original)
        reduced_set = set(reduced)
        
        reduction_rate = 1 - (len(reduced_set) / len(original_set)) if original_set else 0
        
        # Check bounds if configured
        if self.config.min_acceptable_reduction is not None:
            if reduction_rate < self.config.min_acceptable_reduction:
                warnings.append(f"Insufficient reduction: {reduction_rate:.1%}")
        
        if self.config.max_acceptable_reduction is not None:
            if reduction_rate > self.config.max_acceptable_reduction:
                warnings.append(f"Excessive reduction: {reduction_rate:.1%}")
        
        # Check critical CUIs
        if self.config.preserve_critical_cuis:
            critical_in_original = set(self.config.preserve_critical_cuis) & original_set
            critical_missing = critical_in_original - reduced_set
            if critical_missing:
                warnings.append(f"Missing {len(critical_missing)} critical CUIs")
        
        return len(warnings) == 0, warnings
    
    def reduce(self, input_cuis: List[str]) -> Tuple[List[str], ReductionStats]:
        """Main reduction pipeline"""
        start_time = time.time()
        initial_count = len(input_cuis)
        
        if not input_cuis:
            return [], ReductionStats(
                initial_count=0, after_filter=0, after_ic_rollup=0, final_count=0,
                filter_reduction_pct=0, ic_rollup_reduction_pct=0,
                semantic_clustering_reduction_pct=0, total_reduction_pct=0,
                processing_time=0, ic_threshold_used=0
            )
        
        # Remove duplicates
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        
        logger.info(f"Starting reduction: {initial_count} CUIs")
        
        # Step 1: Filter to allowed SAB
        filtered_cuis = self.filter_to_allowed_sab(set(input_cuis))
        after_filter = len(filtered_cuis)
        
        if not filtered_cuis:
            return [], ReductionStats(
                initial_count=initial_count, after_filter=0, after_ic_rollup=0, final_count=0,
                filter_reduction_pct=100, ic_rollup_reduction_pct=0,
                semantic_clustering_reduction_pct=0, total_reduction_pct=100,
                processing_time=time.time() - start_time, ic_threshold_used=0
            )
        
        # Step 2: Build hierarchy
        hierarchy = self.build_hierarchy(filtered_cuis)
        
        # Step 3: Compute IC scores
        ic_scores = self.compute_ic_scores(hierarchy)
        
        # Step 4: Determine threshold
        if ic_scores:
            ic_values = list(ic_scores.values())
            ic_threshold = float(np.percentile(ic_values, self.config.ic_percentile))
        else:
            ic_threshold = 0.0
        
        # Step 5: Semantic rollup
        rolled_up_cuis = self.semantic_rollup(
            filtered_cuis, hierarchy, ic_scores, ic_threshold
        )
        after_rollup = len(rolled_up_cuis)
        
        # Step 6: Semantic clustering
        if self.config.use_semantic_clustering:
            final_cuis = self.semantic_clustering(
                rolled_up_cuis, self.config.semantic_threshold
            )
        else:
            final_cuis = rolled_up_cuis
        
        final_count = len(final_cuis)
        
        # Step 7: Validation
        validation_passed, warnings = self.validate_reduction(input_cuis, final_cuis)
        
        # Calculate statistics
        stats = ReductionStats(
            initial_count=initial_count,
            after_filter=after_filter,
            after_ic_rollup=after_rollup,
            final_count=final_count,
            filter_reduction_pct=(initial_count - after_filter) / initial_count * 100 if initial_count else 0,
            ic_rollup_reduction_pct=(after_filter - after_rollup) / initial_count * 100 if initial_count else 0,
            semantic_clustering_reduction_pct=(after_rollup - final_count) / initial_count * 100 if initial_count else 0,
            total_reduction_pct=(initial_count - final_count) / initial_count * 100 if initial_count else 0,
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_threshold,
            hierarchy_size=len(hierarchy.get('all_cuis', set())),
            cache_hits=self.cache_hits,
            cache_misses=self.cache_misses,
            validation_passed=validation_passed,
            warnings=warnings
        )
        
        logger.info(f"Reduction complete: {initial_count} → {final_count}")
        return final_cuis, stats


def run_cui_reduction(
    texts: List[str],
    config: ReductionConfig
) -> Tuple[List[str], List[str], Dict[str, str], ReductionStats]:
    """
    Main function to run CUI reduction
    ALL parameters come from config - no hardcoded values
    """
    try:
        # Set log level if configured
        if config.log_level:
            logging.getLogger().setLevel(config.log_level)
        
        # Initialize API client
        api_client = CUIAPIClient(config)
        
        # Extract CUIs
        start_time = time.time()
        initial_cuis = api_client.extract_cuis(texts)
        api_time = time.time() - start_time
        
        if not initial_cuis:
            logger.warning("No CUIs extracted")
            return [], [], {}, None
        
        # Initialize reducer
        reducer = CUIReducer(config)
        
        # Reduce CUIs
        reduced_cuis, stats = reducer.reduce(list(initial_cuis))
        stats.api_call_time = api_time
        
        # Get descriptions if table is configured
        descriptions = {}
        if config.cui_descriptions_table:
            try:
                for i in range(0, len(reduced_cuis), config.batch_size):
                    batch = reduced_cuis[i:i + config.batch_size]
                    
                    query = f"""
                    SELECT CUI as cui, Definition as description
                    FROM `{config.project_id}.{config.dataset_id}.{config.cui_descriptions_table}`
                    WHERE CUI IN UNNEST(@cuis)
                    """
                    
                    job_config = bigquery.QueryJobConfig(
                        query_parameters=[
                            bigquery.ArrayQueryParameter("cuis", "STRING", batch)
                        ]
                    )
                    
                    client = bigquery.Client(project=config.project_id)
                    query_job = client.query(query, job_config=job_config)
                    df = query_job.result(timeout=config.query_timeout).to_dataframe()
                    
                    for _, row in df.iterrows():
                        descriptions[row['cui']] = row['description']
                        
            except Exception as e:
                logger.error(f"Failed to get descriptions: {e}")
        
        return list(initial_cuis), reduced_cuis, descriptions, stats
        
    except Exception as e:
        logger.error(f"CUI reduction failed: {e}")
        return [], [], {}, None
