# ============================================================
# STANDARD LIBRARIES
# ============================================================
import logging
import time
import threading
import subprocess
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Set, Any
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

# ============================================================
# THIRD PARTY
# ============================================================
import numpy as np
import requests
import pandas as pd
from scipy import stats
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster

from google.cloud import bigquery
from sklearn.metrics.pairwise import cosine_similarity

# -------------------------
# Logging
# -------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# API CLIENTS
# ============================================================
class CUIAPIClient:
    """Client for CUI extraction API with batching support"""
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 5):
        self.api_base_url = api_base_url.rstrip("/")
        self.timeout = timeout
        self.top_k = top_k
        self.session = requests.Session()

    def _refresh_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token

            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                universal_newlines=True,
            )
            token = result.stdout.strip()
            self._cached_token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            self._token_expiry = now + 3300
            return self._cached_token

    def extract_cuis_batch(self, texts: List[str]) -> Dict[str, Set[str]]:
        """Extract CUIs for each text, maintaining text-CUI mapping"""
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = self._refresh_token()

        response = self.session.post(
            self.api_base_url,
            json=payload,
            headers=headers,
            timeout=self.timeout
        )
        response.raise_for_status()
        data = response.json()

        text_to_cuis = {}
        all_cuis = set()
        for text in texts:
            text_cuis = set(map(str, data.get(text, [])))
            text_to_cuis[text] = text_cuis
            all_cuis.update(text_cuis)
            
        logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
        return text_to_cuis

class SubnetAPIClient:
    """Enhanced Subnet API client with parallel processing"""
    def __init__(self, subnet_url: str):
        self.url = subnet_url.rstrip("/")
        self._session_cache = {}

    def _get_token(self):
        result = subprocess.run(
            ["gcloud", "auth", "print-identity-token"],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            universal_newlines=True,
        )
        token = result.stdout.strip()
        return {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    def get_subnet_single(self, cui: str) -> Dict:
        """Get subnet for a single CUI with caching"""
        if cui in self._session_cache:
            return self._session_cache[cui]
            
        headers = self._get_token()
        payload = {"cuis": [cui], "cross_context": False}
        
        try:
            response = requests.post(
                f"{self.url}/subnet/", 
                json=payload, 
                headers=headers, 
                timeout=60
            )
            response.raise_for_status()
            nodes, edges = response.json().get("output", ([], []))
            
            ancestors = set()
            descendants = set()
            
            for parent, child in edges:
                if parent == cui:
                    descendants.add(child)
                elif child == cui:
                    ancestors.add(parent)
                else:
                    if parent in ancestors or child in descendants:
                        ancestors.add(parent)
                        descendants.add(child)
            
            result = {
                "cui": cui,
                "ancestors": ancestors,
                "descendants": descendants
            }
            self._session_cache[cui] = result
            return result
            
        except Exception as e:
            logger.error(f"Failed to get subnet for {cui}: {e}")
            return {"cui": cui, "ancestors": set(), "descendants": set()}

    def get_subnets_parallel(self, cuis: List[str]) -> Dict[str, Dict]:
        """Get subnets for multiple CUIs in parallel"""
        results = {}
        
        # Adaptive worker count based on number of CUIs
        max_workers = min(20, max(5, len(cuis) // 10))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_cui = {
                executor.submit(self.get_subnet_single, cui): cui 
                for cui in cuis
            }
            
            for future in as_completed(future_to_cui):
                cui = future_to_cui[future]
                try:
                    result = future.result(timeout=60)
                    results[cui] = result
                except Exception as e:
                    logger.error(f"Failed to process {cui}: {e}")
                    results[cui] = {"cui": cui, "ancestors": set(), "descendants": set()}
                    
        logger.info(f"Retrieved subnet data for {len(results)} CUIs")
        return results

# ============================================================
# BIGQUERY OPERATIONS
# ============================================================
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    """Filter CUIs by allowed vocabularies"""
    if not cuis:
        return []
    try:
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('ICD10', 'ICD10CM', 'ICD9CM', 'SNOMEDCT_US', 'LOINC', 'RXNORM')
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
        )
        df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        allowed_cuis = df['CUI'].tolist()
        logger.info(f"Filtered: {len(cuis)} -> {len(allowed_cuis)} CUIs")
        return allowed_cuis
    except Exception as e:
        logger.error(f"Failed to filter CUIs: {str(e)}")
        return list(cuis)

def group_cuis_by_semantic_type(client: bigquery.Client, project_id: str,
                                dataset_id: str, cuis: List[str]) -> Dict[str, List[str]]:
    """Group CUIs by semantic type"""
    groups = defaultdict(list)
    batch_size = 5000
    
    for i in range(0, len(cuis), batch_size):
        batch = cuis[i:i + batch_size]
        query = f"""
        SELECT DISTINCT CUI, TUI
        FROM `{project_id}.{dataset_id}.MRSTY`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", batch)]
        )
        df = client.query(query, job_config=job_config).result().to_dataframe()
        
        for _, row in df.iterrows():
            groups[row["TUI"]].append(row["CUI"])
    
    logger.info(f"Grouped into {len(groups)} semantic types")
    return groups

def load_embeddings_batch(client: bigquery.Client, project_id: str, 
                         embedding_table: str, cuis: List[str]) -> Dict[str, np.ndarray]:
    """Load embeddings from BigQuery"""
    embeddings = {}
    batch_size = 1000
    
    for i in range(0, len(cuis), batch_size):
        batch = cuis[i:i + batch_size]
        query = f"""
        SELECT cui, embedding
        FROM `{project_id}.{embedding_table}`
        WHERE cui IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", batch)]
        )
        
        df = client.query(query, job_config=job_config).result().to_dataframe()
        
        for _, row in df.iterrows():
            if isinstance(row['embedding'], str):
                embedding = np.fromstring(row['embedding'].strip('[]'), sep=',')
            else:
                embedding = np.array(row['embedding'])
            embeddings[row['cui']] = embedding
    
    logger.info(f"Loaded {len(embeddings)} embeddings")
    return embeddings

# ============================================================
# ADAPTIVE ALGORITHMS
# ============================================================
def compute_adaptive_ic_scores(subnet_data: Dict[str, Dict], 
                               cui_set: Set[str]) -> Dict[str, float]:
    """Compute IC scores with adaptive normalization"""
    ic_scores = {}
    descendant_counts = {}
    
    # Calculate descendant counts
    for cui in cui_set:
        cui_data = subnet_data.get(cui, {})
        descendants = cui_data.get("descendants", set())
        relevant_descendants = descendants.intersection(cui_set)
        descendant_counts[cui] = len(relevant_descendants)
    
    # Use adaptive binning to find natural groupings
    if descendant_counts:
        counts = list(descendant_counts.values())
        
        # Use Freedman-Diaconis rule for bin width
        q75, q25 = np.percentile(counts, [75, 25])
        iqr = q75 - q25
        if iqr > 0:
            bin_width = 2 * iqr / (len(counts) ** (1/3))
            n_bins = max(2, int((max(counts) - min(counts)) / bin_width))
        else:
            n_bins = min(10, len(set(counts)))
        
        # Calculate IC with adaptive smoothing
        for cui in cui_set:
            count = descendant_counts[cui]
            # Adaptive smoothing based on data distribution
            smoothing = 1 + (statistics.stdev(counts) / (statistics.mean(counts) + 1) if len(counts) > 1 else 1)
            probability = (count + smoothing) / (len(cui_set) + smoothing * n_bins)
            ic_scores[cui] = -np.log(probability)
    
    return ic_scores

def find_natural_clusters_in_hierarchy(subnet_data: Dict[str, Dict], 
                                      cuis: List[str],
                                      ic_scores: Dict[str, float]) -> Set[str]:
    """Find natural clustering points in the hierarchy using information theory"""
    if not cuis:
        return set()
    
    selected = set()
    covered = set()
    
    # Build coverage matrix
    coverage_relationships = []
    
    for cui in cuis:
        cui_data = subnet_data.get(cui, {})
        ancestors = cui_data.get("ancestors", set())
        descendants = cui_data.get("descendants", set())
        
        # Calculate information loss/gain for each potential representative
        for ancestor in list(ancestors) + [cui]:
            if ancestor not in ic_scores:
                continue
                
            # How many CUIs does this ancestor cover?
            anc_data = subnet_data.get(ancestor, {})
            anc_descendants = anc_data.get("descendants", set())
            coverage_set = anc_descendants.intersection(set(cuis))
            
            if not coverage_set:
                continue
            
            # Information preserved when using ancestor instead of all descendants
            total_info = sum(ic_scores.get(c, 0) for c in coverage_set)
            ancestor_info = ic_scores.get(ancestor, 0) * len(coverage_set)
            
            # Information efficiency ratio
            if total_info > 0:
                efficiency = ancestor_info / total_info
            else:
                efficiency = 0
            
            coverage_relationships.append({
                'ancestor': ancestor,
                'covered': coverage_set,
                'efficiency': efficiency,
                'coverage_size': len(coverage_set)
            })
    
    # Sort by efficiency and coverage size
    coverage_relationships.sort(key=lambda x: (x['efficiency'], x['coverage_size']), reverse=True)
    
    # Greedily select ancestors that maximize coverage with minimal information loss
    for rel in coverage_relationships:
        # Check if this adds new coverage
        new_coverage = rel['covered'] - covered
        if new_coverage:
            selected.add(rel['ancestor'])
            covered.update(rel['covered'])
    
    # Add any uncovered CUIs
    uncovered = set(cuis) - covered - selected
    selected.update(uncovered)
    
    logger.info(f"Natural hierarchy clustering: {len(cuis)} -> {len(selected)}")
    return selected

def adaptive_embedding_clustering(cuis: List[str],
                                 embeddings: Dict[str, np.ndarray],
                                 subnet_data: Dict[str, Dict],
                                 ic_scores: Dict[str, float]) -> List[str]:
    """Clustering with natural boundary detection"""
    valid_cuis = [c for c in cuis if c in embeddings]
    
    if not valid_cuis:
        return cuis
    
    if len(valid_cuis) <= 2:
        return valid_cuis
    
    # Create embedding matrix
    vectors = np.vstack([embeddings[c] for c in valid_cuis])
    
    # Calculate pairwise distances
    distances = pdist(vectors, metric='cosine')
    
    if len(distances) == 0:
        return valid_cuis
    
    # Find natural gaps in distance distribution
    sorted_distances = np.sort(distances)
    
    # Calculate gaps between consecutive distances
    gaps = np.diff(sorted_distances)
    
    # Find significant gaps (using z-score)
    if len(gaps) > 0:
        gap_z_scores = stats.zscore(gaps)
        significant_gaps = np.where(gap_z_scores > 1)[0]
        
        if len(significant_gaps) > 0:
            # Use first significant gap as threshold
            threshold_idx = significant_gaps[0]
            threshold = sorted_distances[threshold_idx]
        else:
            # No significant gaps, use median
            threshold = np.median(distances)
    else:
        threshold = 0.2
    
    # Perform hierarchical clustering
    linkage_matrix = linkage(distances, method='average')
    clusters = fcluster(linkage_matrix, threshold, criterion='distance')
    
    # Select representatives based on natural importance
    selected = []
    
    for cluster_id in np.unique(clusters):
        cluster_indices = np.where(clusters == cluster_id)[0]
        cluster_cuis = [valid_cuis[i] for i in cluster_indices]
        
        if len(cluster_cuis) == 1:
            selected.extend(cluster_cuis)
            continue
        
        # Calculate natural importance for each CUI
        importance_scores = []
        for cui in cluster_cuis:
            # Combine IC and descendant count
            ic = ic_scores.get(cui, 0)
            desc_count = len(subnet_data.get(cui, {}).get("descendants", set()))
            
            # Natural log scale for descendant contribution
            importance = ic + np.log(desc_count + 1)
            importance_scores.append(importance)
        
        # Find natural breakpoints in importance scores
        sorted_importance = sorted(enumerate(importance_scores), key=lambda x: x[1], reverse=True)
        
        if len(sorted_importance) > 1:
            # Calculate gaps in importance
            importance_gaps = [sorted_importance[i][1] - sorted_importance[i+1][1] 
                              for i in range(len(sorted_importance)-1)]
            
            if importance_gaps:
                # Find largest gap
                max_gap_idx = np.argmax(importance_gaps)
                # Keep all CUIs above the gap
                n_representatives = max_gap_idx + 1
            else:
                n_representatives = 1
        else:
            n_representatives = 1
        
        # Select top CUIs by natural importance
        top_indices = [idx for idx, _ in sorted_importance[:n_representatives]]
        selected.extend([cluster_cuis[i] for i in top_indices])
    
    return selected

def calculate_information_coverage(original_cuis: Set[str], 
                                  reduced_cuis: Set[str],
                                  subnet_data: Dict[str, Dict],
                                  ic_scores: Dict[str, float]) -> Dict[str, float]:
    """Calculate information coverage metrics without targets"""
    covered_by_hierarchy = set()
    
    # Find what's covered by hierarchy
    for reduced_cui in reduced_cuis:
        covered_by_hierarchy.add(reduced_cui)
        cui_data = subnet_data.get(reduced_cui, {})
        descendants = cui_data.get("descendants", set())
        covered_by_hierarchy.update(descendants.intersection(original_cuis))
    
    # Calculate information metrics
    original_info = sum(ic_scores.get(cui, 0) for cui in original_cuis)
    covered_info = sum(ic_scores.get(cui, 0) for cui in covered_by_hierarchy)
    reduced_info = sum(ic_scores.get(cui, 0) for cui in reduced_cuis)
    
    metrics = {
        'cui_coverage': len(covered_by_hierarchy) / len(original_cuis) if original_cuis else 0,
        'information_coverage': covered_info / original_info if original_info > 0 else 0,
        'information_density': reduced_info / len(reduced_cuis) if reduced_cuis else 0,
        'compression_ratio': len(reduced_cuis) / len(original_cuis) if original_cuis else 0
    }
    
    return metrics

# ============================================================
# MAIN PIPELINE
# ============================================================
def reduce_cuis_pipeline(texts: List[str], 
                        project_id: str,
                        dataset_id: str,
                        embedding_table: str,
                        cui_api_url: str,
                        subnet_api_url: str) -> Dict[str, Any]:
    """
    Fully adaptive CUI reduction pipeline - no targets or thresholds
    
    Returns:
        Dictionary with reduction results and statistics
    """
    start_time = time.time()
    
    # Step 1: Extract CUIs
    logger.info("Step 1: Extracting CUIs from texts")
    extractor = CUIAPIClient(cui_api_url)
    text_to_cuis = extractor.extract_cuis_batch(texts)
    
    all_cuis = set()
    for text_cuis in text_to_cuis.values():
        all_cuis.update(text_cuis)
    
    original_cui_count = len(all_cuis)
    logger.info(f"Extracted {original_cui_count} unique CUIs")
    
    if not all_cuis:
        return {
            "original_cuis": set(),
            "reduced_cuis": set(),
            "metrics": {},
            "statistics": {}
        }
    
    # Step 2: Filter by vocabularies
    logger.info("Step 2: Filtering by allowed vocabularies")
    filtered_cuis = filter_allowed_cuis(all_cuis, project_id, dataset_id)
    
    if not filtered_cuis:
        logger.warning("No CUIs after filtering")
        return {
            "original_cuis": all_cuis,
            "reduced_cuis": set(),
            "metrics": {},
            "statistics": {}
        }
    
    # Step 3: Group by semantic type
    logger.info("Step 3: Grouping by semantic type")
    client = bigquery.Client(project=project_id)
    grouped_cuis = group_cuis_by_semantic_type(client, project_id, dataset_id, filtered_cuis)
    
    # Step 4: Get subnet data for all CUIs
    logger.info("Step 4: Retrieving subnet data")
    subnet_client = SubnetAPIClient(subnet_api_url)
    subnet_data = subnet_client.get_subnets_parallel(filtered_cuis)
    
    # Step 5: Compute adaptive IC scores
    logger.info("Step 5: Computing adaptive IC scores")
    ic_scores = compute_adaptive_ic_scores(subnet_data, set(filtered_cuis))
    
    # Step 6: Find natural clusters in hierarchy
    logger.info("Step 6: Finding natural hierarchy clusters")
    hierarchically_reduced = set()
    
    for tui, cuis in grouped_cuis.items():
        group_reduced = find_natural_clusters_in_hierarchy(subnet_data, cuis, ic_scores)
        hierarchically_reduced.update(group_reduced)
    
    logger.info(f"Natural hierarchy clustering: {len(filtered_cuis)} -> {len(hierarchically_reduced)}")
    
    # Step 7: Load embeddings
    logger.info("Step 7: Loading embeddings")
    embeddings = load_embeddings_batch(
        client, project_id, embedding_table, list(hierarchically_reduced)
    )
    
    # Step 8: Natural embedding-based clustering
    logger.info("Step 8: Natural clustering in embedding space")
    
    final_cuis = []
    for tui, original_cuis in grouped_cuis.items():
        group_reduced = [cui for cui in original_cuis if cui in hierarchically_reduced]
        
        if group_reduced:
            group_selected = adaptive_embedding_clustering(
                group_reduced, embeddings, subnet_data, ic_scores
            )
            final_cuis.extend(group_selected)
    
    final_cui_set = set(final_cuis)
    
    # Calculate metrics (no targets, just report what emerged)
    metrics = calculate_information_coverage(all_cuis, final_cui_set, subnet_data, ic_scores)
    
    elapsed_time = time.time() - start_time
    
    # Results - what naturally emerged from the process
    results = {
        "original_cuis": all_cuis,
        "reduced_cuis": final_cui_set,
        "metrics": metrics,
        "statistics": {
            "original_count": original_cui_count,
            "filtered_count": len(filtered_cuis),
            "final_count": len(final_cui_set),
            "semantic_groups": len(grouped_cuis),
            "processing_time": elapsed_time,
            "api_calls_made": len(subnet_data),
            "reduction_ratio": 1 - (len(final_cui_set) / original_cui_count) if original_cui_count > 0 else 0
        }
    }
    
    logger.info(f"""
    ========== NATURAL REDUCTION COMPLETE ==========
    Original CUIs: {original_cui_count}
    Final CUIs: {len(final_cui_set)}
    Natural Reduction: {results['statistics']['reduction_ratio']:.1%}
    CUI Coverage: {metrics['cui_coverage']:.1%}
    Information Coverage: {metrics['information_coverage']:.1%}
    Information Density: {metrics['information_density']:.2f}
    Processing time: {elapsed_time:.2f} seconds
    ================================================
    """)
    
    return results

# ============================================================
# MAIN EXECUTION
# ============================================================
def main():
    """Example usage"""
    PROJECT_ID = "your_project_id"
    DATASET_ID = "your_dataset"
    EMBEDDING_TABLE = "your_embedding_table"
    CUI_API_URL = "your_cui_api_url"
    SUBNET_API_URL = "your_subnet_api_url"
    
    texts = [
        "Patient presents with chest pain and shortness of breath",
        "Diagnosed with type 2 diabetes mellitus", 
        "Prescribed metformin 500mg twice daily"
    ]
    
    # Run fully adaptive pipeline - no targets or thresholds
    results = reduce_cuis_pipeline(
        texts=texts,
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        embedding_table=EMBEDDING_TABLE,
        cui_api_url=CUI_API_URL,
        subnet_api_url=SUBNET_API_URL
    )
    
    # Display what naturally emerged
    print(f"Natural outcome: {len(results['original_cuis'])} -> {len(results['reduced_cuis'])} CUIs")
    print(f"Information preserved: {results['metrics']['information_coverage']:.2%}")
    print(f"Compression achieved: {results['statistics']['reduction_ratio']:.2%}")

if __name__ == "__main__":
    main()
