# ============================================================
# STANDARD LIBRARIES
# ============================================================
import logging
import time
import threading
import subprocess
from typing import List, Dict, Tuple, Set, Any
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# ============================================================
# THIRD PARTY
# ============================================================
import numpy as np
import requests
from scipy.spatial.distance import cdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score
from google.cloud import bigquery

# -------------------------
# Logging
# -------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# API CLIENTS
# ============================================================
class CUIAPIClient:
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 5):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.session = requests.Session()

    def _refresh_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token
            result = subprocess.run(
                ['gcloud', 'auth', 'print-identity-token'],
                stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, universal_newlines=True
            )
            token = result.stdout.strip()
            self._cached_token = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
            self._token_expiry = now + 3300
            return self._cached_token

    def extract_cuis_batch(self, texts: List[str]) -> Dict[str, Set[str]]:
        headers = self._refresh_token()
        payload = {'query_texts': texts, 'top_k': self.top_k}
        response = self.session.post(self.api_base_url, json=payload, headers=headers, timeout=self.timeout)
        response.raise_for_status()
        data = response.json()
        return {text: set(map(str, data.get(text, []))) for text in texts}


class SubnetAPIClient:
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, subnet_url: str, batch_size: int = 50, max_workers: int = 4):
        self.url = subnet_url.rstrip('/')
        self.batch_size = batch_size
        self.max_workers = max_workers
        self._cache_nodes = {}
        self._cache_edges = {}

    def _get_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token
            result = subprocess.run(
                ['gcloud', 'auth', 'print-identity-token'],
                stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, universal_newlines=True
            )
            token = result.stdout.strip()
            self._cached_token = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
            self._token_expiry = now + 3300
            return self._cached_token

    def _fetch_batch(self, batch: List[str]) -> Tuple[Set[str], List[Tuple[str, str]]]:
        headers = self._get_token()
        payload = {'cuis': batch, 'cross_context': False}
        response = requests.post(f'{self.url}/subnet/', json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        nodes, edges = response.json().get('output', ([], []))
        return set(nodes), edges

    def get_subnet_batch(self, cuis: List[str]) -> Tuple[Set[str], List[Tuple[str, str]]]:
        all_nodes, all_edges = set(), []
        batches = [cuis[i:i+self.batch_size] for i in range(0, len(cuis), self.batch_size)]

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self._fetch_batch, batch): batch for batch in batches}
            for future in as_completed(futures):
                try:
                    nodes, edges = future.result()
                    all_nodes.update(nodes)
                    all_edges.extend(edges)
                except Exception as e:
                    logger.error(f'Error fetching subnet batch: {e}')
        all_edges = list(set(all_edges))
        return all_nodes, all_edges

# ============================================================
# BIGQUERY OPERATIONS (unchanged)
# ============================================================
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    client = bigquery.Client(project=project_id)
    query = f"""
    SELECT DISTINCT CUI
    FROM `{project_id}.{dataset_id}.MRCONSO`
    WHERE CUI IN UNNEST(@cuis)
      AND SAB IN ('ICD10', 'ICD10CM', 'ICD9CM', 'SNOMEDCT_US', 'LOINC', 'RXNORM')
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=[bigquery.ArrayQueryParameter('cuis', 'STRING', list(cuis))]
    )
    df = client.query(query, job_config=job_config).result().to_dataframe()
    return df['CUI'].tolist()

# ============================================================
# GRAPH + IC + REDUCTION (unchanged)
# ============================================================
# ... (all previous graph, IC, hierarchical reduction, embedding clustering code stays unchanged)

# ============================================================
# MAIN PIPELINE WITH LOGGING
# ============================================================
def reduce_cuis_pipeline(
    texts: List[str],
    project_id: str,
    dataset_id: str,
    embedding_table: str,
    cui_api_url: str,
    subnet_api_url: str
) -> Dict[str, Any]:
    start_time = time.time()

    logger.info('Step 1: Extracting CUIs')
    extractor = CUIAPIClient(cui_api_url)
    text_to_cuis = extractor.extract_cuis_batch(texts)
    all_cuis = set().union(*text_to_cuis.values())
    logger.info(f'Total CUIs extracted: {len(all_cuis)}')

    logger.info('Step 2: Filtering allowed CUIs')
    filtered = filter_allowed_cuis(all_cuis, project_id, dataset_id)
    logger.info(f'Total CUIs after filtering: {len(filtered)}')

    client = bigquery.Client(project=project_id)
    grouped = group_cuis_by_semantic_type(client, project_id, dataset_id, filtered)
    logger.info(f'CUIs grouped into semantic types: {len(grouped)}')

    logger.info('Step 3: Fetching subnet relationships in batches')
    subnet = SubnetAPIClient(subnet_api_url, batch_size=50, max_workers=4)
    nodes, edges = subnet.get_subnet_batch(filtered)
    logger.info(f'Fetched nodes: {len(nodes)}, edges: {len(edges)}')

    # Build graph and IC
    graph = build_hierarchy_graph(nodes, edges)
    descendants = compute_descendants(graph)
    ic_scores = compute_seco_ic(descendants)

    logger.info('Step 4: Hierarchical reduction')
    hierarchy_selected = set()
    for cuis in grouped.values():
        hierarchy_selected |= hierarchical_reduction_by_coverage(cuis, descendants, ic_scores)
    logger.info(f'CUIs selected by hierarchy: {len(hierarchy_selected)}')

    logger.info('Step 5: Loading embeddings')
    embeddings = load_embeddings_batch(client, project_id, embedding_table, filtered)

    logger.info('Step 6: Embedding clustering')
    embedding_selected = set()
    for cuis in grouped.values():
        embedding_selected |= embedding_based_clustering(cuis, embeddings, ic_scores)
    logger.info(f'CUIs selected by embeddings: {len(embedding_selected)}')

    logger.info('Step 7: Integrating signals')
    final = integrate_signals(hierarchy_selected, embedding_selected, ic_scores)
    logger.info(f'Total CUIs after integration: {len(final)}')

    elapsed = time.time() - start_time
    logger.info(f'Total processing time: {elapsed:.2f}s')

    return {
        'original_cuis': all_cuis,
        'reduced_cuis': final,
        'statistics': {
            'original': len(all_cuis),
            'filtered': len(filtered),
            'hierarchy': len(hierarchy_selected),
            'embedding': len(embedding_selected),
            'final': len(final),
            'time_sec': elapsed
        }
    }

# ============================================================
# MAIN FUNCTION
# ============================================================
def main():
    PROJECT_ID = 'your_project_id'
    DATASET_ID = 'your_dataset'
    EMBEDDING_TABLE = 'your_embedding_table'
    CUI_API_URL = 'your_cui_api_url'
    SUBNET_API_URL = 'your_subnet_api_url'

    texts = [
        'Patient presents with chest pain',
        'Diagnosed with type 2 diabetes mellitus',
        'Brain tumor detected on MRI'
    ]

    results = reduce_cuis_pipeline(
        texts=texts,
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        embedding_table=EMBEDDING_TABLE,
        cui_api_url=CUI_API_URL,
        subnet_api_url=SUBNET_API_URL
    )

    logger.info('Final reduced CUIs:')
    for cui in results['reduced_cuis']:
        logger.info(cui)

if __name__ == '__main__':
    main()
