# ============================================================
# FULL END-TO-END CUI REDUCTION PIPELINE (SINGLE CELL)
# ============================================================

import asyncio
import aiohttp
import subprocess
import re
import logging
import time
import threading
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import requests
from requests.adapters import HTTPAdapter, Retry
import nest_asyncio

nest_asyncio.apply()

# ------------------------------------------------------------
# Logging
# ------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------
# Utilities
# ------------------------------------------------------------
def extract_base_cui(cui: str) -> Optional[str]:
    if not cui:
        return None
    m = re.match(r"^(C\d{7})", cui.upper())
    return m.group(1) if m else None

def safe_percentage(n, d):
    return (n / d * 100) if d else 0.0

# ------------------------------------------------------------
# GCP Token Provider
# ------------------------------------------------------------
class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force=False):
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token

            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)

            token = proc.stdout.strip()
            cls._token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            cls._expiry = now + 3300
            return cls._token

# ------------------------------------------------------------
# Stats
# ------------------------------------------------------------
@dataclass
class ReductionStats:
    initial_count: int
    after_ic: int
    final_count: int
    total_reduction_pct: float
    processing_time: float

    def to_dict(self):
        return asdict(self)

# ------------------------------------------------------------
# CUI Extraction API Client
# ------------------------------------------------------------
class CUIAPIClient:
    def __init__(self, api_url: str, top_k: int = 3):
        self.api_url = api_url
        self.top_k = top_k
        self.session = requests.Session()
        retry = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503]
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract_cuis(self, texts: List[str]) -> Set[str]:
        logger.info("Extracting CUIs from input texts")
        r = self.session.post(
            self.api_url,
            headers=GCPTokenProvider.get_headers(),
            json={"query_texts": texts, "top_k": self.top_k},
            timeout=60
        )
        r.raise_for_status()
        out = set()
        for v in r.json().values():
            out.update(map(str, v))
        logger.info(f"Extracted {len(out)} CUIs")
        return out

# ------------------------------------------------------------
# ICD / SNOMED / LOINC Filter
# ------------------------------------------------------------
def filter_allowed_cuis(
    cuis: Set[str],
    project: str,
    dataset: str
) -> List[str]:

    if not cuis:
        return []

    logger.info(f"Filtering {len(cuis)} CUIs to ICD/SNOMED/LOINC")
    client = bigquery.Client(project=project)

    variants = set()
    for c in cuis:
        c = c.upper()
        variants.add(c)
        base = extract_base_cui(c)
        if base:
            variants.add(base)

    query = f"""
    SELECT DISTINCT CUI
    FROM `{project}.{dataset}.MRCONSO`
    WHERE CUI IN UNNEST(@cuis)
      AND SAB IN (
        'ICD9CM','ICD10','ICD10CM',
        'SNOMEDCT_US','LOINC'
      )
    """

    job = client.query(
        query,
        job_config=bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", list(variants))
            ]
        )
    )

    allowed = set(job.to_dataframe()["CUI"])
    filtered = [
        c for c in cuis
        if c.upper() in allowed or extract_base_cui(c) in allowed
    ]

    logger.info(f"{len(filtered)} CUIs retained after vocabulary filter")
    return filtered

# ------------------------------------------------------------
# Enhanced Reducer
# ------------------------------------------------------------
class EnhancedCUIReducer:

    def __init__(
        self,
        project: str,
        dataset: str,
        subnet_api_url: str,
        mrsty_table: str,
        embedding_table: str
    ):
        self.client = bigquery.Client(project=project)
        self.project = project
        self.dataset = dataset
        self.subnet_api_url = subnet_api_url
        self.mrsty_table = mrsty_table
        self.embedding_table = embedding_table
        self.embedding_cache = {}

    # --------------------------------------------------------
    # Hierarchy Expansion (FULL SET, ONCE)
    # --------------------------------------------------------
    async def _fetch_hierarchy(self, cuis: List[str], batch_size: int = 100, max_concurrent: int = 10) -> Dict:
        """
        Batched + parallel hierarchy expansion using the subnetwork API.
        """
        headers = GCPTokenProvider.get_headers()
        parent_to_children = defaultdict(list)
        child_to_parents = defaultdict(list)
        all_cuis = set(cuis)

        # Normalize to base CUIs
        normalized = [
            extract_base_cui(c) for c in cuis if extract_base_cui(c)
        ]

        timeout = aiohttp.ClientTimeout(total=30)
        semaphore = asyncio.Semaphore(max_concurrent)

        async with aiohttp.ClientSession(timeout=timeout) as session:

            async def fetch_batch(batch: List[str]):
                async with semaphore:
                    try:
                        async with session.post(
                            f"{self.subnet_api_url}/subnet/",
                            headers=headers,
                            json={"cuis": batch, "cross_context": False}
                        ) as resp:
                            if resp.status != 200:
                                logger.warning(f"Subnetwork API returned {resp.status}")
                                return [], []
                            data = await resp.json()
                            return data.get("output", [[], []])
                    except Exception as e:
                        logger.error(f"Subnetwork batch failed: {e}")
                        return [], []

            # Create batches
            tasks = []
            for i in range(0, len(normalized), batch_size):
                tasks.append(fetch_batch(normalized[i:i+batch_size]))

            logger.info(f"Fetching hierarchy in {len(tasks)} batches "
                        f"(batch_size={batch_size}, concurrency={max_concurrent})")

            results = await asyncio.gather(*tasks, return_exceptions=False)

            # Aggregate results
            for nodes, edges in results:
                for p, c in edges:
                    p = extract_base_cui(str(p))
                    c = extract_base_cui(str(c))
                    if p and c:
                        parent_to_children[p].append(c)
                        child_to_parents[c].append(p)
                        all_cuis.update([p, c])

        logger.info(f"Hierarchy expansion complete: {len(cuis)} â†’ {len(all_cuis)} CUIs")
        return {
            "all_cuis": all_cuis,
            "parent_to_children": dict(parent_to_children),
            "child_to_parents": dict(child_to_parents)
        }

    # --------------------------------------------------------
    # Semantic Types (Multi-Group Membership)
    # --------------------------------------------------------
    def _semantic_groups(self, cuis: List[str]) -> Dict[str, Set[str]]:
        query = f"""
        SELECT CUI, STY
        FROM `{self.project}.{self.dataset}.{self.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        df = self.client.query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", cuis)
                ]
            )
        ).to_dataframe()

        groups = defaultdict(set)
        for _, r in df.iterrows():
            groups[r.STY].add(r.CUI)

        logger.info(f"Created {len(groups)} semantic groups")
        return groups

    # --------------------------------------------------------
    # IC Computation (Semantic Subgraph)
    # --------------------------------------------------------
    def _compute_ic(self, group: Set[str], hierarchy: Dict) -> Dict[str, float]:
        p2c = hierarchy["parent_to_children"]

        def dfs(cui, seen):
            if cui in seen:
                return 0
            seen.add(cui)
            children = [c for c in p2c.get(cui, []) if c in group]
            return len(children) + sum(dfs(ch, seen) for ch in children)

        desc = {c: dfs(c, set()) for c in group}
        N = len(group) if len(group) > 0 else 1
        return {c: -np.log((desc[c] + 1) / N) for c in group}

    # --------------------------------------------------------
    # Embedding Fetch
    # --------------------------------------------------------
    def _fetch_embeddings(self, cuis: List[str]):
        bases = list({extract_base_cui(c) for c in cuis if extract_base_cui(c)})

        query = f"""
        SELECT REF_CUI, REF_Embedding
        FROM `{self.project}.{self.dataset}.{self.embedding_table}`
        WHERE REF_CUI IN UNNEST(@cuis)
        """
        df = self.client.query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", bases)
                ]
            )
        ).to_dataframe()

        base_map = {}
        for _, r in df.iterrows():
            emb = r.REF_Embedding
            if isinstance(emb, list):
                base_map[r.REF_CUI] = np.asarray(emb, dtype=np.float32)

        for c in cuis:
            b = extract_base_cui(c)
            if b in base_map:
                self.embedding_cache[c] = base_map[b]

        logger.info(f"Fetched embeddings for {len(self.embedding_cache)} CUIs")

    # --------------------------------------------------------
    # Clustering + Selection
    # --------------------------------------------------------
    def _cluster_and_select(self, cuis: List[str], similarity=0.88, distance_thr=0.3) -> List[str]:
        X, labels = [], []
        for c in cuis:
            if c in self.embedding_cache:
                X.append(self.embedding_cache[c])
                labels.append(c)

        if len(X) < 2:
            return cuis

        X = np.vstack(X)
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1 - similarity,
            linkage="average",
            metric="cosine"
        )
        cluster_ids = clustering.fit_predict(X)

        selected = []
        for cid in np.unique(cluster_ids):
            idx = np.where(cluster_ids == cid)[0]
            centroid = X[idx].mean(axis=0)
            dists = cosine_distances([centroid], X[idx])[0]
            keep = idx[dists < distance_thr]
            if len(keep) == 0:
                keep = [idx[np.argmin(dists)]]
            selected.extend(labels[i] for i in keep)

        return list(set(selected))

    # --------------------------------------------------------
    # FULL REDUCTION PIPELINE
    # --------------------------------------------------------
    def reduce(self, input_cuis: List[str]) -> Tuple[List[str], ReductionStats]:
        start = time.time()
        input_cuis = list(set(input_cuis))

        hierarchy = asyncio.run(self._fetch_hierarchy(input_cuis))
        all_nodes = list(hierarchy["all_cuis"])

        semantic_groups = self._semantic_groups(all_nodes)

        retained = []
        for sty, cuis in semantic_groups.items():
            subgraph = {
                "parent_to_children": {
                    p: [c for c in cs if c in cuis]
                    for p, cs in hierarchy["parent_to_children"].items()
                    if p in cuis
                }
            }
            ic = self._compute_ic(cuis, subgraph)
            threshold = np.percentile(list(ic.values()), 50)
            retained.extend(c for c, v in ic.items() if v >= threshold)

        retained = list(set(retained))
        self._fetch_embeddings(retained)
        final = self._cluster_and_select(retained)

        stats = ReductionStats(
            initial_count=len(input_cuis),
            after_ic=len(retained),
            final_count=len(final),
            total_reduction_pct=safe_percentage(len(input_cuis) - len(final), len(input_cuis)),
            processing_time=time.time() - start
        )

        return final, stats

# ------------------------------------------------------------
# MAIN FUNCTION
# ------------------------------------------------------------
def main():
    # -------- CONFIG --------
    PROJECT_ID = "your_project_id"
    DATASET_ID = "your_dataset"
    CUI_API_URL = "your_cui_extraction_api_url"
    SUBNET_API_URL = "your_subnet_api_url"
    MRSTY_TABLE = "MRSTY"
    EMBEDDING_TABLE = "your_embedding_table"

    TEXTS = ["grams"]

    # -------- EXECUTION --------
    api_client = CUIAPIClient(CUI_API_URL)
    extracted = api_client.extract_cuis(TEXTS)

    filtered = filter_allowed_cuis(
        extracted,
        PROJECT_ID,
        DATASET_ID
    )

    reducer = EnhancedCUIReducer(
        project=PROJECT_ID,
        dataset=DATASET_ID,
        subnet_api_url=SUBNET_API_URL,
        mrsty_table=MRSTY_TABLE,
        embedding_table=EMBEDDING_TABLE
    )

    final_cuis, stats = reducer.reduce(filtered)

    logger.info("=" * 80)
    logger.info("FINAL RESULTS")
    logger.info("=" * 80)
    logger.info(f"Final CUIs ({len(final_cuis)}): {final_cuis}")
    logger.info(f"Stats: {stats.to_dict()}")

# ------------------------------------------------------------
# ENTRY POINT
# ------------------------------------------------------------
if __name__ == "__main__":
    main()
-----------------------
Extract CUIs from input text(s)

Filter to ICD / SNOMED / LOINC

Build hierarchy using subnetwork API on the full filtered CUI set

Assign semantic types (MRSTY) to all hierarchy-expanded CUIs

Group CUIs by semantic type and Allow multi-group membership

Compute IC scores within each semantic-type subgraph

Generate IC scores per group

Keep CUIs with IC > 50th percentile within each group

Ungroup all retained CUIs and remove duplicates

Fetch embeddings and cluster by similarity

From each cluster, pick CUIs far from the centroid

Collate all CUIs and remove duplicates
