# ============================================================
# STANDARD LIBRARIES
# ============================================================
import logging
import time
import threading
import subprocess
from typing import List, Dict, Tuple, Set, Any
from collections import defaultdict

# ============================================================
# THIRD PARTY
# ============================================================
import numpy as np
import requests
from scipy.spatial.distance import cdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score
from google.cloud import bigquery

# -------------------------
# Logging
# -------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# API CLIENTS
# ============================================================
class CUIAPIClient:
    """Client for CUI extraction API with token caching"""
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 5):
        self.api_base_url = api_base_url.rstrip("/")
        self.timeout = timeout
        self.top_k = top_k
        self.session = requests.Session()

    def _refresh_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token

            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                universal_newlines=True,
            )
            token = result.stdout.strip()
            self._cached_token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }
            self._token_expiry = now + 3300
            return self._cached_token

    def extract_cuis_batch(self, texts: List[str]) -> Dict[str, Set[str]]:
        headers = self._refresh_token()
        payload = {"query_texts": texts, "top_k": self.top_k}
        response = self.session.post(
            self.api_base_url, json=payload, headers=headers, timeout=self.timeout
        )
        response.raise_for_status()
        data = response.json()
        return {text: set(map(str, data.get(text, []))) for text in texts}


class SubnetAPIClient:
    """Subnet API client with batching and fail-soft behavior"""
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, subnet_url: str, batch_size: int = 500, timeout: int = 30):
        self.url = subnet_url.rstrip("/")
        self.batch_size = batch_size
        self.timeout = timeout

    def _get_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token

            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                universal_newlines=True,
            )
            token = result.stdout.strip()
            self._cached_token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }
            self._token_expiry = now + 3300
            return self._cached_token

    def _fetch_batch(self, cuis: List[str]) -> Tuple[Set[str], List[Tuple[str, str]]]:
        headers = self._get_token()
        payload = {"cuis": cuis, "cross_context": False}
        try:
            response = requests.post(
                f"{self.url}/subnet/",
                json=payload,
                headers=headers,
                timeout=self.timeout,
            )
            response.raise_for_status()
            nodes, edges = response.json().get("output", ([], []))
            return set(nodes), edges
        except Exception as e:
            logger.warning("Subnet batch failed (%s CUIs): %s", len(cuis), e)
            return set(), []

    def get_subnet_batch(self, cuis: List[str]) -> Tuple[Set[str], List[Tuple[str, str]]]:
        all_nodes: Set[str] = set()
        all_edges: List[Tuple[str, str]] = []

        for i in range(0, len(cuis), self.batch_size):
            batch = cuis[i : i + self.batch_size]
            nodes, edges = self._fetch_batch(batch)
            all_nodes |= nodes
            all_edges.extend(edges)

        return all_nodes, all_edges


# ============================================================
# BIGQUERY OPERATIONS
# ============================================================
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    client = bigquery.Client(project=project_id)
    query = f"""
    SELECT DISTINCT CUI
    FROM `{project_id}.{dataset_id}.MRCONSO`
    WHERE CUI IN UNNEST(@cuis)
      AND SAB IN ('ICD10', 'ICD10CM', 'ICD9CM', 'SNOMEDCT_US', 'LOINC', 'RXNORM')
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
    )
    df = client.query(query, job_config=job_config).result().to_dataframe()
    return df["CUI"].tolist()


def group_cuis_by_semantic_type(
    client: bigquery.Client, project_id: str, dataset_id: str, cuis: List[str]
) -> Dict[str, List[str]]:
    groups = defaultdict(list)
    for i in range(0, len(cuis), 5000):
        batch = cuis[i : i + 5000]
        query = f"""
        SELECT DISTINCT CUI, TUI
        FROM `{project_id}.{dataset_id}.MRSTY`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", batch)]
        )
        df = client.query(query, job_config=job_config).result().to_dataframe()
        for _, row in df.iterrows():
            groups[row["TUI"]].append(row["CUI"])
    return groups


def load_embeddings_batch(
    client: bigquery.Client, project_id: str, embedding_table: str, cuis: List[str]
) -> Dict[str, np.ndarray]:
    embeddings = {}
    for i in range(0, len(cuis), 1000):
        batch = cuis[i : i + 1000]
        query = f"""
        SELECT cui, embedding
        FROM `{project_id}.{embedding_table}`
        WHERE cui IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", batch)]
        )
        df = client.query(query, job_config=job_config).result().to_dataframe()
        for _, row in df.iterrows():
            emb = np.array(row["embedding"], dtype=float)
            embeddings[row["cui"]] = emb / (np.linalg.norm(emb) + 1e-10)
    return embeddings


# ============================================================
# GRAPH + IC (SECO)
# ============================================================
def build_hierarchy_graph(nodes: Set[str], edges: List[Tuple[str, str]]) -> Dict[str, Dict]:
    graph = defaultdict(lambda: {"parents": set(), "children": set()})
    for parent, child in edges:
        graph[parent]["children"].add(child)
        graph[child]["parents"].add(parent)
    for n in nodes:
        graph.setdefault(n, {"parents": set(), "children": set()})
    return dict(graph)


def compute_descendants(graph: Dict[str, Dict]) -> Dict[str, Set[str]]:
    cache = {}
    def dfs(node):
        if node in cache:
            return cache[node]
        visited = {node}
        for child in graph[node]["children"]:
            visited |= dfs(child)
        cache[node] = visited
        return visited
    for node in graph:
        dfs(node)
    return cache


def compute_seco_ic(descendants: Dict[str, Set[str]]) -> Dict[str, float]:
    total = len(descendants)
    return {
        cui: 1.0 - (np.log(len(desc)) / np.log(total))
        for cui, desc in descendants.items()
    }


# ============================================================
# REDUCTION
# ============================================================
def hierarchical_reduction_by_coverage(
    cuis: List[str], descendants: Dict[str, Set[str]], ic_scores: Dict[str, float]
) -> Set[str]:
    remaining, covered, selected = set(cuis), set(), set()
    while covered < remaining:
        best, gain = None, 0.0
        for cui in remaining - selected:
            new = (descendants[cui] & remaining) - covered
            score = sum(ic_scores[c] for c in new)
            if score > gain:
                best, gain = cui, score
        if not best:
            selected |= remaining - covered
            break
        selected.add(best)
        covered |= descendants[best]
    return selected


def embedding_based_clustering(
    cuis: List[str], embeddings: Dict[str, np.ndarray], ic_scores: Dict[str, float]
) -> Set[str]:
    valid = [c for c in cuis if c in embeddings]
    if len(valid) <= 2:
        return set(valid)

    vecs = np.vstack([embeddings[c] for c in valid])
    dist = cdist(vecs, vecs, metric="cosine")
    Z = linkage(squareform(dist), method="average")

    best_score, labels = -1, None
    for k in range(2, min(10, len(valid)) + 1):
        labs = fcluster(Z, k, criterion="maxclust")
        score = silhouette_score(dist, labs, metric="precomputed")
        if score > best_score:
            best_score, labels = score, labs

    selected = set()
    for cid in set(labels):
        idx = np.where(labels == cid)[0]
        cluster = [valid[i] for i in idx]
        scores = {
            cui: ic_scores.get(cui, 0) - np.mean(dist[idx[i], idx])
            for i, cui in enumerate(cluster)
        }
        selected.add(max(scores, key=scores.get))
    return selected


# ============================================================
# MAIN PIPELINE
# ============================================================
def reduce_cuis_pipeline(
    texts: List[str],
    project_id: str,
    dataset_id: str,
    embedding_table: str,
    cui_api_url: str,
    subnet_api_url: str,
) -> Dict[str, Any]:

    extractor = CUIAPIClient(cui_api_url)
    all_cuis = set().union(*extractor.extract_cuis_batch(texts).values())

    filtered = filter_allowed_cuis(all_cuis, project_id, dataset_id)
    client = bigquery.Client(project=project_id)
    grouped = group_cuis_by_semantic_type(client, project_id, dataset_id, filtered)

    subnet = SubnetAPIClient(subnet_api_url)
    nodes, edges = subnet.get_subnet_batch(filtered)

    if not nodes:
        logger.warning("Subnet unavailable; using flat hierarchy")
        nodes, edges = set(filtered), []

    graph = build_hierarchy_graph(nodes, edges)
    descendants = compute_descendants(graph)
    ic_scores = compute_seco_ic(descendants)

    hierarchy_selected = set().union(
        *[hierarchical_reduction_by_coverage(c, descendants, ic_scores) for c in grouped.values()]
    )

    embeddings = load_embeddings_batch(client, project_id, embedding_table, filtered)
    embedding_selected = set().union(
        *[embedding_based_clustering(c, embeddings, ic_scores) for c in grouped.values()]
    )

    final = hierarchy_selected | embedding_selected

    return {
        "original_cuis": all_cuis,
        "reduced_cuis": final,
        "statistics": {
            "original": len(all_cuis),
            "filtered": len(filtered),
            "hierarchy": len(hierarchy_selected),
            "embedding": len(embedding_selected),
            "final": len(final),
        },
    }
