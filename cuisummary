import asyncio
import aiohttp
import subprocess
import re
import logging
from pathlib import Path
import json
from typing import List, Dict, Set, Optional, Tuple
from collections import defaultdict, deque
import time
from dataclasses import dataclass, asdict
import hashlib
import numpy as np
from google.cloud import bigquery
import nest_asyncio
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import requests
from requests.adapters import HTTPAdapter, Retry
import threading

nest_asyncio.apply()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force: bool = False) -> Dict[str, str]:
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token

            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)

            token = proc.stdout.strip()
            if not token:
                raise RuntimeError("Empty token from gcloud")

            cls._token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            cls._expiry = now + 3300  # ~55 minutes
            return cls._token

# Data Classes
@dataclass
class ReductionStats:
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    group_stats: Dict = None
    missing_embeddings_count: int = 0
    embedding_fetch_time: float = 0.0
    hierarchy_fetch_time: float = 0.0

    def to_dict(self):
        return asdict(self)

# Filter CUIs
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    
    try:
        logger.info(f"Filtering {len(cuis)} CUIs to retain only ICD, SNOMED, and LOINC...")
        client = bigquery.Client(project=project_id)
        
        # Normalize CUIs for the query (remove suffixes)
        normalized_map = {}
        for cui in cuis:
            # Extract base CUI (C + 7 digits)
            match = re.match(r'^(C\d{7})', str(cui).upper())
            if match:
                base_cui = match.group(1)
                if base_cui not in normalized_map:
                    normalized_map[base_cui] = []
                normalized_map[base_cui].append(cui)
        
        # Query using base CUIs
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('CCSR_ICD10CM','CCSR_ICD10PCS','DMDICD10','ICD10','ICD10AE','ICD10AM','ICD10AMAE',
                      'ICD10CM','ICD10DUT','ICD10PCS','ICD9CM','ICPC2ICD10DUT','ICPC2ICD10ENG',
                      'SNOMEDCT_US', 'LOINC')
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", list(normalized_map.keys()))
            ]
        )
        
        df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        
        # Map back to original CUIs with suffixes
        allowed_base = set(df['CUI'].tolist())
        allowed_with_suffix = []
        
        for base_cui in allowed_base:
            if base_cui in normalized_map:
                # Include all original CUIs (with suffixes) that map to this base
                allowed_with_suffix.extend(normalized_map[base_cui])
        
        logger.info(f"{len(allowed_with_suffix)} CUIs after filter (including suffixes)")
        return allowed_with_suffix
        
    except Exception as e:
        logger.error(f"Failed to filter CUIs by SAB: {str(e)}")
        return []

# CUI API Client
class CUIAPIClient:
    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3, max_retries: int = 3):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.max_retries = max_retries

        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        if not texts:
            return set()
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = GCPTokenProvider.get_headers()
        logger.info(f"Extracting CUIs from {len(texts)} text(s)...")
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            if response.status_code == 401 and retry_auth:
                logger.warning("Token expired, refreshing...")
                GCPTokenProvider.get_headers(force=True)
                return self.extract_cuis_batch(texts, retry_auth=False)
            response.raise_for_status()
            data = response.json()
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
            return all_cuis
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return set()

# Enhanced CUI Reducer
class EnhancedCUIReducer:
    
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        subnet_api_url: str,
        cui_description_table: str,
        cui_embeddings_table: str,
        cui_narrower_table: str,
        max_hierarchy_depth: int = 5,
        query_timeout: int = 300
    ):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.subnet_api_url = subnet_api_url
        self.cui_description_table = cui_description_table
        self.cui_embeddings_table = cui_embeddings_table
        self.cui_narrower_table = cui_narrower_table
        self.max_hierarchy_depth = max_hierarchy_depth
        self.query_timeout = query_timeout

        self._hierarchy_cache = {}
        self._ic_scores_cache = {}
        self._description_cache = {}
        self._embedding_batch_cache = {}
        self._missing_embeddings_total = 0
        
        # Pre-compile regex for better performance
        self._cui_regex = re.compile(r'^(C\d{7})')

    def reduce(
        self,
        input_cuis: List[str],
        ic_percentile: int = 75,
        similarity_threshold: float = 0.88,
        distance_from_centroid_threshold: float = 0.3
    ) -> Tuple[List[str], ReductionStats]:

        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        
        logger.info(f"Starting semantic-group based reduction for {initial_count} CUIs")

        # Pre-fetch ALL embeddings in one query for better performance
        logger.info("Pre-fetching all embeddings...")
        embedding_fetch_start = time.time()
        self._prefetch_all_embeddings(input_cuis)
        embedding_fetch_time = time.time() - embedding_fetch_start
        logger.info(f"Embedding fetch completed in {embedding_fetch_time:.2f}s")

        # Group by semantic type
        semantic_groups = self._group_by_semantic_type_dynamic(input_cuis)
        logger.info(f"Created {len(semantic_groups)} semantic groups")
        
        all_reduced_cuis = []
        group_stats = {}
        total_after_ic = 0
        hierarchy_fetch_start = time.time()
        
        for group_name, group_cuis in semantic_groups.items():
            if not group_cuis:
                continue
            
            # Build hierarchy within group
            group_hierarchy = self._build_hierarchy_depthwise(group_cuis)
            
            # Compute IC scores within group
            group_ic_scores = self._compute_ic_scores_within_group(
                group_hierarchy, 
                group_cuis,
                group_name
            )
            
            # Keep only CUIs >= percentile IC
            if group_ic_scores:
                ic_threshold = np.percentile(
                    list(group_ic_scores.values()), 
                    ic_percentile
                )
                
                high_ic_cuis = [
                    cui for cui in group_cuis 
                    if group_ic_scores.get(cui, 0) >= ic_threshold
                ]
            else:
                high_ic_cuis = group_cuis
            
            total_after_ic += len(high_ic_cuis)
            
            # Semantic clustering and diversity selection
            if len(high_ic_cuis) > 1:
                group_reduced = self._cluster_and_select_diverse(
                    high_ic_cuis,
                    similarity_threshold,
                    distance_from_centroid_threshold
                )
            else:
                group_reduced = high_ic_cuis
            
            all_reduced_cuis.extend(group_reduced)
            
            group_stats[group_name] = {
                'original': len(group_cuis),
                'after_ic_filter': len(high_ic_cuis),
                'final': len(group_reduced),
                'reduction_pct': (1 - len(group_reduced)/len(group_cuis)) * 100 if group_cuis else 0
            }
        
        hierarchy_fetch_time = time.time() - hierarchy_fetch_start
        
        # Remove duplicates
        final_cuis = list(set(all_reduced_cuis))
        final_count = len(final_cuis)
        
        # Validate coverage
        coverage_score = self._calculate_coverage_fast(input_cuis, final_cuis)
        logger.info(f"Coverage score: {coverage_score:.2%} of original CUIs have representative")

        # Fetch descriptions for final CUIs
        self._fetch_descriptions(final_cuis)

        logger.info(f"Total missing embeddings: {self._missing_embeddings_total}")
        
        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=total_after_ic,
            final_count=final_count,
            ic_rollup_reduction_pct=self._safe_percentage(initial_count - total_after_ic, initial_count),
            semantic_clustering_reduction_pct=self._safe_percentage(total_after_ic - final_count, initial_count),
            total_reduction_pct=self._safe_percentage(initial_count - final_count, initial_count),
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_percentile,
            hierarchy_size=sum(len(h.get("all_cuis", [])) for h in self._hierarchy_cache.values()),
            api_call_time=0.0,
            group_stats=group_stats,
            missing_embeddings_count=self._missing_embeddings_total,
            embedding_fetch_time=embedding_fetch_time,
            hierarchy_fetch_time=hierarchy_fetch_time
        )

        return final_cuis, stats

    def _prefetch_all_embeddings(self, all_cuis: List[str]) -> None:
        """Pre-fetch all embeddings in a single query for performance"""
        
        # Normalize all CUIs
        normalized_set = set()
        for cui in all_cuis:
            norm = self._normalize_cui(cui)
            if norm:
                normalized_set.add(norm)
        
        normalized_list = list(normalized_set)
        
        if not normalized_list:
            return
        
        # Fetch all embeddings in one query
        query = f"""
        SELECT 
            REF_CUI AS cui,
            REF_Embedding AS embedding
        FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
        WHERE REF_CUI IN UNNEST(@cuis)
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", normalized_list)
            ]
        )
        
        df = self.client.query(query, job_config=job_config).result(timeout=120).to_dataframe()
        
        # Cache embeddings
        for _, row in df.iterrows():
            cui = row['cui']
            emb = row['embedding']
            
            if emb is not None:
                arr = np.asarray(emb, dtype=np.float32)
                if arr.ndim == 1 and arr.size > 0:
                    self._embedding_batch_cache[cui] = arr
        
        logger.info(f"Pre-fetched {len(self._embedding_batch_cache)} embeddings")

    def _group_by_semantic_type_dynamic(self, cuis: List[str]) -> Dict[str, List[str]]:
        """Dynamically group CUIs by their semantic type names (STY) from MRSTY table"""
        
        # Normalize CUIs for query
        normalized_map = {}
        for cui in cuis:
            norm = self._normalize_cui(cui)
            if norm:
                if norm not in normalized_map:
                    normalized_map[norm] = []
                normalized_map[norm].append(cui)
        
        # Query to get all CUI-STY mappings
        query = f"""
        WITH cui_types AS (
            SELECT DISTINCT 
                CUI,
                TUI,
                STY
            FROM `{self.project_id}.{self.dataset_id}.MRSTY`
            WHERE CUI IN UNNEST(@cuis)
        )
        SELECT * FROM cui_types
        ORDER BY STY, CUI
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", list(normalized_map.keys()))
            ]
        )
        
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        
        if df.empty:
            logger.warning("No semantic type information found in MRSTY")
            return {'UNKNOWN': cuis}
        
        # Group by semantic type
        semantic_groups = defaultdict(set)
        cui_to_types = defaultdict(set)
        
        for _, row in df.iterrows():
            base_cui = row['CUI']
            sty = row['STY']
            
            # Map back to original CUIs with suffixes
            if base_cui in normalized_map:
                for original_cui in normalized_map[base_cui]:
                    semantic_groups[sty].add(original_cui)
                    cui_to_types[original_cui].add(sty)
        
        # Handle CUIs with multiple or no semantic types
        final_groups = defaultdict(list)
        
        for cui in cuis:
            if cui not in cui_to_types:
                final_groups['UNKNOWN'].append(cui)
            else:
                types = cui_to_types[cui]
                if len(types) == 1:
                    final_groups[list(types)[0]].append(cui)
                else:
                    # Choose most specific type (longer name)
                    sorted_types = sorted(types, key=lambda x: (-len(x), x))
                    final_groups[sorted_types[0]].append(cui)
        
        return dict(final_groups)

    def _compute_ic_scores_within_group(
        self, 
        hierarchy: Dict, 
        group_cuis: List[str],
        group_name: str
    ) -> Dict[str, float]:
        """Compute IC scores relative to the group"""
        
        cache_key = f"{group_name}_{tuple(sorted(group_cuis))}"
        if cache_key in self._ic_scores_cache:
            return self._ic_scores_cache[cache_key]
        
        parent_to_children = hierarchy.get("parent_to_children", {})
        group_set = set(group_cuis)
        
        # Count descendants within this semantic group
        descendant_counts = {}
        
        def count_group_descendants(cui: str, visited: Set[str] = None) -> int:
            if visited is None:
                visited = set()
            if cui in visited or cui not in group_set:
                return 0
            visited.add(cui)
            
            children = [c for c in parent_to_children.get(cui, []) if c in group_set]
            count = len(children)
            for child in children:
                count += count_group_descendants(child, visited)
            
            descendant_counts[cui] = count
            return count
        
        for cui in group_cuis:
            if cui not in descendant_counts:
                try:
                    count_group_descendants(cui)
                except RecursionError:
                    descendant_counts[cui] = 0
        
        # Calculate IC scores
        group_size = len(group_cuis)
        ic_scores = {}
        for cui in group_cuis:
            desc_count = descendant_counts.get(cui, 0)
            ic_scores[cui] = max(0.0, -np.log((desc_count + 1) / group_size))
        
        self._ic_scores_cache[cache_key] = ic_scores
        return ic_scores

    def _cluster_and_select_diverse(
        self,
        cui_list: List[str],
        similarity_threshold: float,
        distance_threshold: float
    ) -> List[str]:

        if len(cui_list) <= 1:
            return cui_list

        try:
            # Normalize CUIs - Remove suffixes
            raw_to_norm = {}
            norm_to_raw = defaultdict(list)
            
            for raw_cui in cui_list:
                normalized = self._normalize_cui(raw_cui)
                if normalized:
                    raw_to_norm[raw_cui] = normalized
                    norm_to_raw[normalized].append(raw_cui)
            
            normalized_cuis = list(set(raw_to_norm.values()))
            
            if len(normalized_cuis) < 2:
                return cui_list
            
            # Use pre-fetched embeddings
            valid_embeddings = []
            valid_cuis = []
            missing_norms = []
            
            for norm_cui in normalized_cuis:
                if norm_cui in self._embedding_batch_cache:
                    valid_embeddings.append(self._embedding_batch_cache[norm_cui])
                    valid_cuis.append(norm_cui)
                else:
                    missing_norms.append(norm_cui)
            
            if missing_norms:
                self._missing_embeddings_total += len(missing_norms)
                if len(missing_norms) <= 5:
                    logger.debug(f"Missing embeddings for: {missing_norms}")
            
            if len(valid_embeddings) < 2:
                logger.warning(f"Only {len(valid_embeddings)} valid embeddings found")
                return cui_list
            
            embeddings = np.vstack(valid_embeddings)
            cuis = np.array(valid_cuis)
            
            # Clustering
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - similarity_threshold,
                metric="cosine",
                linkage="average"
            )
            
            labels = clustering.fit_predict(embeddings)
            
            # Select diverse CUIs from each cluster
            selected_normalized = set()
            
            for cluster_id in np.unique(labels):
                cluster_indices = np.where(labels == cluster_id)[0]
                
                if len(cluster_indices) == 1:
                    selected_normalized.add(cuis[cluster_indices[0]])
                    continue
                
                cluster_embeddings = embeddings[cluster_indices]
                centroid = np.mean(cluster_embeddings, axis=0)
                distances = cosine_distances([centroid], cluster_embeddings)[0]
                
                far_indices = cluster_indices[distances > distance_threshold]
                
                if len(far_indices) > 0:
                    selected_normalized.update(cuis[far_indices])
                else:
                    farthest_idx = cluster_indices[np.argmax(distances)]
                    selected_normalized.add(cuis[farthest_idx])

            # Map back to original CUIs with suffixes
            final_cuis = []
            
            for norm_cui in selected_normalized:
                if norm_cui in norm_to_raw:
                    final_cuis.extend(norm_to_raw[norm_cui])
            
            for norm_cui in missing_norms:
                if norm_cui in norm_to_raw:
                    final_cuis.extend(norm_to_raw[norm_cui])
            
            # Remove duplicates while preserving order
            seen = set()
            final_cuis = [x for x in final_cuis if not (x in seen or seen.add(x))]
            
            return final_cuis

        except Exception as e:
            logger.error(f"Clustering failed: {str(e)}", exc_info=True)
            return cui_list

    def _calculate_coverage_fast(self, original_cuis: List[str], reduced_cuis: List[str]) -> float:
        """Fast coverage calculation using pre-fetched embeddings"""
        
        if not original_cuis or not reduced_cuis:
            return 0.0
        
        original_set = set(original_cuis)
        reduced_set = set(reduced_cuis)
        
        # Direct matches
        covered = original_set & reduced_set
        covered_count = len(covered)
        
        # For non-direct matches, check similarity
        remaining = original_set - covered
        
        if remaining and len(reduced_set) > 0:
            # Normalize and get embeddings
            reduced_embeddings = []
            
            for cui in reduced_set:
                norm = self._normalize_cui(cui)
                if norm and norm in self._embedding_batch_cache:
                    reduced_embeddings.append(self._embedding_batch_cache[norm])
            
            if reduced_embeddings:
                reduced_matrix = np.vstack(reduced_embeddings)
                
                for cui in remaining:
                    norm = self._normalize_cui(cui)
                    if norm and norm in self._embedding_batch_cache:
                        similarities = 1 - cosine_distances(
                            [self._embedding_batch_cache[norm]], 
                            reduced_matrix
                        )[0]
                        
                        if np.max(similarities) >= 0.85:
                            covered_count += 1
        
        return covered_count / len(original_cuis)

    def _build_hierarchy_depthwise(self, cuis: List[str]) -> Dict:
        key = tuple(sorted(cuis))
        if key in self._hierarchy_cache:
            return self._hierarchy_cache[key]

        hierarchy = asyncio.run(self._fetch_hierarchy_parallel(cuis))
        self._hierarchy_cache[key] = hierarchy
        return hierarchy

    async def _fetch_hierarchy_parallel(self, cuis: List[str]) -> Dict:
        """Fetch hierarchy with better parallelization"""
        
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(cuis)

        # Normalize CUIs for API call
        normalized_cuis = []
        for cui in cuis:
            norm = self._normalize_cui(cui)
            if norm:
                normalized_cuis.append(norm)
        
        normalized_cuis = list(set(normalized_cuis))

        headers = GCPTokenProvider.get_headers()
        batch_size = 100  # Increased batch size
        max_concurrent = 10
        timeout = aiohttp.ClientTimeout(total=30)

        async with aiohttp.ClientSession(timeout=timeout) as session:
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def fetch_batch_with_semaphore(batch):
                async with semaphore:
                    try:
                        async with session.post(
                            f"{self.subnet_api_url}/subnet/",
                            json={"cuis": batch, "cross_context": False},
                            headers=headers
                        ) as resp:
                            if resp.status != 200:
                                return [], []
                            data = await resp.json()
                            return data.get("output", ([], []))
                    except asyncio.TimeoutError:
                        logger.warning(f"Timeout for batch of {len(batch)} CUIs")
                        return [], []
                    except Exception as e:
                        logger.warning(f"Error fetching batch: {str(e)}")
                        return [], []

            tasks = []
            for i in range(0, len(normalized_cuis), batch_size):
                batch = normalized_cuis[i:i + batch_size]
                tasks.append(fetch_batch_with_semaphore(batch))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, Exception):
                    continue
                nodes, edges = result
                for p, c in edges:
                    p_norm = self._normalize_cui(p)
                    c_norm = self._normalize_cui(c)
                    if p_norm and c_norm:
                        parent_to_children[p_norm].append(c_norm)
                        child_to_parents[c_norm].append(p_norm)
                        all_cuis.update([p_norm, c_norm])

        return {
            "child_to_parents": dict(child_to_parents),
            "parent_to_children": dict(parent_to_children),
            "all_cuis": all_cuis
        }

    def _fetch_descriptions(self, cuis: List[str]):
        """Fetch descriptions for CUIs"""
        normalized_to_fetch = set()
        
        for cui in cuis:
            norm = self._normalize_cui(cui)
            if norm and norm not in self._description_cache:
                normalized_to_fetch.add(norm)
        
        if not normalized_to_fetch:
            return

        query = f"""
            SELECT CUI AS cui, Definition AS description
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_description_table}`
            WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", list(normalized_to_fetch))
            ]
        )

        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()

        for _, row in df.iterrows():
            self._description_cache[row['cui']] = row['description']

    @staticmethod
    def _normalize_cui(cui: str) -> Optional[str]:
        """Extract the base CUI (C + 7 digits) from formats like C0000000-1, C0000000-12"""
        if not cui:
            return None
        
        # Match C followed by exactly 7 digits (total 8 characters)
        match = re.match(r'^(C\d{7})', str(cui).upper())
        return match.group(1) if match else None

    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        return (numerator / denominator * 100) if denominator else 0.0


# Main execution
if __name__ == "__main__": 
    # Configuration
    project_id = "your_project_id"
    dataset_id = "your_dataset"
    api_url = "your_cui_extraction_api_url"
    subnet_api_url = "your_subnet_api_url"
    cui_desc_table = "your_cui_description_table"
    embedding_table = "your_embedding_table"
    descendants_table = "your_descendants_table"

    # Example usage
    texts = ["grams"] 
    
    # Extract CUIs from text
    api_client = CUIAPIClient(api_base_url=api_url)
    extracted_cuis = api_client.extract_cuis_batch(texts)

    # Filter to allowed SABs
    filtered_cuis = filter_allowed_cuis(extracted_cuis, project_id, dataset_id)

    # Initialize reducer
    reducer = EnhancedCUIReducer(
        project_id,
        dataset_id,
        subnet_api_url=subnet_api_url,
        cui_description_table=cui_desc_table,
        cui_embeddings_table=embedding_table,
        cui_narrower_table=descendants_table
    )

    # Perform reduction
    final_cuis, stats = reducer.reduce(
        filtered_cuis,
        ic_percentile=50,
        similarity_threshold=0.88,
        distance_from_centroid_threshold=0.3
    )

    # Print results
    logger.info(f"Reduction Stats: {stats.to_dict()}")
    logger.info(f"Final CUI count: {len(final_cuis)}")
    
    # Display timing breakdown
    logger.info(f"\nTiming Breakdown:")
    logger.info(f"  - Embedding fetch: {stats.embedding_fetch_time:.2f}s")
    logger.info(f"  - Hierarchy fetch: {stats.hierarchy_fetch_time:.2f}s")
    logger.info(f"  - Total processing: {stats.processing_time:.2f}s")
    
    # Display group statistics
    if stats.group_stats:
        logger.info(f"\nGroup Statistics:")
        for group, group_stat in stats.group_stats.items():
            logger.info(f"  {group}: {group_stat['original']} â†’ {group_stat['final']} ({group_stat['reduction_pct']:.1f}% reduction)")
