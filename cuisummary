import time
import numpy as np
import logging
from typing import List, Dict, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import subprocess
import threading

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ------------------- CONFIG -------------------
@dataclass
class ReductionConfig:
    project_id: str
    dataset_id: str
    mrconso_table: str = "MRCONSO"
    mrrel_table: str = "MRREL"
    mrsty_table: str = "MRSTY"
    cui_embeddings_table: str = "cui_embeddings"
    cui_descriptions_table: str = "cui_descriptions"
    api_url: str = None
    api_timeout: int = 60
    api_top_k: int = 3
    allowed_sab: List[str] = None
    semantic_threshold: float = 0.88
    use_semantic_clustering: bool = True
    max_hierarchy_depth: int = 3
    query_timeout: int = 300
    max_query_results: int = 100000

    def __post_init__(self):
        if self.allowed_sab is None:
            self.allowed_sab = ['ICD10', 'ICD10CM', 'ICD10PCS', 'ICD9CM', 'SNOMEDCT_US', 'LOINC']

@dataclass
class ReductionStats:
    initial_count: int
    after_filter: int
    after_ic_threshold: int
    final_count: int
    filter_reduction_pct: float
    ic_threshold_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0

    def to_dict(self):
        return asdict(self)

# ------------------- CUI API CLIENT -------------------
class CUIAPIClient:
    def __init__(self, config: ReductionConfig):
        self.config = config
        self.api_base_url = config.api_url.rstrip('/') if config.api_url else None
        self.timeout = config.api_timeout
        self.top_k = config.api_top_k
        self.session = self._setup_session()
        self._token = None
        self._token_expiry = 0
        self._token_lock = threading.Lock()

    def _setup_session(self):
        session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429,500,502,503,504])
        adapter = HTTPAdapter(max_retries=retry)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session

    def _get_token(self):
        with self._token_lock:
            if time.time() < self._token_expiry:
                return self._token
            try:
                result = subprocess.run(
                    ['gcloud','auth','print-identity-token'],
                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, timeout=10
                )
                if result.returncode == 0:
                    self._token = result.stdout.strip()
                    self._token_expiry = time.time() + 3300
                    logger.info("GCP token refreshed")
                    return self._token
            except Exception as e:
                logger.error(f"Token refresh failed: {e}")
            return None

    def extract_cuis(self, texts: List[str]) -> set:
        if not texts or not self.api_base_url:
            return set()
        token = self._get_token()
        if not token:
            logger.error("No valid auth token")
            return set()
        headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
        try:
            response = self.session.post(
                self.api_base_url,
                json={"query_texts": texts, "top_k": self.top_k},
                headers=headers,
                timeout=self.timeout
            )
            response.raise_for_status()
            data = response.json()
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            logger.info(f"Extracted {len(all_cuis)} CUIs from {len(texts)} texts")
            return all_cuis
        except Exception as e:
            logger.error(f"CUI extraction failed: {e}")
            return set()

# ------------------- CUI REDUCER -------------------
class CUIReducer:
    def __init__(self, config: ReductionConfig):
        self.config = config
        self.client = bigquery.Client(project=config.project_id)
        self._hierarchy_cache = None
        self._ic_scores_cache = None
        self._descriptions_cache = {}

    # ------------------- SAB FILTER -------------------
    def filter_to_allowed_sab(self, cuis: set) -> List[str]:
        if not cuis:
            return []
        try:
            query = f"""
            SELECT DISTINCT CUI
            FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.mrconso_table}`
            WHERE CUI IN UNNEST(@cuis)
              AND SAB IN UNNEST(@sabs)
            LIMIT {self.config.max_query_results}
            """
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis","STRING",list(cuis)),
                    bigquery.ArrayQueryParameter("sabs","STRING",self.config.allowed_sab)
                ]
            )
            df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            filtered_cuis = df['CUI'].tolist()
            logger.info(f"SAB filter: {len(cuis)} -> {len(filtered_cuis)} CUIs")
            return filtered_cuis
        except Exception as e:
            logger.error(f"SAB filter failed: {e}")
            return list(cuis)

    # ------------------- HIERARCHY & IC -------------------
    def build_hierarchy(self, relevant_cuis: List[str]) -> dict:
        if self._hierarchy_cache:
            return self._hierarchy_cache
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(relevant_cuis)
        visited = set()
        frontier = set(relevant_cuis)
        for depth in range(self.config.max_hierarchy_depth):
            if not frontier:
                break
            query = f"""
            SELECT DISTINCT cui1, cui2, rel
            FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.mrrel_table}`
            WHERE (cui1 IN UNNEST(@frontier) OR cui2 IN UNNEST(@frontier))
              AND rel IN ('PAR','CHD')
            LIMIT {self.config.max_query_results}
            """
            job_config = bigquery.QueryJobConfig(
                query_parameters=[bigquery.ArrayQueryParameter("frontier","STRING",list(frontier))]
            )
            df = self.client.query(query, job_config=job_config).result(timeout=self.config.query_timeout).to_dataframe()
            if len(df)==0: break
            next_frontier = set()
            for _, row in df.iterrows():
                cui1,cui2,rel=str(row['cui1']),str(row['cui2']),str(row['rel'])
                if rel=='PAR':
                    parent_to_children[cui1].append(cui2)
                    child_to_parents[cui2].append(cui1)
                elif rel=='CHD':
                    parent_to_children[cui2].append(cui1)
                    child_to_parents[cui1].append(cui2)
                all_cuis.update([cui1,cui2])
                if cui1 not in visited: next_frontier.add(cui1)
                if cui2 not in visited: next_frontier.add(cui2)
            visited.update(frontier)
            frontier = next_frontier - visited
        hierarchy = {'child_to_parents':dict(child_to_parents), 'parent_to_children':dict(parent_to_children), 'all_cuis':all_cuis}
        self._hierarchy_cache = hierarchy
        logger.info(f"Hierarchy built with {len(all_cuis)} CUIs")
        return hierarchy

    def compute_ic_scores(self, hierarchy: dict) -> dict:
        if self._ic_scores_cache:
            return self._ic_scores_cache
        parent_to_children = hierarchy.get('parent_to_children',{})
        all_cuis = hierarchy.get('all_cuis',set())
        total=len(all_cuis)
        if total==0: return {}
        descendant_counts={}
        def count_descendants(cui:str,visited:set=None)->int:
            if visited is None: visited=set()
            if cui in visited or cui in descendant_counts: return descendant_counts.get(cui,0)
            visited.add(cui)
            children = parent_to_children.get(cui,[])
            count=len(children)
            for child in children: count+=count_descendants(child,visited)
            descendant_counts[cui]=count
            return count
        for cui in all_cuis:
            if cui not in descendant_counts:
                try: count_descendants(cui)
                except RecursionError: descendant_counts[cui]=0
        ic_scores={cui: max(0.0,-np.log((descendant_counts.get(cui,0)+1)/total)) for cui in all_cuis}
        self._ic_scores_cache=ic_scores
        return ic_scores

    # ------------------- SEMANTIC-TYPE BUCKETING -------------------
    def get_semantic_types(self, cui_list: List[str]) -> Dict[str,List[str]]:
        if not cui_list: return {}
        query=f"""
        SELECT CUI, STY
        FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter("cuis","STRING",cui_list)])
        result={}
        try:
            df=self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            for _,row in df.iterrows():
                cui=str(row['CUI'])
                sty=str(row['STY'])
                result.setdefault(cui,[]).append(sty)
            for c in cui_list: result.setdefault(c,["UNKNOWN"])
            return result
        except Exception as e:
            logger.warning(f"MRSTY lookup failed: {e}")
            return {c:["UNKNOWN"] for c in cui_list}

    def map_sty_to_bucket(self, sty_list: List[str]) -> str:
        sty_join=" ".join(sty_list).upper()
        if any(k in sty_join for k in ("ANATOMIC","BODY","ORGAN","STRUCTURE")): return "ANATOMY"
        if any(k in sty_join for k in ("PROCEDURE","THERAPEUTIC","DIAGNOSTIC","IMAGING","RADIOLOGY")): return "PROCEDURE"
        if any(k in sty_join for k in ("FINDING","SIGN","SYMPTOM","PATHOLOGY","DISEASE")): return "FINDING"
        if any(k in sty_join for k in ("LABORATORY","TEST","LOINC")): return "LAB"
        if any(k in sty_join for k in ("DEVICE","EQUIPMENT")): return "DEVICE"
        if any(k in sty_join for k in ("DRUG","CHEMICAL")): return "DRUG"
        if "UNKNOWN" in sty_join: return "UNKNOWN"
        return "OTHER"

    # ------------------- EMBEDDING CLUSTERING -------------------
    def semantic_clustering(self, cui_list: List[str], similarity_threshold: float) -> List[str]:
        if not self.config.use_semantic_clustering or len(cui_list)<=1:
            return cui_list
        try:
            query=f"""
            SELECT REF_CUI as cui, REF_Embedding as embedding
            FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.cui_embeddings_table}`
            WHERE REF_CUI IN UNNEST(@cuis)
            LIMIT {self.config.max_query_results}
            """
            job_config=bigquery.QueryJobConfig(
                query_parameters=[bigquery.ArrayQueryParameter("cuis","STRING",cui_list)]
            )
            df=self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            if len(df)==0: return cui_list
            embeddings=np.vstack(df['embedding'].values)
            cuis=df['cui'].values
            clustering=AgglomerativeClustering(
                n_clusters=None,distance_threshold=1-similarity_threshold,
                metric='cosine',linkage='average'
            )
            labels=clustering.fit_predict(embeddings)
            final_cuis=[]
            for cluster_id in np.unique(labels):
                indices=np.where(labels==cluster_id)[0]
                cluster_embs = embeddings[indices]
                cluster_mean = np.mean(cluster_embs, axis=0)
                # Pick all CUIs sufficiently far from centroid to retain diversity
                dists = np.linalg.norm(cluster_embs - cluster_mean, axis=1)
                # Keep CUIs farther than 0.5*std or top N
                keep_idx = [indices[i] for i,dist in enumerate(dists) if dist > np.std(dists)*0.5]
                # Ensure at least one representative
                if not keep_idx:
                    keep_idx = [indices[0]]
                final_cuis.extend(cuis[keep_idx])
            return list(set(final_cuis))
        except Exception as e:
            logger.warning(f"Clustering failed: {e}")
            return cui_list

    # ------------------- REDUCTION PIPELINE -------------------
    def reduce(self, input_cuis: List[str]) -> Tuple[List[str], ReductionStats]:
        start = time.time()
        initial_count = len(input_cuis)
        input_cuis = list(set(input_cuis))
        
        # 1️⃣ SAB filter
        filtered = self.filter_to_allowed_sab(set(input_cuis))
        after_filter = len(filtered)
        if not filtered:
            return [], ReductionStats(initial_count,0,0,0,100,0,0,100,0,0)
        
        # 2️⃣ Hierarchy + IC
        hierarchy = self.build_hierarchy(filtered)
        ic_scores = self.compute_ic_scores(hierarchy)
        
        # 3️⃣ Adaptive IC threshold
        ic_array = np.array([ic_scores.get(c,0) for c in filtered])
        ic_threshold = np.mean(ic_array) - np.std(ic_array)
        bucket_filtered = [c for c in filtered if ic_scores.get(c,0) >= ic_threshold]
        after_ic = len(bucket_filtered)
        
        # 4️⃣ Optional: Semantic-type buckets could be applied here if desired
        
        # 5️⃣ Embedding clustering
        final = self.semantic_clustering(bucket_filtered, self.config.semantic_threshold)
        final_count = len(final)
        
        stats = ReductionStats(
            initial_count=initial_count,
            after_filter=after_filter,
            after_ic_threshold=after_ic,
            final_count=final_count,
            filter_reduction_pct=(initial_count-after_filter)/initial_count*100,
            ic_threshold_reduction_pct=(after_filter-after_ic)/initial_count*100,
            semantic_clustering_reduction_pct=(after_ic-final_count)/initial_count*100,
            total_reduction_pct=(initial_count-final_count)/initial_count*100,
            processing_time=time.time()-start,
            hierarchy_size=len(hierarchy.get('all_cuis',set()))
        )
        logger.info(f"Reduction complete: {initial_count} -> {final_count} ({stats.total_reduction_pct:.1f}%)")
        return final, stats

    # ------------------- DESCRIPTIONS -------------------
    def get_descriptions(self, cui_list: List[str]) -> Dict[str,str]:
        if not cui_list: return {}
        uncached=[c for c in cui_list if c not in self._descriptions_cache]
        if uncached:
            try:
                query=f"""
                SELECT CUI as cui, Definition as description
                FROM `{self.config.project_id}.{self.config.dataset_id}.{self.config.cui_descriptions_table}`
                WHERE CUI IN UNNEST(@cuis)
                """
                job_config=bigquery.QueryJobConfig(
                    query_parameters=[bigquery.ArrayQueryParameter("cuis","STRING",uncached)]
                )
                df=self.client.query(query, job_config=job_config).result(timeout=30).to_dataframe()
                for _,row in df.iterrows(): self._descriptions_cache[row['cui']]=row['description']
            except Exception as e:
                logger.warning(f"Description lookup failed: {e}")
        return {c:self._descriptions_cache.get(c,"N/A") for c in cui_list}

# ------------------- MAIN FUNCTION -------------------
def run_cui_reduction(texts: List[str], config: ReductionConfig) -> Tuple[List[str],List[str],Dict[str,str],ReductionStats]:
    try:
        api_client=CUIAPIClient(config)
        start=time.time()
        initial_cuis=api_client.extract_cuis(texts)
        api_time=time.time()-start
        if not initial_cuis: return [],[],{},None
        reducer=CUIReducer(config)
        reduced,stats=reducer.reduce(list(initial_cuis))
        stats.api_call_time=api_time
        descriptions=reducer.get_descriptions(reduced)
        return list(initial_cuis),reduced,descriptions,stats
    except Exception as e:
        logger.error(f"Reduction pipeline failed: {e}")
        return [],[],{},None

# ------------------- EXAMPLE USAGE -------------------
if __name__ == "__main__":
    config = ReductionConfig(
        project_id="your-project-id",
        dataset_id="your-dataset",
        api_url="https://your-api.run.app/endpoint",
        max_hierarchy_depth=3,
        semantic_threshold=0.88,
        use_semantic_clustering=True
    )

    texts = [
        "Patient has a brain tumor and underwent MRI scan.",
        "X-ray shows fracture in the left femur."
    ]

    initial_cuis, reduced_cuis, descriptions, stats = run_cui_reduction(texts, config)

    print("\n===== Original CUIs =====")
    print(initial_cuis)

    print("\n===== Reduced CUIs =====")
    print(reduced_cuis)

    print("\n===== Descriptions =====")
    for cui, desc in descriptions.items():
        print(f"{cui}: {desc}")

    if stats:
        print("\n===== Reduction Stats =====")
        print(f"Initial CUIs: {stats.initial_count}")
        print(f"After Filter: {stats.after_filter}")
        print(f"After IC Threshold: {stats.after_ic_threshold}")
        print(f"After Clustering: {stats.final_count}")
        print(f"Total Reduction: {stats.total_reduction_pct:.1f}%")
        print(f"Processing Time: {stats.processing_time:.2f}s, API Call Time: {stats.api_call_time:.2f}s")
