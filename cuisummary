import logging
import time
import threading
import subprocess
from typing import List, Dict, Set, Tuple
from collections import defaultdict
import requests
from google.cloud import bigquery
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# CUI API CLIENT
# ============================================================
class CUIAPIClient:
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 5):
        self.api_base_url = api_base_url.rstrip("/")
        self.timeout = timeout
        self.top_k = top_k
        self.session = requests.Session()

    def _refresh_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token
            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                universal_newlines=True,
            )
            token = result.stdout.strip()
            self._cached_token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }
            self._token_expiry = now + 3300
            return self._cached_token

    def extract_cuis_batch(self, texts: List[str]) -> Dict[str, Set[str]]:
        headers = self._refresh_token()
        payload = {"query_texts": texts, "top_k": self.top_k}
        response = self.session.post(
            self.api_base_url, json=payload, headers=headers, timeout=self.timeout
        )
        response.raise_for_status()
        data = response.json()
        result = {text: set(map(str, data.get(text, []))) for text in texts}
        logger.info(f"[Step 1] Extracted CUIs per text: { {k: len(v) for k,v in result.items()} }")
        return result

# ============================================================
# SUBNET API CLIENT WITH CACHING
# ============================================================
class SubnetAPIClient:
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, subnet_url: str):
        self.url = subnet_url.rstrip("/")
        self._cache_nodes: Dict[str, Set[str]] = {}
        self._cache_edges: Dict[str, List[Tuple[str, str]]] = {}

    def _get_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token
            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                universal_newlines=True,
            )
            token = result.stdout.strip()
            self._cached_token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }
            self._token_expiry = now + 3300
            return self._cached_token

    def get_subnet_batch(self, cuis: List[str]) -> Tuple[Set[str], List[Tuple[str, str]]]:
        uncached_cuis = [c for c in cuis if c not in self._cache_nodes]
        nodes, edges = set(), []

        if uncached_cuis:
            headers = self._get_token()
            payload = {"cuis": uncached_cuis, "cross_context": False}
            response = requests.post(f"{self.url}/subnet/", json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            api_nodes, api_edges = response.json().get("output", ([], []))
            api_nodes_set = set(api_nodes)
            nodes.update(api_nodes_set)
            edges.extend(api_edges)
            for cui in uncached_cuis:
                self._cache_nodes[cui] = api_nodes_set
                self._cache_edges[cui] = api_edges

        for cui in cuis:
            if cui in self._cache_nodes:
                nodes.update(self._cache_nodes[cui])
                edges.extend(self._cache_edges[cui])
        edges = list(set(edges))
        logger.info(f"[Step 2] Subnet nodes: {len(nodes)}, edges: {len(edges)}")
        return nodes, edges

# ============================================================
# BIGQUERY UTILITIES
# ============================================================
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    client = bigquery.Client(project=project_id)
    query = f"""
    SELECT DISTINCT CUI
    FROM `{project_id}.{dataset_id}.MRCONSO`
    WHERE CUI IN UNNEST(@cuis)
      AND SAB IN ('ICD10', 'ICD10CM', 'ICD9CM', 'SNOMEDCT_US', 'LOINC', 'RXNORM')
    """
    job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))])
    df = client.query(query, job_config=job_config).result().to_dataframe()
    logger.info(f"[Step 3] Allowed CUIs after filtering: {len(df)}")
    return df["CUI"].tolist()

# ============================================================
# GRAPH + IC
# ============================================================
def build_graph(nodes: Set[str], edges: List[Tuple[str, str]]) -> Dict[str, Dict]:
    graph = defaultdict(lambda: {"parents": set(), "children": set()})
    for parent, child in edges:
        if parent in nodes and child in nodes:
            graph[parent]["children"].add(child)
            graph[child]["parents"].add(parent)
    for n in nodes:
        graph.setdefault(n, {"parents": set(), "children": set()})
    logger.info(f"[Step 4] Built graph with {len(graph)} nodes")
    return dict(graph)

def compute_descendants(graph: Dict[str, Dict]) -> Dict[str, Set[str]]:
    cache = {}
    def dfs(node):
        if node in cache:
            return cache[node]
        visited = {node}
        for child in graph[node]["children"]:
            visited |= dfs(child)
        cache[node] = visited
        return visited
    for node in graph:
        dfs(node)
    logger.info(f"[Step 5] Computed descendants for {len(graph)} nodes")
    return cache

def compute_ic(descendants: Dict[str, Set[str]]) -> Dict[str, float]:
    total = len(descendants)
    ic = {cui: 1.0 - (np.log(len(desc)) / np.log(total)) for cui, desc in descendants.items()}
    logger.info(f"[Step 6] Computed IC scores for {len(ic)} CUIs")
    return ic

# ============================================================
# HIERARCHICAL REDUCTION
# ============================================================
def hierarchical_reduction(cuis: List[str], descendants: Dict[str, Set[str]], ic_scores: Dict[str, float]) -> Set[str]:
    remaining = set(cuis)
    covered = set()
    selected = set()
    while covered < remaining:
        best_cui, best_gain = None, 0
        for cui in remaining - selected:
            new_gain = sum(ic_scores.get(c, 0) for c in (descendants[cui] & remaining - covered))
            if new_gain > best_gain:
                best_cui, best_gain = cui, new_gain
        if not best_cui:
            selected |= remaining - covered
            break
        selected.add(best_cui)
        covered |= descendants[best_cui]
    logger.info(f"[Step 7] Selected {len(selected)} CUIs after hierarchical reduction")
    return selected

# ============================================================
# PIPELINE
# ============================================================
def reduce_cuis_pipeline(texts: List[str], project_id: str, dataset_id: str, cui_api_url: str, subnet_api_url: str) -> Dict[str, any]:
    start_time = time.time()
    
    extractor = CUIAPIClient(cui_api_url)
    text_to_cuis = extractor.extract_cuis_batch(texts)
    all_cuis = set().union(*text_to_cuis.values())
    logger.info(f"[Pipeline] Total extracted CUIs: {len(all_cuis)}")
    
    filtered_cuis = filter_allowed_cuis(all_cuis, project_id, dataset_id)
    if not filtered_cuis:
        return {"original_cuis": all_cuis, "reduced_cuis": set(), "statistics": {}}
    
    subnet = SubnetAPIClient(subnet_api_url)
    nodes, edges = subnet.get_subnet_batch(filtered_cuis)
    
    graph = build_graph(nodes, edges)
    descendants = compute_descendants(graph)
    ic_scores = compute_ic(descendants)
    
    final_cuis = hierarchical_reduction(filtered_cuis, descendants, ic_scores)
    elapsed = time.time() - start_time
    logger.info(f"[Pipeline] Reduced to {len(final_cuis)} CUIs in {elapsed:.2f}s")
    
    return {
        "original_cuis": all_cuis,
        "reduced_cuis": final_cuis,
        "statistics": {
            "original_count": len(all_cuis),
            "filtered_count": len(filtered_cuis),
            "final_count": len(final_cuis),
            "processing_time": elapsed
        }
    }

# ============================================================
# MAIN TEST
# ============================================================
def main():
    PROJECT_ID = "your_project_id"
    DATASET_ID = "your_dataset"
    CUI_API_URL = "https://your_cui_api"
    SUBNET_API_URL = "https://your_subnet_api"
    
    texts = [
        "Patient presents with brain tumor",
        "Severe chest pain and shortness of breath",
        "Type 2 diabetes mellitus diagnosis"
    ]
    
    results = reduce_cuis_pipeline(texts, PROJECT_ID, DATASET_ID, CUI_API_URL, SUBNET_API_URL)
    print("Original CUIs:", results["original_cuis"])
    print("Reduced CUIs:", results["reduced_cuis"])
    print("Statistics:", results["statistics"])

if __name__ == "__main__":
    main()
