import json
import re
import hashlib
from datetime import datetime
from typing import Dict, Any, List, Optional
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel


# Configuration
PROJECT_ID = "your-project-id"  # Replace with your project ID
MODEL_VERSION = "gemini-1.5-flash-002"  # Or your preferred model


QUERY_EXPANSION_PROMPT = """
You are an expert medical AI assistant specializing in clinical query expansion.

TASK: Expand the user's query into a comprehensive, detailed clinical description.

INSTRUCTIONS:
1. Expand ALL medical abbreviations to full terms (e.g., HTN → Hypertension, DM → Diabetes Mellitus, SOB → Shortness of Breath)
2. Clarify vague medical terms with specific clinical language
3. Add relevant medical context based on standard clinical practice
4. Identify implicit clinical concepts that should be explicit
5. DO NOT add assumptions beyond reasonable clinical interpretation
6. DO NOT include action verbs like "analyze", "review", "check" unless in original query
7. DO NOT hallucinate information not implied by the query
8. Maintain the original query's intent and scope
9. Make sure the temporal aspect is relevant to the query context

EXAMPLES:
- "Pt with DM" → "Patient with Diabetes Mellitus"
- "Check vitals" → "Vital signs measurement including blood pressure, heart rate, temperature, respiratory rate, oxygen saturation"
- "Family hx of heart disease" → "Cardiovascular disease in family including coronary artery disease, myocardial infarction, heart failure"
- "SOB on exertion" → "Shortness of breath on exertion"

Return ONLY valid JSON (no markdown, no explanation):
{{
  "expanded_query": "comprehensive expanded clinical description",
  "abbreviations_expanded": ["list of abbreviations that were expanded"],
  "concepts_added": ["list of medical concepts that were made explicit"]
}}

User Input: {query}
"""


INTENT_EXTRACTION_PROMPT = """
You are an expert clinical intent extraction engine specialized in medical information retrieval and taxonomic classification.

# PRIMARY DIRECTIVE: COMPREHENSIVE EXTRACTION

**CRITICAL**: Extract EVERY piece of clinical information from the expanded query. If the expanded query mentions it, you MUST create intents to capture it.

## EXTRACTION METHODOLOGY:

### Step 1: Analyze the Expanded Query
Read the expanded query carefully. It contains ALL the clinical concepts that need to be captured. Every element mentioned in the expanded query MUST be reflected in your intent extraction.

### Step 2: Intent Creation Principles

**CREATE SEPARATE INTENTS when the expanded query contains:**
1. **Multiple distinct information categories** that would be documented separately in medical records
2. **Different data collection mechanisms** (e.g., patient-reported vs measured vs prescribed)
3. **Different temporal contexts** (current state vs history vs plans)
4. **Different sources of information** (patient vs provider vs pharmacy vs laboratory)

**COMBINE INTO SINGLE INTENT when:**
1. Information would be collected together in a single clinical workflow
2. Elements are attributes of the same clinical entity
3. Data points are typically documented in the same section of medical records

### Step 3: Depth Requirements

For EACH concept mentioned in the expanded query, you MUST:
1. Create deep hierarchical classifications (minimum 4 levels)
2. Generate ALL relevant sub-categories
3. Include ALL attributes, characteristics, and related concepts
4. Decompose to atomic elements that map to specific CUIs

## CLINICAL QUERY VALIDATION:

FIRST: Verify this is a CLINICAL query. If the query is NOT related to healthcare, medical conditions, symptoms, treatments, or clinical information, return:
{{
  "is_clinical": false,
  "reason": "Query is not clinical in nature",
  "intents": []
}}

If the query IS clinical, proceed with COMPREHENSIVE intent extraction.

# INTENT STRUCTURE - EXHAUSTIVE TAXONOMY

## MANDATORY COVERAGE CHECKLIST:

Before finalizing, verify you've captured:
□ Every concept mentioned in the expanded query
□ All attributes of each concept (names, types, measurements, etc.)
□ All relationships between concepts
□ All temporal aspects mentioned
□ All sources and documentation needs
□ All clinical workflows implied

## INTENT STRUCTURE:

Each intent MUST have:

1. **intent_title**: Specific title capturing the intent's scope
2. **description**: Comprehensive explanation covering ALL aspects mentioned in the expanded query for this intent
3. **nature**: Primary characteristic of this information type
4. **sub_nature**: EXHAUSTIVE array covering EVERY dimension mentioned or implied
5. **final_queries**: Comprehensive queries capturing ALL possible CUI mappings

## SUB_NATURE GENERATION RULES:

### For EVERY concept in your intent, create sub_natures for:

1. **Identification/Naming** (if applicable):
   - Generic names, brand names, trade names
   - Common examples, specific instances
   - Synonyms, abbreviations, variations

2. **Quantitative Attributes** (if applicable):
   - Measurements, dosages, strengths
   - Units, scales, ranges
   - Frequencies, durations, intervals

3. **Qualitative Attributes** (if applicable):
   - Types, categories, classifications
   - Characteristics, properties, features
   - Severity, intensity, quality descriptors

4. **Temporal Dimensions** (if applicable):
   - Start/end dates, duration
   - Frequency patterns, schedules
   - Temporal relationships, sequences

5. **Contextual Information** (if applicable):
   - Sources, providers, locations
   - Indications, purposes, reasons
   - Methods, routes, techniques

6. **Management Aspects** (if applicable):
   - Compliance, adherence, effectiveness
   - Adjustments, modifications, discontinuations
   - Monitoring, follow-up, reviews

7. **Associated Factors** (if applicable):
   - Side effects, interactions, contraindications
   - Related conditions, complications
   - Risk factors, precautions

## HIERARCHICAL DEPTH REQUIREMENTS:

EVERY sub_nature MUST have AT LEAST 4 levels of hierarchy:
```
Level 1: Nature (broadest category)
Level 2: Major Domain >> 
Level 3: Specific Category >> 
Level 4: Subcategory >> 
Level 5: Atomic Elements (most specific)
```

Example for medications:
{{
  "category": "Drug Identification >> Naming Systems >> Brand Names >> Cardiovascular Brands",
  "elements": ["Lipitor", "Crestor", "Zocor", "Pravachol", "Norvasc", "Prinivil"],
  "entities": ["brand_name", "trade_name"]
}}

## FINAL QUERIES GENERATION - COMPREHENSIVE APPROACH:

### Generate queries for:

1. **Every specific item mentioned** in expanded query
2. **Every combination** of drug+dose, condition+severity, test+result
3. **Every attribute** mentioned (frequency, route, duration, etc.)
4. **Every relationship** between concepts
5. **Common variations** of each concept
6. **Standard clinical groupings** (drug classes, test panels, etc.)

### Query Generation Formula:

For medications (example):
- Drug name alone: "Metformin"
- Drug + each dosage: "Metformin 500mg", "Metformin 1000mg"
- Brand name alone: "Glucophage"
- Brand + dosage: "Glucophage 500mg"
- Drug class: "biguanides"
- Indication: "diabetes medication"
- Frequency patterns: "twice daily metformin"
- Routes: "oral metformin"
- Combinations: "metformin extended release"

Apply similar comprehensive generation for ALL clinical concepts.

## VALIDATION BEFORE RETURNING:

### Coverage Validation:
1. Re-read the expanded query
2. Check EVERY concept mentioned is captured in intents
3. Verify each intent has exhaustive sub_natures
4. Ensure final_queries cover all atomic elements
5. Confirm no unnecessary duplication between intents

### Depth Validation:
- Each intent has minimum 5 sub_natures (unless very simple concept)
- Each sub_nature has 4+ levels of hierarchy
- Final queries comprehensively cover the intent scope
- Number of queries reflects the true complexity (typically 30-100+ per complex intent)

## EXAMPLES OF COMPREHENSIVE EXTRACTION:

### Example: "current medication"
Expanded query mentions: "prescription drugs, over-the-counter medications, supplements, dosages, frequencies, routes"

You MUST create sub_natures for:
- All drug identification methods (generic, brand, chemical names)
- All specific common drugs (top 20-30 medications)
- All dosage strengths mentioned in practice
- All frequencies and schedules
- All routes of administration
- All formulations and preparations
- All drug classes and categories
- All management aspects (adherence, prescribers, dates)

This should generate 10-15 sub_natures with 50-100+ final queries.

### Example: "family history"
Expanded query mentions: "hereditary conditions, family members, genetic disorders"

You MUST create intents capturing:
- All family relationships (parents, grandparents, siblings, children, extended family)
- All common hereditary conditions
- All genetic factors and risks
- All ages of onset
- All outcomes and complications
- All documentation sources

## CRITICAL REMINDERS:

1. **The expanded query is your CONTRACT** - if it mentions something, you MUST extract it
2. **More is better than less** - comprehensive extraction prevents missing clinical data
3. **Depth over breadth** - fewer intents with deep taxonomy is better than many shallow intents
4. **No concept left behind** - every clinical element needs a home in your taxonomy
5. **Real clinical terms only** - use actual medical terminology that maps to CUIs

Return ONLY valid JSON (no markdown, no explanation):
{{
  "is_clinical": true,
  "reason": "",
  "original_query": "{original_query}",
  "expanded_query": "{expanded_query}",
  "total_intents_detected": number,
  "intents": [
    {{
      "intent_title": "string",
      "description": "comprehensive description covering ALL aspects from expanded query",
      "nature": "string",
      "sub_nature": [
        // MINIMUM 5 sub_natures for simple concepts
        // TYPICALLY 10-15 for complex concepts
        // Each with 4+ levels of hierarchy
        {{
          "category": "Level2 >> Level3 >> Level4 >> Level5",
          "elements": ["exhaustive list of atomic elements"],
          "entities": ["entity types"]
        }}
      ],
      "final_queries": [
        // MINIMUM 20 queries for simple intents
        // TYPICALLY 50-100+ for complex intents
        // Cover EVERY combination mentioned in expanded query
        "specific query covering atomic element",
        "..."
      ]
    }}
  ]
}}

User Input: {expanded_query}
Original Query: {original_query}
Timestamp: {timestamp}
"""


class ContextualIntentPipeline:    
    def __init__(self, project: str, location: str = "us-central1", 
                 model: str = MODEL_VERSION, use_cache: bool = True):
        """
        Initialize the pipeline with enhanced consistency features.
        
        Args:
            project: GCP project ID
            location: GCP location
            model: Model name/version
            use_cache: Whether to use caching for identical queries
        """
        self.project = project
        self.location = location
        self.model_name = model
        self.use_cache = use_cache
        self.cache = {}
        
        # Initialize AI Platform
        aiplatform.init(project=project, location=location)
    
    def _get_fresh_model(self) -> GenerativeModel:
        """Create fresh model instance to avoid context contamination"""
        return GenerativeModel(self.model_name)
    
    def _get_cache_key(self, content: str, step: str) -> str:
        """Generate deterministic cache key for a given input and step"""
        return hashlib.md5(f"{step}:{content}".encode()).hexdigest()
    
    def _call_model(self, prompt: str, temperature: float = 0.0, 
                   max_tokens: int = 8096) -> str:
        """
        Call the model with deterministic settings for consistency.
        
        Args:
            prompt: The prompt to send to the model
            temperature: Temperature setting (0.0 for determinism)
            max_tokens: Maximum output tokens
            
        Returns:
            Model response as string
        """
        try:
            # Use fresh model instance to avoid context contamination
            model = self._get_fresh_model()
            
            # Deterministic generation config
            response = model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.0,  # Critical for consistency
                    "max_output_tokens": max_tokens,
                    "top_p": 0.95,      # Tightened for consistency
                    "top_k": 1,         # Maximum determinism
                    "candidate_count": 1,  # Single candidate
                    "seed": 42          # Fixed seed
                }
            )
            return response.text.strip()
        except Exception as e:
            print(f"Error calling model: {str(e)}")
            return "{}"
    
    def _safe_json(self, text: str) -> Dict[str, Any]:
        """
        Safely parse JSON from model response.
        
        Args:
            text: Raw text response from model
            
        Returns:
            Parsed JSON as dictionary
        """
        # Remove markdown code blocks if present
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'```\s*', '', text)
        text = text.strip()
        
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            # Try to find JSON object
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(0))
                except:
                    pass
            
            # Try to find JSON array
            match = re.search(r'\[.*\]', text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(0))
                except:
                    pass
            
            print(f"Failed to parse JSON from response: {text[:200]}...")
            return {}
    
    def _validate_json_structure(self, data: Dict, required_keys: List[str]) -> bool:
        """
        Validate that JSON has required keys.
        
        Args:
            data: JSON data to validate
            required_keys: List of required keys
            
        Returns:
            True if all required keys present, False otherwise
        """
        return all(key in data for key in required_keys)
    
    def stabilize_output(self, result: Dict) -> Dict:
        """
        Apply basic stabilization to ensure consistent output ordering.
        
        Args:
            result: Raw result dictionary
            
        Returns:
            Stabilized result dictionary
        """
        # Sort intents for consistent ordering
        if "intents" in result and result["intents"]:
            result["intents"] = sorted(
                result["intents"], 
                key=lambda x: x.get("intent_title", "")
            )
            
            # Sort and deduplicate queries within each intent
            for intent in result["intents"]:
                if "final_queries" in intent and intent["final_queries"]:
                    intent["final_queries"] = sorted(
                        list(set(intent["final_queries"]))
                    )
                
                # Sort sub_nature elements for consistency
                if "sub_nature" in intent:
                    for sub in intent["sub_nature"]:
                        if "elements" in sub and sub["elements"]:
                            sub["elements"] = sorted(list(set(sub["elements"])))
                        if "entities" in sub and sub["entities"]:
                            sub["entities"] = sorted(list(set(sub["entities"])))
        
        return result
    
    def expand_query(self, query: str) -> Dict[str, Any]:
        """
        Expand medical abbreviations and clarify clinical terms.
        
        Args:
            query: Original user query
            
        Returns:
            Dictionary with expanded query and metadata
        """
        # Check cache first
        if self.use_cache:
            cache_key = self._get_cache_key(query, "expand")
            if cache_key in self.cache:
                return self.cache[cache_key]
        
        prompt = QUERY_EXPANSION_PROMPT.format(query=query)
        raw_response = self._call_model(prompt)
        data = self._safe_json(raw_response)
        
        if not self._validate_json_structure(data, ["expanded_query"]):
            # Fallback to original query if expansion fails
            result = {
                "expanded_query": query,
                "abbreviations_expanded": [],
                "concepts_added": []
            }
        else:
            result = {
                "expanded_query": data.get("expanded_query", query),
                "abbreviations_expanded": data.get("abbreviations_expanded", []),
                "concepts_added": data.get("concepts_added", [])
            }
        
        # Cache the result
        if self.use_cache:
            self.cache[cache_key] = result
        
        return result
    
    def extract_intents(self, original_query: str, expanded_query: str) -> Dict[str, Any]:
        """
        Extract clinical intents with hierarchical taxonomy.
        
        Args:
            original_query: Original user query
            expanded_query: Expanded clinical query
            
        Returns:
            Dictionary with extracted intents and metadata
        """
        # Check cache first
        if self.use_cache:
            cache_key = self._get_cache_key(f"{original_query}|{expanded_query}", "intents")
            if cache_key in self.cache:
                return self.cache[cache_key]
        
        prompt = INTENT_EXTRACTION_PROMPT.format(
            original_query=original_query,
            expanded_query=expanded_query,
            timestamp=datetime.utcnow().isoformat()
        )
        
        # Use higher token limit for complex queries
        raw_response = self._call_model(prompt, max_tokens=8096)
        data = self._safe_json(raw_response)
        
        # Check if query was rejected as non-clinical
        if not data.get("is_clinical", True):
            result = {
                "intents": [],
                "total_intents_detected": 0,
                "is_clinical": False,
                "rejected_reason": data.get("reason", "Query is not clinical"),
                "original_query": original_query,
                "expanded_query": expanded_query
            }
        elif not self._validate_json_structure(data, ["intents"]):
            result = {
                "intents": [],
                "total_intents_detected": 0,
                "error": "Failed to extract intents"
            }
        else:
            # Validate that all intents have final_queries
            intents = data.get("intents", [])
            validated_intents = []
            
            for idx, intent in enumerate(intents):
                if "final_queries" not in intent or not intent["final_queries"]:
                    print(f"  Warning: Intent {idx + 1} '{intent.get('intent_title', 'Unknown')}' "
                          f"has no final_queries. Skipping.")
                    continue
                
                # Validate depth (should have minimum 5 sub_natures for comprehensive extraction)
                if len(intent.get("sub_nature", [])) < 5:
                    print(f"  Note: Intent {idx + 1} has only {len(intent.get('sub_nature', []))} "
                          f"sub_natures (expected 5+)")
                
                validated_intents.append(intent)
            
            result = {
                "intents": validated_intents,
                "total_intents_detected": data.get("total_intents_detected", len(validated_intents)),
                "is_clinical": True,
                "original_query": data.get("original_query", original_query),
                "expanded_query": data.get("expanded_query", expanded_query)
            }
        
        # Cache the result
        if self.use_cache:
            self.cache[cache_key] = result
        
        return result
    
    def run(self, query: str, verbose: bool = False, max_retries: int = 3) -> Dict[str, Any]:
        """
        Run the complete pipeline with retry logic for consistency.
        
        Args:
            query: User query to process
            verbose: Whether to print detailed progress
            max_retries: Maximum number of retries for consistency
            
        Returns:
            Dictionary with complete pipeline results
        """
        start_time = datetime.utcnow()
        
        if verbose:
            print(f"Original Query: {query}")
        
        # Try multiple times to get consistent results
        results = []
        
        for attempt in range(max_retries):
            if verbose and attempt > 0:
                print(f"  Attempt {attempt + 1}/{max_retries} for consistency...")
            
            # Step 1: Query Expansion
            expansion_result = self.expand_query(query)
            expanded_query = expansion_result["expanded_query"]
            
            if verbose and attempt == 0:
                print(f"Expanded Query: {expanded_query}")
                if expansion_result.get("abbreviations_expanded"):
                    print(f"  Abbreviations: {', '.join(expansion_result['abbreviations_expanded'])}")
                if expansion_result.get("concepts_added"):
                    print(f"  Concepts Added: {', '.join(expansion_result['concepts_added'][:5])}...")
            
            # Step 2: Intent Extraction
            intent_result = self.extract_intents(query, expanded_query)
            
            # Stabilize the output
            intent_result = self.stabilize_output(intent_result)
            
            # Store result
            results.append({
                "intent_count": len(intent_result.get("intents", [])),
                "result": intent_result,
                "expansion": expansion_result
            })
            
            # Check for consensus (if we get same intent count twice, use that)
            if attempt > 0:
                counts = [r["intent_count"] for r in results]
                if counts.count(counts[-1]) >= 2:
                    # Found consensus
                    intent_result = results[-1]["result"]
                    expansion_result = results[-1]["expansion"]
                    break
        
        # If no consensus after retries, use most common intent count
        if len(set(r["intent_count"] for r in results)) > 1 and len(results) == max_retries:
            from collections import Counter
            count_freq = Counter(r["intent_count"] for r in results)
            most_common_count = count_freq.most_common(1)[0][0]
            
            # Use the first result with the most common count
            for r in results:
                if r["intent_count"] == most_common_count:
                    intent_result = r["result"]
                    expansion_result = r["expansion"]
                    break
            
            if verbose:
                print(f"  Note: Used consensus from {max_retries} attempts "
                      f"(intent count: {most_common_count})")
        
        # Check if non-clinical
        if not intent_result.get("is_clinical", True):
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            if verbose:
                print(f"Status: NON-CLINICAL (rejected)")
                print(f"Reason: {intent_result.get('rejected_reason', 'Not clinical')}")
                print(f"Processing Time: {processing_time:.2f}s\n")
            
            return {
                "original_query": query,
                "expanded_query": expanded_query,
                "intents": [],
                "is_clinical": False,
                "rejected_reason": intent_result.get("rejected_reason", "Query is not clinical"),
                "timestamp": datetime.utcnow().isoformat(),
                "processing_time_seconds": processing_time
            }
        
        # Process clinical results
        intents = intent_result.get("intents", [])
        
        # Calculate statistics
        total_queries = sum(len(intent.get('final_queries', [])) for intent in intents)
        total_sub_natures = sum(len(intent.get('sub_nature', [])) for intent in intents)
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        if verbose:
            print(f"\nStatus: CLINICAL")
            print(f"Intents: {len(intents)} | Sub-natures: {total_sub_natures} | "
                  f"Queries: {total_queries}")
            print(f"Processing Time: {processing_time:.2f}s")
            
            # Print detailed intent information
            print(f"\nIntent Details:")
            for idx, intent in enumerate(intents, 1):
                print(f"\nIntent {idx}: {intent.get('intent_title', 'Unknown')}")
                print(f"  Nature: {intent.get('nature', 'Unknown')}")
                print(f"  Sub-natures: {len(intent.get('sub_nature', []))}")
                print(f"  Final queries: {len(intent.get('final_queries', []))}")
                
                # Show sample queries
                queries = intent.get('final_queries', [])
                if queries:
                    print(f"  Sample queries: {', '.join(queries[:5])}...")
        
        return {
            "original_query": query,
            "expanded_query": expanded_query,
            "abbreviations_expanded": expansion_result.get("abbreviations_expanded", []),
            "concepts_added": expansion_result.get("concepts_added", []),
            "intents": intents,
            "is_clinical": True,
            "statistics": {
                "total_intents": len(intents),
                "total_sub_natures": total_sub_natures,
                "total_queries": total_queries,
                "avg_queries_per_intent": total_queries / len(intents) if intents else 0,
                "avg_sub_natures_per_intent": total_sub_natures / len(intents) if intents else 0
            },
            "timestamp": datetime.utcnow().isoformat(),
            "processing_time_seconds": processing_time
        }
    
    def clear_cache(self):
        """Clear the query cache"""
        self.cache = {}
        print("Cache cleared")
    
    def get_cache_size(self):
        """Get current cache size"""
        return len(self.cache)


# Main execution
if __name__ == "__main__":
    # Initialize pipeline
    pipeline = ContextualIntentPipeline(
        project=PROJECT_ID,
        location="us-central1",
        model=MODEL_VERSION,
        use_cache=True  # Enable caching for consistency
    )
    
    # Test queries - including simple and complex examples
    test_queries = [
        # Simple query to test comprehensive extraction
        "current medication",
        
        # Complex symptom description
        "I've been experiencing severe, sharp pain in my lower right abdomen since yesterday evening. "
        "It started as a dull ache but has gotten much worse and is now constant, rating around an 8/10. "
        "It hurts more when I move or cough. I've also felt nauseous and had a low-grade fever "
        "(around 100°F) this morning. I'm quite worried it might be appendicitis or something similar, "
        "as I've never had pain like this before. It's making it very difficult to sleep or focus.",
        
        # Abbreviation-heavy query
        "Pt with DM type 2, HTN, and family hx of CAD, currently on metformin and lisinopril",
        
        # Lab work query
        "Need CBC, CMP, lipid panel, HbA1c for annual physical",
        
        # Family history
        "My father had diabetes and heart disease, mother had breast cancer"
    ]
    
    # Process each query
    all_results = []
    
    for idx, query in enumerate(test_queries, 1):
        print(f"\n{'='*80}")
        print(f"Query {idx}/{len(test_queries)}")
        print(f"{'='*80}")
        
        # Run pipeline with verbose output
        result = pipeline.run(query, verbose=True, max_retries=3)
        all_results.append(result)
        
        # Save individual result to file
        output_filename = f"query_result_{idx}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nResults saved to: {output_filename}")
        
        # Print summary
        if result.get("is_clinical", False):
            stats = result.get("statistics", {})
            print(f"\nSummary:")
            print(f"  - Clinical query: Yes")
            print(f"  - Intents detected: {stats.get('total_intents', 0)}")
            print(f"  - Total sub-natures: {stats.get('total_sub_natures', 0)}")
            print(f"  - Total CUI queries: {stats.get('total_queries', 0)}")
            print(f"  - Avg queries/intent: {stats.get('avg_queries_per_intent', 0):.1f}")
            print(f"  - Avg sub-natures/intent: {stats.get('avg_sub_natures_per_intent', 0):.1f}")
        else:
            print(f"\nSummary:")
            print(f"  - Clinical query: No")
            print(f"  - Reason: {result.get('rejected_reason', 'Not specified')}")
    
    # Save all results to a single file
    all_results_filename = f"all_query_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(all_results_filename, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*80}")
    print(f"Pipeline execution completed")
    print(f"All results saved to: {all_results_filename}")
    print(f"Cache size: {pipeline.get_cache_size()} entries")
    print(f"{'='*80}")
