"""
MODULE 1: Two-Level CUI Reduction - SIMPLE & ACCURATE

Pipeline:
  Text → Extract → SAB Filter → Level 1 (Extremes) → Level 2 (Diversity) → Done

Strategy:
- Level 1: Pick EXTREMES (broad + specific) for coverage
- Level 2: Pick DIVERSE (furthest apart) for search matching
- Simple, clean, effective for query-document matching

~300 lines
"""

import time
import subprocess
import threading
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass
from collections import OrderedDict
import numpy as np
import networkx as nx
from concurrent.futures import ThreadPoolExecutor, as_completed
from sklearn.cluster import AgglomerativeClustering
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from google.cloud import bigquery

# Configuration
BQ_BATCH_SIZE = 2000
ALLOWED_SABS = ['ICD10CM', 'ICD10PCS', 'ICD9CM', 'SNOMEDCT_US', 'LOINC']

print_lock = threading.Lock()

def print_safe(msg: str):
    with print_lock:
        print(msg, flush=True)

# ══════════════════════════════════════════════════════════════════
# SIMPLE CACHE
# ══════════════════════════════════════════════════════════════════

class SimpleCache:
    """Simple LRU cache - good enough"""
    def __init__(self, maxsize: int = 10000):
        self.cache = OrderedDict()
        self.maxsize = maxsize
    
    def get(self, key):
        if key in self.cache:
            self.cache.move_to_end(key)
            return self.cache[key]
        return None
    
    def put(self, key, value):
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            if len(self.cache) >= self.maxsize:
                self.cache.popitem(last=False)
        self.cache[key] = value

# ══════════════════════════════════════════════════════════════════
# HIERARCHY CLIENT
# ══════════════════════════════════════════════════════════════════

class HierarchyClient:
    """Simple hierarchy client with caching"""
    
    def __init__(self, network_obj):
        self.network = network_obj
        self.cache = SimpleCache()
    
    def get_children(self, cui: str) -> List[str]:
        if not self.network.has_node(cui):
            return []
        return list(self.network.successors(cui))
    
    def get_ancestors(self, cui: str, max_depth: int = 10) -> List[List[str]]:
        """Get ancestor paths (cached)"""
        key = (cui, max_depth)
        cached = self.cache.get(key)
        if cached:
            return cached
        
        if not self.network.has_node(cui):
            return []
        
        # Simple BFS
        queue = [(0, [cui])]
        visited = set()
        paths = []
        
        while queue:
            depth, path = queue.pop(0)
            node = path[-1]
            
            if node in visited or depth >= max_depth:
                paths.append(path)
                continue
            
            visited.add(node)
            parents = list(self.network.predecessors(node)) if self.network.has_node(node) else []
            
            if not parents:
                paths.append(path)
            else:
                for parent in parents:
                    if parent not in path:
                        queue.append((depth + 1, path + [parent]))
        
        self.cache.put(key, paths)
        return paths
    
    def get_ic_score(self, cui: str) -> float:
        """Estimate IC from depth"""
        paths = self.get_ancestors(cui)
        if not paths:
            return 3.0
        
        avg_depth = sum(len(p) for p in paths) / len(paths)
        return min(10.0, 2.0 + np.log1p(avg_depth) * 2.0)

# ══════════════════════════════════════════════════════════════════
# LEVEL 1: HIERARCHY REDUCTION
# ══════════════════════════════════════════════════════════════════

class HierarchyReducer:
    """Simple hierarchy-based reduction"""
    
    def __init__(self, hierarchy: HierarchyClient):
        self.h = hierarchy
    
    def reduce(self, cuis: List[str]) -> List[str]:
        """Reduce CUIs using hierarchy"""
        if not cuis:
            return []
        
        # Cluster by first ancestor
        clusters = {}
        for cui in set(cuis):
            paths = self.h.get_ancestors(cui, max_depth=5)
            ancestor = paths[0][0] if paths else cui
            
            if ancestor not in clusters:
                clusters[ancestor] = set()
            clusters[ancestor].add(cui)
        
        # Pick EXTREMES per cluster (broad + specific for search coverage)
        reduced = []
        for group in clusters.values():
            if len(group) <= 2:
                reduced.extend(group)
            else:
                # Score by IC
                scored = [(cui, self.h.get_ic_score(cui)) for cui in group]
                scored.sort(key=lambda x: x[1])
                
                # Pick min (broad) and max (specific)
                reduced.append(scored[0][0])   # Most broad
                reduced.append(scored[-1][0])  # Most specific
        
        return list(set(reduced))

# ══════════════════════════════════════════════════════════════════
# LEVEL 2: EMBEDDING REDUCTION
# ══════════════════════════════════════════════════════════════════

class EmbeddingReducer:
    """Simple embedding-based clustering"""
    
    def __init__(self, project_id: str, dataset_id: str, table: str):
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.table = table
        self.client = bigquery.Client(project=project_id)
    
    def fetch_embeddings(self, cuis: List[str]) -> Dict[str, np.ndarray]:
        """Fetch embeddings from BigQuery"""
        embeddings = {}
        
        for i in range(0, len(cuis), BQ_BATCH_SIZE):
            batch = cuis[i:i + BQ_BATCH_SIZE]
            
            query = f"""
            SELECT CUI, embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.table}`
            WHERE CUI IN UNNEST(@cuis)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", batch)]
            )
            
            try:
                results = self.client.query(query, job_config=job_config).result(timeout=60)
                for row in results:
                    embeddings[row.CUI] = np.array(row.embedding, dtype=np.float32)
            except Exception as e:
                print_safe(f"Embedding fetch failed: {e}")
        
        return embeddings
    
    def reduce(self, cuis: List[str]) -> List[str]:
        """Cluster by embeddings, pick DIVERSE representatives for search coverage"""
        if not cuis or len(cuis) == 1:
            return cuis
        
        # Get embeddings
        embeddings = self.fetch_embeddings(cuis)
        valid_cuis = [c for c in cuis if c in embeddings]
        
        if len(valid_cuis) <= 1:
            return cuis
        
        # Stack embeddings
        X = np.stack([embeddings[c] for c in valid_cuis])
        
        # Cluster (adaptive √n)
        n_clusters = max(1, int(len(valid_cuis) ** 0.5))
        clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='cosine', linkage='average')
        labels = clustering.fit_predict(X)
        
        # Pick DIVERSE per cluster (iterative furthest point)
        reduced = []
        for label in set(labels):
            mask = labels == label
            cluster_cuis = [valid_cuis[i] for i in range(len(valid_cuis)) if mask[i]]
            cluster_vecs = X[mask]
            
            if len(cluster_cuis) == 1:
                reduced.append(cluster_cuis[0])
            else:
                # Pick diverse using iterative furthest point
                n_select = max(1, int(len(cluster_cuis) ** 0.5))
                selected_indices = [0]  # Start with first
                
                for _ in range(min(n_select - 1, len(cluster_cuis) - 1)):
                    max_min_dist = -1
                    best_idx = -1
                    
                    for i in range(len(cluster_cuis)):
                        if i in selected_indices:
                            continue
                        
                        # Min distance to any selected
                        min_dist = min(np.linalg.norm(cluster_vecs[i] - cluster_vecs[j]) 
                                     for j in selected_indices)
                        
                        if min_dist > max_min_dist:
                            max_min_dist = min_dist
                            best_idx = i
                    
                    if best_idx != -1:
                        selected_indices.append(best_idx)
                
                reduced.extend([cluster_cuis[i] for i in selected_indices])
        
        return reduced

# ══════════════════════════════════════════════════════════════════
# CUI EXTRACTOR
# ══════════════════════════════════════════════════════════════════

class CUIExtractor:
    """Simple CUI extraction with token refresh"""
    
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))
        
        self.token_expiry = 0
        self._refresh_token()
    
    def _refresh_token(self):
        proc = subprocess.run(["gcloud", "auth", "print-identity-token"], 
                            stdout=subprocess.PIPE, text=True, timeout=10)
        self.headers = {
            "Authorization": f"Bearer {proc.stdout.strip()}",
            "Content-Type": "application/json"
        }
        self.token_expiry = time.time() + 3300  # 55 minutes
    
    def extract(self, text: str) -> List[str]:
        """Extract CUIs for text"""
        # Refresh token if expired
        if time.time() >= self.token_expiry:
            self._refresh_token()
        
        try:
            resp = self.session.post(
                self.api_url,
                json={"query_texts": [text], "top_k": 3},
                headers=self.headers,
                timeout=30
            )
            resp.raise_for_status()
            
            cuis = []
            for v in resp.json().values():
                if isinstance(v, list):
                    cuis.extend(v)
            
            return list(set(str(c) for c in cuis))
        except Exception as e:
            print_safe(f"Extraction failed: {e}")
            return []

# ══════════════════════════════════════════════════════════════════
# SAB FILTER
# ══════════════════════════════════════════════════════════════════

def filter_sab(cuis: List[str], project_id: str, dataset_id: str) -> List[str]:
    """Filter by SAB"""
    if not cuis:
        return []
    
    client = bigquery.Client(project=project_id)
    filtered = []
    
    for i in range(0, len(cuis), BQ_BATCH_SIZE):
        batch = cuis[i:i + BQ_BATCH_SIZE]
        
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis) AND SAB IN UNNEST(@sabs)
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch),
                bigquery.ArrayQueryParameter("sabs", "STRING", ALLOWED_SABS)
            ]
        )
        
        results = client.query(query, job_config=job_config).result(timeout=60)
        filtered.extend([row.CUI for row in results])
    
    return filtered

# ══════════════════════════════════════════════════════════════════
# RESULT STRUCTURE
# ══════════════════════════════════════════════════════════════════

@dataclass
class Result:
    text_id: int
    text: str
    extracted: List[str]
    filtered: List[str]
    level1: List[str]
    level2: List[str]
    reduction_pct: float
    time: float

# ══════════════════════════════════════════════════════════════════
# PIPELINE
# ══════════════════════════════════════════════════════════════════

def process_text(
    text_id: int,
    text: str,
    extractor: CUIExtractor,
    hierarchy_reducer: HierarchyReducer,
    embedding_reducer: EmbeddingReducer,
    project_id: str,
    dataset_id: str
) -> Result:
    """Process single text"""
    start = time.time()
    
    print_safe(f"[{text_id}] Processing: {text[:50]}...")
    
    # Extract
    extracted = extractor.extract(text)
    print_safe(f"[{text_id}] Extracted: {len(extracted)}")
    
    # SAB filter
    filtered = filter_sab(extracted, project_id, dataset_id)
    print_safe(f"[{text_id}] Filtered: {len(filtered)}")
    
    # Level 1: Hierarchy
    level1 = hierarchy_reducer.reduce(filtered)
    print_safe(f"[{text_id}] Level 1: {len(filtered)} → {len(level1)}")
    
    # Level 2: Embeddings
    level2 = embedding_reducer.reduce(level1)
    print_safe(f"[{text_id}] Level 2: {len(level1)} → {len(level2)}")
    
    reduction = (1 - len(level2) / len(filtered)) * 100 if filtered else 0
    elapsed = time.time() - start
    
    print_safe(f"[{text_id}] ✓ Done: {reduction:.1f}% reduction in {elapsed:.1f}s")
    
    return Result(text_id, text, extracted, filtered, level1, level2, reduction, elapsed)


def run_pipeline(
    texts: List[str],
    network_path: str,
    api_url: str,
    project_id: str,
    dataset_id: str,
    embedding_table: str,
    workers: int = 5
) -> List[Result]:
    """
    Simple two-level reduction pipeline
    
    Args:
        texts: Clinical texts
        network_path: Path to UMLS NetworkX pickle
        api_url: CUI extraction API
        project_id: GCP project
        dataset_id: BigQuery dataset
        embedding_table: Embeddings table
        workers: Parallel workers
    
    Returns:
        List of Results
    """
    print_safe("="*60)
    print_safe(f"MODULE 1: {len(texts)} texts, {workers} workers")
    print_safe("="*60)
    
    # Load network
    import pickle
    with open(network_path, "rb") as f:
        network = pickle.load(f)
    print_safe(f"Loaded: {network.number_of_nodes()} nodes\n")
    
    # Initialize
    hierarchy = HierarchyClient(network)
    hierarchy_reducer = HierarchyReducer(hierarchy)
    embedding_reducer = EmbeddingReducer(project_id, dataset_id, embedding_table)
    extractor = CUIExtractor(api_url)
    
    # Process in parallel
    results = []
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = [
            executor.submit(process_text, i+1, text, extractor, 
                          hierarchy_reducer, embedding_reducer, project_id, dataset_id)
            for i, text in enumerate(texts)
        ]
        
        for future in as_completed(futures):
            try:
                results.append(future.result())
            except Exception as e:
                print_safe(f"Failed: {e}")
    
    # Sort and summarize
    results.sort(key=lambda r: r.text_id)
    
    total_filtered = sum(len(r.filtered) for r in results)
    total_final = sum(len(r.level2) for r in results)
    overall = (1 - total_final / total_filtered) * 100 if total_filtered else 0
    
    print_safe("\n" + "="*60)
    print_safe("SUMMARY")
    print_safe("="*60)
    print_safe(f"Texts: {len(results)}")
    print_safe(f"Total CUIs: {total_filtered} → {total_final}")
    print_safe(f"Reduction: {overall:.1f}%")
    print_safe("="*60)
    
    return results


# ══════════════════════════════════════════════════════════════════
# USAGE
# ══════════════════════════════════════════════════════════════════

if __name__ == "__main__":
    
    texts = [
        "patient has ankle pain",
        "difficulty walking",
        "chronic headache"
    ]
    
    results = run_pipeline(
        texts=texts,
        network_path="/path/to/umls_network.pkl",
        api_url="https://your-api.com/extract",
        project_id="your-project",
        dataset_id="your-dataset",
        embedding_table="cui_embeddings",
        workers=3
    )
    
    # Use results
    for r in results:
        print(f"\n{r.text}")
        print(f"  Final: {r.level2[:5]}")
        print(f"  Reduction: {r.reduction_pct:.1f}%")
