#load graph pikcle file

import pickle
import time
import os

# Path to your PKL
NETWORK_PKL = "/home/jupyter/CUIreduction/networkx_cui_context_v1_1_0.pkl"


# Check file exists
assert os.path.exists(NETWORK_PKL), f"PKL not found: {NETWORK_PKL}"

# Load PKL and measure time
start_time = time.time()
with open(NETWORK_PKL, "rb") as f:
    UMLS_NETWORK_OBJ = pickle.load(f)
print(f"PKL loaded in {time.time() - start_time:.2f} seconds")

# Optional: inspect type
print("Loaded object type:", type(UMLS_NETWORK_OBJ))

#left hand side

import asyncio
import aiohttp
import subprocess
import re
import logging
from pathlib import Path
import json
from typing import List, Dict, Set, Optional, Tuple
from collections import defaultdict
import time
from dataclasses import dataclass, asdict
import threading
import pickle
import numpy as np
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import nest_asyncio
import requests
from requests.adapters import HTTPAdapter, Retry

nest_asyncio.apply()

# --------------------- Logging ---------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --------------------- GCP Token ---------------------
class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force: bool = False) -> Dict[str, str]:
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token

            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)

            token = proc.stdout.strip()
            if not token:
                raise RuntimeError("Empty token from gcloud")

            cls._token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            cls._expiry = now + 3300  # ~55 minutes
            return cls._token

# --------------------- Data Classes ---------------------
@dataclass
class ReductionStats:
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    group_stats: Dict = None
    missing_embeddings_count: int = 0

    def to_dict(self):
        return asdict(self)

# --------------------- Filter CUIs ---------------------
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    try:
        logger.info(f"Filtering {len(cuis)} CUIs to retain only ICD, SNOMED, and LOINC...")
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('CCSR_ICD10CM','CCSR_ICD10PCS','DMDICD10','ICD10','ICD10AE','ICD10AM','ICD10AMAE',
                      'ICD10CM','ICD10DUT','ICD10PCS','ICD9CM','ICPC2ICD10DUT','ICPC2ICD10ENG',
                      'SNOMEDCT_US', 'LOINC')
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
        )
        df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        allowed_cuis = df['CUI'].tolist()
        logger.info(f"{len(allowed_cuis)} CUIs after filter")
        return allowed_cuis
    except Exception as e:
        logger.error(f"Failed to filter CUIs by SAB: {str(e)}")
        return []

# --------------------- CUI API Client ---------------------
class CUIAPIClient:
    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3, max_retries: int = 3):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.max_retries = max_retries

        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        if not texts:
            return set()
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = GCPTokenProvider.get_headers()
        logger.info(f"Extracting CUIs from {len(texts)} text(s)...")
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            if response.status_code == 401 and retry_auth:
                logger.warning("Token expired, refreshing...")
                GCPTokenProvider.get_headers(force=True)
                return self.extract_cuis_batch(texts, retry_auth=False)
            response.raise_for_status()
            data = response.json()
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
            return all_cuis
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return set()

# --------------------- Enhanced Reducer ---------------------
class EnhancedCUIReducer:
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        cui_embeddings_table: str,
        mrsty_table: str,
        umls_network_obj: dict
    ):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.cui_embeddings_table = cui_embeddings_table
        self.mrsty_table = mrsty_table
        self._preloaded_network = umls_network_obj

        self._hierarchy_cache = {}
        self._ic_scores_cache = {}
        self._description_cache = {}
        self._missing_embeddings_total = 0

    # --------------------- Main Reduction Function ---------------------
    def reduce(
        self,
        input_cuis: List[str],
        ic_percentile: int = 75,
        similarity_threshold: float = 0.75,
        distance_from_centroid_threshold: float = 0.15
    ) -> Tuple[List[str], ReductionStats]:

        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        logger.info(f"Starting semantic-group based reduction for {initial_count} CUIs")

        # --------------------- Semantic type grouping ---------------------
        semantic_groups = self._group_by_semantic_type_dynamic(input_cuis)
        logger.info(f"Created {len(semantic_groups)} semantic groups")

        all_reduced_cuis = []
        group_stats = {}
        total_after_ic = 0

        for group_name, group_cuis in semantic_groups.items():
            if not group_cuis:
                continue

            # --------------------- Build hierarchy from pickle ---------------------
            hierarchy = self._build_hierarchy_depthwise(group_cuis)

            # --------------------- IC scores ---------------------
            ic_scores = self._compute_ic_scores_within_group(hierarchy, group_cuis, group_name)
            ic_threshold = np.percentile(list(ic_scores.values()), ic_percentile) if ic_scores else 0.0
            high_ic_cuis = [cui for cui in group_cuis if ic_scores.get(cui, 0) >= ic_threshold]
            total_after_ic += len(high_ic_cuis)

            # --------------------- Embedding-based clustering ---------------------
            if len(high_ic_cuis) > 1:
                group_reduced = self._cluster_and_select_diverse(high_ic_cuis, similarity_threshold, distance_from_centroid_threshold)
            else:
                group_reduced = high_ic_cuis

            all_reduced_cuis.extend(group_reduced)
            group_stats[group_name] = {
                'original': len(group_cuis),
                'after_ic_filter': len(high_ic_cuis),
                'final': len(group_reduced),
                'reduction_pct': (1 - len(group_reduced)/len(group_cuis)) * 100 if group_cuis else 0
            }

        # --------------------- Final deduplication ---------------------
        final_cuis = list(set(all_reduced_cuis))
        # self._fetch_descriptions(final_cuis)

        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=total_after_ic,
            final_count=len(final_cuis),
            ic_rollup_reduction_pct=self._safe_percentage(initial_count - total_after_ic, initial_count),
            semantic_clustering_reduction_pct=self._safe_percentage(total_after_ic - len(final_cuis), initial_count),
            total_reduction_pct=self._safe_percentage(initial_count - len(final_cuis), initial_count),
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_percentile,
            hierarchy_size=sum(len(h.get("all_cuis", [])) for h in self._hierarchy_cache.values()),
            api_call_time=0.0,
            group_stats=group_stats,
            missing_embeddings_count=self._missing_embeddings_total
        )

        return final_cuis, stats

    # --------------------- Semantic type grouping ---------------------
    def _group_by_semantic_type_dynamic(self, cuis: List[str]) -> Dict[str, List[str]]:
        query = f"""
        SELECT DISTINCT CUI, STY
        FROM `{self.project_id}.{self.dataset_id}.{self.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cuis)]
        )
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        if df.empty:
            return {'UNKNOWN': cuis}

        semantic_groups = defaultdict(set)
        cui_to_types = defaultdict(set)
        for _, row in df.iterrows():
            semantic_groups[row['STY']].add(row['CUI'])
            cui_to_types[row['CUI']].add(row['STY'])

        final_groups = defaultdict(list)
        for cui in cuis:
            types = cui_to_types.get(cui, {'UNKNOWN'})
            sorted_types = sorted(types, key=lambda x: (-len(x), x))
            for t in sorted_types:
                final_groups[t].append(cui)
        return dict(final_groups)

    # --------------------- Hierarchy from pickle ---------------------
    def _build_hierarchy_depthwise(self, cuis: List[str]) -> Dict:
        key = tuple(sorted(cuis))
        if key in self._hierarchy_cache:
            return self._hierarchy_cache[key]

        all_cuis = set()
        parent_to_children = defaultdict(list)
        child_to_parents = defaultdict(list)

        for cui in cuis:
            if not UMLS_NETWORK_OBJ.has_node(cui):
                continue
            # Children
            for child in UMLS_NETWORK_OBJ.successors(cui):
                parent_to_children[cui].append(child)
                child_to_parents[child].append(cui)
                all_cuis.update([cui, child])
            # Parents
            for parent in UMLS_NETWORK_OBJ.predecessors(cui):
                child_to_parents[cui].append(parent)
                parent_to_children[parent].append(cui)
                all_cuis.update([cui, parent])

        hierarchy = {
            "parent_to_children": dict(parent_to_children),
            "child_to_parents": dict(child_to_parents),
            "all_cuis": all_cuis
        }
        self._hierarchy_cache[key] = hierarchy
        return hierarchy

    # --------------------- IC Scores ---------------------
    def _compute_ic_scores_within_group(
        self,
        hierarchy: Dict,
        group_cuis: List[str],
        group_name: str
    ) -> Dict[str, float]:
        cache_key = f"{group_name}_{tuple(sorted(group_cuis))}"
        if cache_key in self._ic_scores_cache:
            return self._ic_scores_cache[cache_key]

        parent_to_children = hierarchy.get("parent_to_children", {})
        group_set = set(group_cuis)
        descendant_counts = {}

        def count_descendants(cui: str, visited: Set[str] = None) -> int:
            if visited is None:
                visited = set()
            if cui in visited or cui not in group_set:
                return 0
            visited.add(cui)
            children = [c for c in parent_to_children.get(cui, []) if c in group_set]
            count = len(children)
            for child in children:
                count += count_descendants(child, visited)
            descendant_counts[cui] = count
            return count

        for cui in group_cuis:
            if cui not in descendant_counts:
                try:
                    count_descendants(cui)
                except RecursionError:
                    descendant_counts[cui] = 0

        group_size = len(group_cuis)
        ic_scores = {cui: max(0.0, -np.log((descendant_counts.get(cui,0)+1)/group_size)) for cui in group_cuis}
        self._ic_scores_cache[cache_key] = ic_scores
        return ic_scores

    # --------------------- Embedding Clustering ---------------------
    def _cluster_and_select_diverse(self, cui_list, similarity_threshold, distance_threshold):
        if len(cui_list) <= 1:
            return cui_list
        query = f"""
        SELECT CUI AS cui, embedding AS embedding
        FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)]
        )
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        if df.empty:
            return cui_list
        valid = [(row["cui"], np.asarray(row["embedding"], dtype=np.float32)) for _, row in df.iterrows() if row["embedding"] is not None]
        if len(valid) < 2:
            return cui_list
        cuis, embeddings = zip(*valid)
        embeddings = np.vstack(embeddings)
        clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1-similarity_threshold, metric="cosine", linkage="average")
        labels = clustering.fit_predict(embeddings)
        selected = set()
        for label in np.unique(labels):
            idx = np.where(labels == label)[0]
            cluster_emb = embeddings[idx]
            centroid = np.mean(cluster_emb, axis=0)
            distances = cosine_distances([centroid], cluster_emb)[0]
            far = idx[distances > distance_threshold]
            if len(far):
                selected.update([cuis[i] for i in far])
            else:
                selected.add(cuis[idx[np.argmax(distances)]])
        return list(selected)

    # --------------------- Fetch descriptions ---------------------
    # def _fetch_descriptions(self, cuis: List[str]):
    # to_fetch = [c for c in cuis if c not in self._description_cache]
    # if not to_fetch:
    #     return
    # query = f"""
    # SELECT CUI AS cui, DEF AS description
    # FROM `{self.project_id}.{self.dataset_id}.MRDEF`
    # WHERE CUI IN UNNEST(@cuis)
    # """
    # job_config = bigquery.QueryJobConfig(
    #     query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", to_fetch)]
    # )
    # df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
    # for _, row in df.iterrows():
    #     self._description_cache[row['cui']] = row['description']
    # logger.info(f"Fetched {len(df)} definitions")

    # --------------------- Utility ---------------------
    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        return (numerator / denominator * 100) if denominator else 0.0

# --------------------- Main Pipeline ---------------------
if __name__ == "__main__":

    # # --------------------- Load Pickle ---------------------
    # NETWORK_PKL = "/home/jupyter/NER/networkx_cui_context_v1_1_0.pkl"
    # with open(NETWORK_PKL, "rb") as f:
    #     UMLS_NETWORK_OBJ = pickle.load(f)
    # logger.info(f"Pickle loaded. Type: {type(UMLS_NETWORK_OBJ)}")

    # --------------------- Inputs ---------------------
    project_id = project_id
    dataset_id = dataset
    cui_embeddings_table = embedding_table
    mrsty_table = "MRSTY"
    texts = ["grams"]

    # --------------------- Extract CUIs ---------------------
    api_url = url
    api_client = CUIAPIClient(api_url)
    extracted_cuis = api_client.extract_cuis_batch(texts)

    # --------------------- Filter CUIs ---------------------
    filtered_cuis = filter_allowed_cuis(extracted_cuis, project_id, dataset_id)

    # --------------------- Reduce CUIs ---------------------
    reducer = EnhancedCUIReducer(
        project_id=project_id,
        dataset_id=dataset_id,
        cui_embeddings_table=cui_embeddings_table,
        mrsty_table=mrsty_table,
        umls_network_obj=UMLS_NETWORK_OBJ
    )
    final_cuis, stats = reducer.reduce(filtered_cuis)

    logger.info(f"Final CUIs ({len(final_cuis)}): {len(final_cuis)}")
    # logger.info(f"Reduction Stats: {stats.to_dict()}")

    
    
parent_counts = [
    len(list(UMLS_NETWORK_OBJ.predecessors(c)))
    for c in final_cuis
    if UMLS_NETWORK_OBJ.has_node(c)
]

print("CUIs with NO parents:", sum(p == 0 for p in parent_counts))
print("Total reduced CUIs:", len(parent_counts))
print("Max parents:", max(parent_counts))


import networkx as nx

def build_reduced_graph_with_parents(full_graph, reduced_cuis):
    reduced_set = set(reduced_cuis)
    G = nx.DiGraph()

    # Add reduced CUIs
    G.add_nodes_from(reduced_set)

    for cui in reduced_set:
        if not full_graph.has_node(cui):
            continue

        parents = list(full_graph.predecessors(cui))
        for parent in parents:
            # Add parent anchor + edge
            G.add_node(parent)
            G.add_edge(cui, parent)

    return G

reduced_graph = build_reduced_graph_with_parents(
    UMLS_NETWORK_OBJ,
    final_cuis
)

with open("/home/jupyter/logicmatch/reduced_cui_graph.pkl", "wb") as f:
    pickle.dump(reduced_graph, f)

logger.info(
    f"Saved reduced CUI graph with "
    f"{reduced_graph.number_of_nodes()} nodes and "
    f"{reduced_graph.number_of_edges()} edges"
)

from typing import Dict, List
import numpy as np
from google.cloud import bigquery

def _fetch_embeddings(reducer, cuis: List[str]) -> Dict[str, np.ndarray]:
    """
    Fetch embeddings for a list of CUIs using the reducer's BigQuery client.
    
    Args:
        reducer: an instance of EnhancedCUIReducer
        cuis: list of CUIs
    
    Returns:
        dict mapping CUI -> np.ndarray embedding
    """
    if not cuis:
        return {}

    query = f"""
    SELECT CUI AS cui, embedding
    FROM `{reducer.project_id}.{reducer.dataset_id}.{reducer.cui_embeddings_table}`
    WHERE CUI IN UNNEST(@cuis)
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cuis)]
    )

    df = reducer.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()

    embeddings = {}
    for _, row in df.iterrows():
        if row["embedding"] is not None:
            embeddings[row["cui"]] = np.asarray(row["embedding"], dtype=np.float32)

    return embeddings

# Fetch embeddings for the reduced CUIs
reduced_embeddings = _fetch_embeddings(reducer, final_cuis)

len(reduced_embeddings)


#right hand side

import networkx as nx
import pickle
import numpy as np

# ----------------- Inputs -----------------
topic_texts = ["grams in patient report"]

# ----------------- Extract CUIs -----------------
api_client = CUIAPIClient(api_url)
topic_cuis = api_client.extract_cuis_batch(topic_texts)
print(f"Extracted {len(topic_cuis):,} CUIs from topics")

# ----------------- Filter allowed CUIs -----------------
filtered_topic_cuis = filter_allowed_cuis(topic_cuis, project_id, dataset_id)
print(f"{len(filtered_topic_cuis):,} CUIs after filtering")

# ----------------- Keep only CUIs present in UMLS network -----------------
final_topic_cuis = [cui for cui in filtered_topic_cuis if UMLS_NETWORK_OBJ.has_node(cui)]
print(f"Reduced topic CUIs (exist in network): {len(final_topic_cuis):,}")

# ----------------- Build full topic CUI graph with edges -----------------
# Instead of just subgraph nodes, include hierarchy edges from UMLS_NETWORK_OBJ
topic_graph = UMLS_NETWORK_OBJ.subgraph(final_topic_cuis).copy()

# If some nodes are disconnected (no edges), keep them as isolated nodes
for cui in final_topic_cuis:
    if cui not in topic_graph:
        topic_graph.add_node(cui)

print(f"Topic CUI graph: {topic_graph.number_of_nodes():,} nodes, "
      f"{topic_graph.number_of_edges():,} edges")

# Optionally save graph
pickle_file = "reduced_topic_cui_graph_with_edges.pkl"
with open(pickle_file, "wb") as f:
    pickle.dump(topic_graph, f)
print(f"Saved topic CUI graph with edges to {pickle_file}")

# ----------------- Prepare reduced CUI graph -----------------
# Your left-hand side graph (from user queries)
with open("reduced_cui_graph.pkl", "rb") as f:
    reduced_graph = pickle.load(f)

reduced_cuis = set(reduced_graph.nodes())
print(f"Reduced CUIs (LHS): {len(reduced_cuis):,}")

# ----------------- Ancestor function -----------------
def get_ancestors_with_paths(graph: nx.DiGraph, cui: str, max_depth: int = 5):
    """Get all ancestors with their depths and paths"""
    if cui not in graph:
        return {}
    
    ancestor_info = {}
    visited = {cui}
    current_level = {cui: (0, [cui])}
    
    for depth in range(1, max_depth + 1):
        next_level = {}
        for node, (node_depth, path) in current_level.items():
            parents = set(graph.predecessors(node))
            for parent in parents:
                if parent not in visited:
                    new_path = path + [parent]
                    ancestor_info[parent] = (depth, new_path)
                    next_level[parent] = (depth, new_path)
                    visited.add(parent)
        if not next_level:
            break
        current_level = next_level
    return ancestor_info

# ----------------- Ancestor matching -----------------
matches = []

for topic_cui in final_topic_cuis:
    # Only process if not directly matched
    if topic_cui in reduced_cuis:
        matches.append((topic_cui, topic_cui, 'direct'))
        continue
    
    # Get ancestors
    ancestor_info = get_ancestors_with_paths(topic_graph, topic_cui, max_depth=5)
    
    # Find intersection with reduced CUIs
    matching_ancestors = set(ancestor_info.keys()) & reduced_cuis
    if matching_ancestors:
        # Pick closest ancestor (smallest depth)
        best_ancestor = min(matching_ancestors, key=lambda a: ancestor_info[a][0])
        depth, path = ancestor_info[best_ancestor]
        matches.append((topic_cui, best_ancestor, 'ancestor'))

print(f"Total matches (direct + ancestor): {len(matches):,}")


#logic

import pickle
import numpy as np
import networkx as nx
import pandas as pd
from typing import Dict, List, Tuple, Set, Optional, Union
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp
from functools import partial
import json


class CUIMapper:
    """    
    1. Direct matches
    2. Ancestor matches (with hybrid scoring)
    3. Embedding matches (only if needed)
    """
    
    def __init__(
        self,
        reduced_cui_graph_path: str,
        reduced_embeddings: Union[str, Dict],
        topic_cui_graph_path: str,
        topic_embeddings: Union[str, Dict],
        gold_mappings: Optional[Dict[str, str]] = None,
        n_jobs: int = -1
    ):
        """
        Initialize CUI matcher
        
        Args:
            reduced_cui_graph_path: Path to reduced CUI graph pickle
            reduced_embeddings: In-memory dict OR path to pickle
            topic_cui_graph_path: Path to topic CUI graph pickle  
            topic_embeddings: In-memory dict OR path to pickle
            gold_mappings: Optional gold standard for evaluation
            n_jobs: Number of parallel jobs (-1 = all cores)
        """
        print("Loading graphs and embeddings...")
        
        # Load graphs
        with open(reduced_cui_graph_path, 'rb') as f:
            self.reduced_graph = pickle.load(f)
        with open(topic_cui_graph_path, 'rb') as f:
            self.topic_graph = pickle.load(f)
        
        # Load or use in-memory embeddings
        if isinstance(reduced_embeddings, str):
            with open(reduced_embeddings, 'rb') as f:
                self.reduced_embeddings = pickle.load(f)
        else:
            self.reduced_embeddings = reduced_embeddings
        
        if isinstance(topic_embeddings, str):
            with open(topic_embeddings, 'rb') as f:
                self.topic_embeddings = pickle.load(f)
        else:
            self.topic_embeddings = topic_embeddings
        
        # Extract CUI lists
        self.reduced_cuis = set(self.reduced_graph.nodes())
        self.topic_cuis = set(self.topic_graph.nodes())
        
        # Evaluation
        self.gold_mappings = gold_mappings
        
        # Parallel processing
        self.n_jobs = mp.cpu_count() if n_jobs == -1 else n_jobs
        
        print(f"  Reduced CUIs: {len(self.reduced_cuis):,}")
        print(f"  Topic CUIs: {len(self.topic_cuis):,}")
        print(f"  Reduced embeddings: {len(self.reduced_embeddings):,}")
        print(f"  Topic embeddings: {len(self.topic_embeddings):,}")
        print(f"  Parallel workers: {self.n_jobs}")
        if gold_mappings:
            print(f"  Gold standard: {len(gold_mappings):,} mappings")
        print()
    
    def _get_ancestors_with_paths(
        self,
        graph: nx.DiGraph,
        cui: str,
        max_depth: int = 5
    ) -> Dict[str, Tuple[int, List[str]]]:
        """Get all ancestors with their depths and paths"""
        if cui not in graph:
            return {}
        
        ancestor_info = {}
        visited = {cui}
        current_level = {cui: (0, [cui])}
        
        for depth in range(1, max_depth + 1):
            next_level = {}
            
            for node, (node_depth, path) in current_level.items():
                parents = set(graph.predecessors(node))
                
                for parent in parents:
                    if parent not in visited:
                        new_path = path + [parent]
                        ancestor_info[parent] = (depth, new_path)
                        next_level[parent] = (depth, new_path)
                        visited.add(parent)
            
            if not next_level:
                break
            
            current_level = next_level
        
        return ancestor_info
    
    def _compute_hybrid_score(
        self,
        ancestor_depth: Optional[int] = None,
        embedding_similarity: Optional[float] = None,
        ancestor_weight: float = 0.4,
        embedding_weight: float = 0.6
    ) -> float:
        """
        Compute hybrid score combining ancestor distance and embedding similarity
        Closer ancestors = higher score, Higher similarity = higher score
        """
        if ancestor_depth is None and embedding_similarity is None:
            return 0.0
        
        # Score ancestor by exponential decay with depth
        ancestor_score = np.exp(-ancestor_depth / 3.0) if ancestor_depth is not None else 0.0
        embedding_score = embedding_similarity if embedding_similarity is not None else 0.0
        
        # Weighted combination
        if ancestor_depth is not None and embedding_similarity is not None:
            return ancestor_weight * ancestor_score + embedding_weight * embedding_score
        elif ancestor_depth is not None:
            return ancestor_score
        else:
            return embedding_score
    
    def direct_match(self, topic_cuis: Set[str]) -> pd.DataFrame:
        """Step 1: Find direct CUI matches"""
        print("="*70)
        print("STEP 1: DIRECT MATCHING")
        print("="*70)
        
        matches = []
        for topic_cui in topic_cuis:
            if topic_cui in self.reduced_cuis:
                matches.append({
                    'topic_cui': topic_cui,
                    'reduced_cui': topic_cui,
                    'method': 'direct',
                    'confidence': 1.0,
                    'similarity': 1.0,
                    'hybrid_score': 1.0
                })
        
        df = pd.DataFrame(matches)
        
        print(f"✓ Direct matches found: {len(df):,}")
        print()
        
        return df
    
    def ancestor_match(
        self,
        unmatched_topic_cuis: Set[str],
        max_depth: int = 5,
        use_hybrid: bool = True
    ) -> pd.DataFrame:
        """Step 2: Find ancestor matches with hybrid scoring"""
        print("="*70)
        print("STEP 2: ANCESTOR MATCHING")
        print("="*70)
        print(f"Remaining CUIs: {len(unmatched_topic_cuis):,}")
        
        matches = []
        
        for topic_cui in unmatched_topic_cuis:
            # Get ancestors with paths
            ancestor_info = self._get_ancestors_with_paths(
                self.topic_graph, topic_cui, max_depth
            )
            
            # Find intersection with reduced CUIs
            matching_ancestors = set(ancestor_info.keys()) & self.reduced_cuis
            
            if not matching_ancestors:
                continue
            
            # Select best ancestor
            if use_hybrid and topic_cui in self.topic_embeddings:
                # Use hybrid scoring
                topic_emb = self.topic_embeddings[topic_cui]
                best_score = -1
                best_ancestor = None
                best_info = None
                
                for anc in matching_ancestors:
                    depth, path = ancestor_info[anc]
                    
                    # Get embedding similarity if available
                    emb_sim = None
                    if anc in self.reduced_embeddings:
                        anc_emb = self.reduced_embeddings[anc]
                        emb_sim = float(cosine_similarity([topic_emb], [anc_emb])[0][0])
                    
                    # Compute hybrid score
                    hybrid_score = self._compute_hybrid_score(depth, emb_sim)
                    
                    if hybrid_score > best_score:
                        best_score = hybrid_score
                        best_ancestor = anc
                        best_info = (depth, path, emb_sim, hybrid_score)
                
                if best_ancestor:
                    depth, path, emb_sim, hybrid_score = best_info
                    confidence = max(0.5, 1.0 - (depth * 0.1))
                    
                    matches.append({
                        'topic_cui': topic_cui,
                        'reduced_cui': best_ancestor,
                        'method': 'ancestor',
                        'confidence': confidence,
                        'similarity': emb_sim,
                        'ancestor_depth': depth,
                        'ancestor_path': str(path),
                        'hybrid_score': hybrid_score,
                        'num_candidates': len(matching_ancestors)
                    })
            else:
                # Use simple closest ancestor
                best_ancestor = min(matching_ancestors, 
                                   key=lambda a: (ancestor_info[a][0], a))
                depth, path = ancestor_info[best_ancestor]
                confidence = max(0.5, 1.0 - (depth * 0.1))
                
                matches.append({
                    'topic_cui': topic_cui,
                    'reduced_cui': best_ancestor,
                    'method': 'ancestor',
                    'confidence': confidence,
                    'ancestor_depth': depth,
                    'ancestor_path': str(path),
                    'num_candidates': len(matching_ancestors)
                })
        
        df = pd.DataFrame(matches)
        
        print(f"✓ Ancestor matches found: {len(df):,}")
        print()
        
        return df
    
    def _embedding_match_batch(
        self,
        topic_cui_batch: List[str],
        similarity_threshold: float,
        top_k: int
    ) -> List[Dict]:
        """Process a batch of topic CUIs for embedding matching"""
        matches = []
        
        # Prepare reduced embeddings
        reduced_cuis_with_emb = [
            cui for cui in self.reduced_cuis
            if cui in self.reduced_embeddings
        ]
        
        if not reduced_cuis_with_emb:
            return matches
        
        reduced_emb_matrix = np.array([
            self.reduced_embeddings[cui] for cui in reduced_cuis_with_emb
        ])
        
        for topic_cui in topic_cui_batch:
            if topic_cui not in self.topic_embeddings:
                continue
            
            topic_emb = self.topic_embeddings[topic_cui]
            
            # Compute similarities
            similarities = cosine_similarity([topic_emb], reduced_emb_matrix)[0]
            
            # Get top-k
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            top_scores = similarities[top_indices]
            
            # Take best match above threshold
            if top_scores[0] >= similarity_threshold:
                best_reduced_cui = reduced_cuis_with_emb[top_indices[0]]
                
                matches.append({
                    'topic_cui': topic_cui,
                    'reduced_cui': best_reduced_cui,
                    'method': 'embedding',
                    'confidence': float(top_scores[0]),
                    'similarity': float(top_scores[0]),
                    'hybrid_score': float(top_scores[0])
                })
        
        return matches
    
    def embedding_match(
        self,
        unmatched_topic_cuis: Set[str],
        similarity_threshold: float = 0.7,
        top_k: int = 3,
        parallel: bool = True
    ) -> pd.DataFrame:
        """Step 3: Find embedding matches (only if needed)"""
        print("="*70)
        print("STEP 3: EMBEDDING MATCHING")
        print("="*70)
        print(f"Remaining CUIs: {len(unmatched_topic_cuis):,}")
        
        # Filter to CUIs with embeddings
        topic_cuis_with_emb = [
            cui for cui in unmatched_topic_cuis 
            if cui in self.topic_embeddings
        ]
        
        if not topic_cuis_with_emb:
            print("✓ No CUIs with embeddings to match")
            print()
            return pd.DataFrame()
        
        # Parallel or sequential processing
        if parallel and self.n_jobs > 1 and len(topic_cuis_with_emb) > 100:
            # Split into batches
            batch_size = max(10, len(topic_cuis_with_emb) // (self.n_jobs * 4))
            batches = [topic_cuis_with_emb[i:i+batch_size] 
                      for i in range(0, len(topic_cuis_with_emb), batch_size)]
            
            # Process in parallel
            match_fn = partial(
                self._embedding_match_batch,
                similarity_threshold=similarity_threshold,
                top_k=top_k
            )
            
            with ThreadPoolExecutor(max_workers=self.n_jobs) as executor:
                batch_results = list(executor.map(match_fn, batches))
            
            # Flatten results
            matches = [match for batch in batch_results for match in batch]
        else:
            # Sequential processing
            matches = self._embedding_match_batch(
                topic_cuis_with_emb,
                similarity_threshold,
                top_k
            )
        
        df = pd.DataFrame(matches)
        
        print(f"✓ Embedding matches found: {len(df):,}")
        print()
        
        return df
    
    def evaluate(self, results_df: pd.DataFrame) -> Dict:
        """Evaluate results against gold standard"""
        if not self.gold_mappings:
            return {}
        
        predicted = dict(zip(results_df['topic_cui'], results_df['reduced_cui']))
        
        gold_cuis = set(self.gold_mappings.keys())
        pred_cuis = set(predicted.keys())
        
        tp = sum(1 for cui in (gold_cuis & pred_cuis) 
                if predicted[cui] == self.gold_mappings[cui])
        fp = sum(1 for cui in (gold_cuis & pred_cuis)
                if predicted[cui] != self.gold_mappings[cui])
        fn = len(gold_cuis - pred_cuis)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = tp / len(gold_cuis & pred_cuis) if len(gold_cuis & pred_cuis) > 0 else 0.0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'accuracy': accuracy,
            'true_positives': tp,
            'false_positives': fp,
            'false_negatives': fn,
            'coverage': len(pred_cuis) / len(gold_cuis) if len(gold_cuis) > 0 else 0.0
        }
    
    def match_all(
        self,
        topic_cuis: Optional[Set[str]] = None,
        ancestor_max_depth: int = 5,
        use_hybrid_scoring: bool = True,
        embedding_threshold: float = 0.7,
        embedding_top_k: int = 3,
        skip_embedding_if_ratio_high: float = 0.9,
        parallel_embedding: bool = True
    ) -> Tuple[pd.DataFrame, Set[str], Dict]:
        """
        Execute complete matching pipeline
        
        Args:
            topic_cuis: CUIs to match (None = all)
            ancestor_max_depth: Max depth for ancestor search
            use_hybrid_scoring: Use hybrid scoring for ancestor selection
            embedding_threshold: Min similarity for embedding matches
            embedding_top_k: Top K candidates to evaluate
            skip_embedding_if_ratio_high: Skip embedding step if match rate already high
            parallel_embedding: Use parallel processing for embeddings
        
        Returns:
            (results_df, unmatched_cuis, stats)
        """
        if topic_cuis is None:
            topic_cuis = self.topic_cuis
        else:
            topic_cuis = set(topic_cuis)
        
        print("\n" + "="*70)
        print("CUI MATCHING PIPELINE")
        print("="*70)
        print(f"Total topic CUIs to match: {len(topic_cuis):,}")
        print("="*70)
        print()
        
        all_results = []
        
        # Step 1: Direct matches
        direct_results = self.direct_match(topic_cuis)
        all_results.append(direct_results)
        matched_cuis = set(direct_results['topic_cui']) if len(direct_results) > 0 else set()
        
        # Step 2: Ancestor matches
        unmatched_after_direct = topic_cuis - matched_cuis
        print(f"After direct matching: {len(unmatched_after_direct):,} CUIs remaining\n")
        
        ancestor_results = self.ancestor_match(
            unmatched_after_direct,
            max_depth=ancestor_max_depth,
            use_hybrid=use_hybrid_scoring
        )
        all_results.append(ancestor_results)
        matched_cuis.update(set(ancestor_results['topic_cui']) if len(ancestor_results) > 0 else set())
        
        # Check if we should skip embedding step
        unmatched_after_ancestor = topic_cuis - matched_cuis
        current_match_rate = len(matched_cuis) / len(topic_cuis) if len(topic_cuis) > 0 else 0
        
        print(f"After ancestor matching: {len(unmatched_after_ancestor):,} CUIs remaining")
        print(f"Current match rate: {current_match_rate*100:.1f}%\n")
        
        # Step 3: Embedding matches (conditional)
        if len(unmatched_after_ancestor) > 0:
            if current_match_rate >= skip_embedding_if_ratio_high:
                print(f"Skipping embedding step (match rate {current_match_rate*100:.1f}% >= {skip_embedding_if_ratio_high*100:.1f}%)")
                print()
                embedding_results = pd.DataFrame()
            else:
                embedding_results = self.embedding_match(
                    unmatched_after_ancestor,
                    similarity_threshold=embedding_threshold,
                    top_k=embedding_top_k,
                    parallel=parallel_embedding
                )
                all_results.append(embedding_results)
                matched_cuis.update(set(embedding_results['topic_cui']) if len(embedding_results) > 0 else set())
        else:
            print("No unmatched CUIs - skipping embedding step\n")
            embedding_results = pd.DataFrame()
        
        # Combine all results
        results_df = pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()
        unmatched_cuis = topic_cuis - matched_cuis
        
        # Calculate statistics
        stats = {
            'total_topic_cuis': len(topic_cuis),
            'total_matched': len(matched_cuis),
            'total_unmatched': len(unmatched_cuis),
            'match_rate': len(matched_cuis) / len(topic_cuis) if len(topic_cuis) > 0 else 0,
            'by_method': {
                'direct': len(direct_results),
                'ancestor': len(ancestor_results),
                'embedding': len(embedding_results)
            },
            'avg_confidence': results_df['confidence'].mean() if len(results_df) > 0 else 0,
            'avg_hybrid_score': results_df['hybrid_score'].mean() if 'hybrid_score' in results_df.columns and len(results_df) > 0 else 0
        }
        
        # Evaluation
        if self.gold_mappings:
            eval_metrics = self.evaluate(results_df)
            stats['evaluation'] = eval_metrics
        
        # Print final summary
        self._print_summary(stats)
        
        return results_df, unmatched_cuis, stats
    
    def _print_summary(self, stats: Dict):
        """Print final summary"""
        print("="*70)
        print("FINAL SUMMARY")
        print("="*70)
        print(f"Total matched:   {stats['total_matched']:,} / {stats['total_topic_cuis']:,} ({stats['match_rate']*100:.1f}%)")
        print(f"Total unmatched: {stats['total_unmatched']:,}")
        
        print(f"\nBreakdown by method:")
        for method, count in stats['by_method'].items():
            pct = (count / stats['total_matched'] * 100) if stats['total_matched'] > 0 else 0
            print(f"  {method:12s}: {count:,} ({pct:.1f}%)")
        
        print(f"\nQuality metrics:")
        print(f"  Avg confidence:   {stats['avg_confidence']:.3f}")
        print(f"  Avg hybrid score: {stats['avg_hybrid_score']:.3f}")
        
        if 'evaluation' in stats:
            print(f"\nEvaluation (vs gold standard):")
            eval_m = stats['evaluation']
            print(f"  Precision: {eval_m['precision']:.3f}")
            print(f"  Recall:    {eval_m['recall']:.3f}")
            print(f"  F1 Score:  {eval_m['f1_score']:.3f}")
            print(f"  Accuracy:  {eval_m['accuracy']:.3f}")
        
        print("="*70)
        print()
    
    def save_results(
        self,
        results_df: pd.DataFrame,
        output_csv: str = 'cui_matches.csv',
        stats: Dict = None,
        stats_json: str = 'cui_stats.json'
    ):
        """Save results to files"""
        # Save main results
        results_df.to_csv(output_csv, index=False)
        print(f"✓ Results saved to: {output_csv}")
        
        # Save statistics
        if stats:
            with open(stats_json, 'w') as f:
                json.dump(stats, f, indent=2)
            print(f"✓ Statistics saved to: {stats_json}")
        
        print()


# =============================================================================
# USAGE EXAMPLE
# =============================================================================

if __name__ == "__main__":
    
    # Initialize mapper
    mapper = CUIMapper(
        reduced_cui_graph_path='reduced_cui_graph.pkl',# left side
        reduced_embeddings=reduced_embeddings,  # left side
        topic_cui_graph_path='reduced_topic_cui_graph.pkl',# right side
        topic_embeddings=reduced_topic_embeddings,      # right side        
        n_jobs=-1
    )
    
    # Run matching
    results, unmatched, stats = mapper.match_all(
        ancestor_max_depth=5,
        use_hybrid_scoring=True,
        embedding_threshold=0.7,
        skip_embedding_if_ratio_high=0.9,  # Skip embeddings if 90%+ already matched
        parallel_embedding=True
    )
    
    # Save results
    mapper.save_results(
        results,
        output_csv='cui_matches.csv',
        stats=stats,
        stats_json='cui_stats.json'
    )
    
    # Create lookup dictionary
    cui_map = dict(zip(results['topic_cui'], results['reduced_cui']))
    print(f"Created mapping dictionary with {len(cui_map):,} entries")
