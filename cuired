#load pickle file : 

import pickle
import time
import os

# Path to your PKL
NETWORK_PKL = "/home/jupyter/CUIreduction/networkx_cui_context_v1_1_0.pkl"


# Check file exists
assert os.path.exists(NETWORK_PKL), f"PKL not found: {NETWORK_PKL}"

# Load PKL and measure time
start_time = time.time()
with open(NETWORK_PKL, "rb") as f:
    UMLS_NETWORK_OBJ = pickle.load(f)
print(f"PKL loaded in {time.time() - start_time:.2f} seconds")

# Optional: inspect type
print("Loaded object type:", type(UMLS_NETWORK_OBJ))

import asyncio
import aiohttp
import subprocess
import re
import logging
from pathlib import Path
import json
from typing import List, Dict, Set, Optional, Tuple
from collections import defaultdict
import time
from dataclasses import dataclass, asdict
import threading
import pickle
import numpy as np
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import nest_asyncio
import requests
from requests.adapters import HTTPAdapter, Retry

nest_asyncio.apply()

# --------------------- Logging ---------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --------------------- GCP Token ---------------------
class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force: bool = False) -> Dict[str, str]:
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token

            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)

            token = proc.stdout.strip()
            if not token:
                raise RuntimeError("Empty token from gcloud")

            cls._token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            cls._expiry = now + 3300  # ~55 minutes
            return cls._token

# --------------------- Data Classes ---------------------
@dataclass
class ReductionStats:
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    group_stats: Dict = None
    missing_embeddings_count: int = 0

    def to_dict(self):
        return asdict(self)

# --------------------- Filter CUIs ---------------------
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    try:
        logger.info(f"Filtering {len(cuis)} CUIs to retain only ICD, SNOMED, and LOINC...")
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('CCSR_ICD10CM','CCSR_ICD10PCS','DMDICD10','ICD10','ICD10AE','ICD10AM','ICD10AMAE',
                      'ICD10CM','ICD10DUT','ICD10PCS','ICD9CM','ICPC2ICD10DUT','ICPC2ICD10ENG',
                      'SNOMEDCT_US', 'LOINC')
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
        )
        df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        allowed_cuis = df['CUI'].tolist()
        logger.info(f"{len(allowed_cuis)} CUIs after filter")
        return allowed_cuis
    except Exception as e:
        logger.error(f"Failed to filter CUIs by SAB: {str(e)}")
        return []

# --------------------- CUI API Client ---------------------
class CUIAPIClient:
    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3, max_retries: int = 3):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.max_retries = max_retries

        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        if not texts:
            return set()
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = GCPTokenProvider.get_headers()
        logger.info(f"Extracting CUIs from {len(texts)} text(s)...")
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            if response.status_code == 401 and retry_auth:
                logger.warning("Token expired, refreshing...")
                GCPTokenProvider.get_headers(force=True)
                return self.extract_cuis_batch(texts, retry_auth=False)
            response.raise_for_status()
            data = response.json()
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
            return all_cuis
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return set()

# --------------------- Enhanced Reducer ---------------------
class EnhancedCUIReducer:
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        cui_embeddings_table: str,
        mrsty_table: str,
        umls_network_obj: dict
    ):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.cui_embeddings_table = cui_embeddings_table
        self.mrsty_table = mrsty_table
        self._preloaded_network = umls_network_obj

        self._hierarchy_cache = {}
        self._ic_scores_cache = {}
        self._description_cache = {}
        self._missing_embeddings_total = 0

    # --------------------- Main Reduction Function ---------------------
    def reduce(
        self,
        input_cuis: List[str],
        ic_percentile: int = 25,
        similarity_threshold: float = 0.95,
        distance_from_centroid_threshold: float = 0.4
    ) -> Tuple[List[str], ReductionStats]:

        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        logger.info(f"Starting semantic-group based reduction for {initial_count} CUIs")

        # --------------------- Semantic type grouping ---------------------
        semantic_groups = self._group_by_semantic_type_dynamic(input_cuis)
        logger.info(f"Created {len(semantic_groups)} semantic groups")

        all_reduced_cuis = []
        group_stats = {}
        total_after_ic = 0

        for group_name, group_cuis in semantic_groups.items():
            if not group_cuis:
                continue

            # --------------------- Build hierarchy from pickle ---------------------
            hierarchy = self._build_hierarchy_depthwise(group_cuis)

            # --------------------- IC scores ---------------------
            ic_scores = self._compute_ic_scores_within_group(hierarchy, group_cuis, group_name)
            ic_threshold = np.percentile(list(ic_scores.values()), ic_percentile) if ic_scores else 0.0
            high_ic_cuis = [cui for cui in group_cuis if ic_scores.get(cui, 0) >= ic_threshold]
            total_after_ic += len(high_ic_cuis)

            # --------------------- Embedding-based clustering ---------------------
            if len(high_ic_cuis) > 1:
                group_reduced = self._cluster_and_select_diverse(high_ic_cuis, similarity_threshold, distance_from_centroid_threshold)
            else:
                group_reduced = high_ic_cuis

            all_reduced_cuis.extend(group_reduced)
            group_stats[group_name] = {
                'original': len(group_cuis),
                'after_ic_filter': len(high_ic_cuis),
                'final': len(group_reduced),
                'reduction_pct': (1 - len(group_reduced)/len(group_cuis)) * 100 if group_cuis else 0
            }

        # --------------------- Final deduplication ---------------------
        final_cuis = list(set(all_reduced_cuis))
        # self._fetch_descriptions(final_cuis)

        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=total_after_ic,
            final_count=len(final_cuis),
            ic_rollup_reduction_pct=self._safe_percentage(initial_count - total_after_ic, initial_count),
            semantic_clustering_reduction_pct=self._safe_percentage(total_after_ic - len(final_cuis), initial_count),
            total_reduction_pct=self._safe_percentage(initial_count - len(final_cuis), initial_count),
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_percentile,
            hierarchy_size=sum(len(h.get("all_cuis", [])) for h in self._hierarchy_cache.values()),
            api_call_time=0.0,
            group_stats=group_stats,
            missing_embeddings_count=self._missing_embeddings_total
        )

        return final_cuis, stats

    # --------------------- Semantic type grouping ---------------------
    def _group_by_semantic_type_dynamic(self, cuis: List[str]) -> Dict[str, List[str]]:
        query = f"""
        SELECT DISTINCT CUI, STY
        FROM `{self.project_id}.{self.dataset_id}.{self.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cuis)]
        )
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        if df.empty:
            return {'UNKNOWN': cuis}

        semantic_groups = defaultdict(set)
        cui_to_types = defaultdict(set)
        for _, row in df.iterrows():
            semantic_groups[row['STY']].add(row['CUI'])
            cui_to_types[row['CUI']].add(row['STY'])

        final_groups = defaultdict(list)
        for cui in cuis:
            types = cui_to_types.get(cui, {'UNKNOWN'})
            sorted_types = sorted(types, key=lambda x: (-len(x), x))
            for t in sorted_types:
                final_groups[t].append(cui)
        return dict(final_groups)

    # --------------------- Hierarchy from pickle ---------------------
    def _build_hierarchy_depthwise(self, cuis: List[str]) -> Dict:
        key = tuple(sorted(cuis))
        if key in self._hierarchy_cache:
            return self._hierarchy_cache[key]

        all_cuis = set()
        parent_to_children = defaultdict(list)
        child_to_parents = defaultdict(list)

        for cui in cuis:
            if not UMLS_NETWORK_OBJ.has_node(cui):
                continue
            # Children
            for child in UMLS_NETWORK_OBJ.successors(cui):
                parent_to_children[cui].append(child)
                child_to_parents[child].append(cui)
                all_cuis.update([cui, child])
            # Parents
            for parent in UMLS_NETWORK_OBJ.predecessors(cui):
                child_to_parents[cui].append(parent)
                parent_to_children[parent].append(cui)
                all_cuis.update([cui, parent])

        hierarchy = {
            "parent_to_children": dict(parent_to_children),
            "child_to_parents": dict(child_to_parents),
            "all_cuis": all_cuis
        }
        self._hierarchy_cache[key] = hierarchy
        return hierarchy

    # --------------------- IC Scores ---------------------
    def _compute_ic_scores_within_group(
        self,
        hierarchy: Dict,
        group_cuis: List[str],
        group_name: str
    ) -> Dict[str, float]:
        cache_key = f"{group_name}_{tuple(sorted(group_cuis))}"
        if cache_key in self._ic_scores_cache:
            return self._ic_scores_cache[cache_key]

        parent_to_children = hierarchy.get("parent_to_children", {})
        group_set = set(group_cuis)
        descendant_counts = {}

        def count_descendants(cui: str, visited: Set[str] = None) -> int:
            if visited is None:
                visited = set()
            if cui in visited or cui not in group_set:
                return 0
            visited.add(cui)
            children = [c for c in parent_to_children.get(cui, []) if c in group_set]
            count = len(children)
            for child in children:
                count += count_descendants(child, visited)
            descendant_counts[cui] = count
            return count

        for cui in group_cuis:
            if cui not in descendant_counts:
                try:
                    count_descendants(cui)
                except RecursionError:
                    descendant_counts[cui] = 0

        group_size = len(group_cuis)
        ic_scores = {cui: max(0.0, -np.log((descendant_counts.get(cui,0)+1)/group_size)) for cui in group_cuis}
        self._ic_scores_cache[cache_key] = ic_scores
        return ic_scores

    # --------------------- Embedding Clustering ---------------------
    def _cluster_and_select_diverse(self, cui_list, similarity_threshold, distance_threshold):
        if len(cui_list) <= 1:
            return cui_list
        query = f"""
        SELECT CUI AS cui, embedding AS embedding
        FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)]
        )
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        if df.empty:
            return cui_list
        valid = [(row["cui"], np.asarray(row["embedding"], dtype=np.float32)) for _, row in df.iterrows() if row["embedding"] is not None]
        if len(valid) < 2:
            return cui_list
        cuis, embeddings = zip(*valid)
        embeddings = np.vstack(embeddings)
        clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1-similarity_threshold, metric="cosine", linkage="average")
        labels = clustering.fit_predict(embeddings)
        selected = set()
        for label in np.unique(labels):
            idx = np.where(labels == label)[0]
            cluster_emb = embeddings[idx]
            centroid = np.mean(cluster_emb, axis=0)
            distances = cosine_distances([centroid], cluster_emb)[0]
            far = idx[distances > distance_threshold]
            if len(far):
                selected.update([cuis[i] for i in far])
            else:
                selected.add(cuis[idx[np.argmax(distances)]])
        return list(selected)

    # --------------------- Fetch descriptions ---------------------
    # def _fetch_descriptions(self, cuis: List[str]):
    # to_fetch = [c for c in cuis if c not in self._description_cache]
    # if not to_fetch:
    #     return
    # query = f"""
    # SELECT CUI AS cui, DEF AS description
    # FROM `{self.project_id}.{self.dataset_id}.MRDEF`
    # WHERE CUI IN UNNEST(@cuis)
    # """
    # job_config = bigquery.QueryJobConfig(
    #     query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", to_fetch)]
    # )
    # df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
    # for _, row in df.iterrows():
    #     self._description_cache[row['cui']] = row['description']
    # logger.info(f"Fetched {len(df)} definitions")

    # --------------------- Utility ---------------------
    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        return (numerator / denominator * 100) if denominator else 0.0

# --------------------- Main Pipeline ---------------------
if __name__ == "__main__":

    # # --------------------- Load Pickle ---------------------
    # NETWORK_PKL = "/home/jupyter/NER/networkx_cui_context_v1_1_0.pkl"
    # with open(NETWORK_PKL, "rb") as f:
    #     UMLS_NETWORK_OBJ = pickle.load(f)
    # logger.info(f"Pickle loaded. Type: {type(UMLS_NETWORK_OBJ)}")

    # --------------------- Inputs ---------------------
    project_id = project_id
    dataset_id = dataset
    cui_embeddings_table = embedding_table
    mrsty_table = "MRSTY"
    texts = ["grams"]

    # --------------------- Extract CUIs ---------------------
    api_url = url
    api_client = CUIAPIClient(api_url)
    extracted_cuis = api_client.extract_cuis_batch(texts)

    # --------------------- Filter CUIs ---------------------
    filtered_cuis = filter_allowed_cuis(extracted_cuis, project_id, dataset_id)

    # --------------------- Reduce CUIs ---------------------
    reducer = EnhancedCUIReducer(
        project_id=project_id,
        dataset_id=dataset_id,
        cui_embeddings_table=cui_embeddings_table,
        mrsty_table=mrsty_table,
        umls_network_obj=UMLS_NETWORK_OBJ
    )
    final_cuis, stats = reducer.reduce(filtered_cuis)

    # logger.info(f"Final CUIs ({len(final_cuis)}): {final_cuis}")
    logger.info(f"Reduction Stats: {stats.to_dict()}")

