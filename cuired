"""
PRODUCTION CUI REDUCTION SYSTEM v2.0
=====================================
Works with EXISTING UMLS tables only
No new BigQuery tables required
Configuration in YAML files

Author: Clinical Intelligence Team
"""

import time
import threading
import logging
import traceback
import os
import yaml
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass, field, asdict
from collections import defaultdict
from enum import Enum
import numpy as np
import networkx as nx
import pickle
import psutil
import requests
import json
from functools import wraps
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess

from google.cloud import bigquery
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import pandas as pd

# ========================= LOGGING =========================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

print_lock = threading.Lock()

def thread_safe_print(msg: str, level: str = "INFO"):
    """Thread-safe logging"""
    with print_lock:
        if level == "ERROR":
            logger.error(msg)
        elif level == "WARNING":
            logger.warning(msg)
        else:
            logger.info(msg)

# ========================= PERFORMANCE MONITORING =========================

class PerformanceMonitor:
    """Track performance metrics"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.lock = threading.Lock()
    
    def record(self, metric_name: str, value: float):
        with self.lock:
            self.metrics[metric_name].append(value)
    
    def get_summary(self) -> Dict[str, Dict[str, float]]:
        summary = {}
        with self.lock:
            for name, values in self.metrics.items():
                if values:
                    summary[name] = {
                        'count': len(values),
                        'mean': np.mean(values),
                        'p95': np.percentile(values, 95),
                        'min': np.min(values),
                        'max': np.max(values)
                    }
        return summary

perf_monitor = PerformanceMonitor()

def monitor_performance(metric_name: str):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = func(*args, **kwargs)
                elapsed = (time.time() - start) * 1000
                perf_monitor.record(metric_name, elapsed)
                return result
            except Exception as e:
                elapsed = (time.time() - start) * 1000
                perf_monitor.record(f"{metric_name}_error", elapsed)
                raise
        return wrapper
    return decorator

# ========================= ADAPTIVE CACHING =========================

class FullyAdaptiveLRUCache:
    """Memory-aware LRU cache"""
    
    def __init__(self, memory_threshold: float = 0.2):
        self.cache = {}
        self.order = []
        self.lock = threading.RLock()
        self.memory_threshold = memory_threshold
        self.hits = 0
        self.misses = 0
    
    def get(self, key):
        with self.lock:
            if key in self.cache:
                self.order.remove(key)
                self.order.append(key)
                self.hits += 1
                return self.cache[key]
            self.misses += 1
            return None
    
    def put(self, key, value):
        with self.lock:
            if key in self.cache:
                self.order.remove(key)
            self.cache[key] = value
            self.order.append(key)
            self._evict_if_needed()
    
    def _evict_if_needed(self):
        try:
            mem = psutil.virtual_memory()
            while mem.available < self.memory_threshold * mem.total and self.order:
                oldest = self.order.pop(0)
                del self.cache[oldest]
                mem = psutil.virtual_memory()
        except Exception as e:
            logger.warning(f"Cache eviction error: {e}")
    
    def get_stats(self) -> Dict:
        with self.lock:
            total = self.hits + self.misses
            hit_rate = self.hits / total if total > 0 else 0.0
            return {
                'size': len(self.cache),
                'hits': self.hits,
                'misses': self.misses,
                'hit_rate': hit_rate
            }

# ========================= ENUMS =========================

class ScoringMode(Enum):
    ASSESSMENT = "assessment"
    REDUCTION = "reduction"

class UsageContext(Enum):
    QUERY = "query"
    DOCUMENT = "document"

# ========================= DATA MODELS =========================

@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]
    tokens: List[str]
    token_count: int
    ic_score: float
    
    @staticmethod
    def tokenize(text: str) -> List[str]:
        import re
        text = re.sub(r'[^\w\s-]', ' ', text.lower())
        tokens = [t.strip() for t in text.split() if t.strip() and len(t) > 1]
        return tokens

@dataclass
class ComponentScore:
    name: str
    score: float
    weight: float
    explanation: str
    details: Dict = field(default_factory=dict)
    
    @property
    def weighted_score(self) -> float:
        return self.score * self.weight

@dataclass
class CUIScore:
    cui: str
    preferred_term: str
    completeness_score: float
    component_scores: List[ComponentScore]
    retained_cuis: List[str]
    ranking: Optional[int] = None
    
    def to_dict(self) -> Dict:
        return {
            'cui': self.cui,
            'preferred_term': self.preferred_term,
            'completeness_score': self.completeness_score,
            'ranking': self.ranking,
            'components': [
                {
                    'name': c.name,
                    'score': c.score,
                    'weight': c.weight,
                    'contribution': c.weighted_score,
                    'explanation': c.explanation
                }
                for c in self.component_scores
            ],
            'retained_cuis': self.retained_cuis
        }

@dataclass
class ReductionResult:
    mode: ScoringMode
    usage_context: UsageContext
    input_cuis: List[str]
    output_cuis: List[str]
    scores: Dict[str, CUIScore]
    retention_map: Dict[str, List[str]]
    processing_time_ms: float
    metadata: Dict = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        return {
            'mode': self.mode.value,
            'context': self.usage_context.value,
            'input_count': len(self.input_cuis),
            'output_count': len(self.output_cuis),
            'reduction_rate': 1 - len(self.output_cuis) / len(self.input_cuis) if self.input_cuis else 0,
            'processing_time_ms': self.processing_time_ms,
            'output_cuis': self.output_cuis,
            'metadata': self.metadata
        }

@dataclass
class LearnedConfiguration:
    """Configuration computed from UMLS data"""
    config_name: str
    weights: Dict[str, float]
    params: Dict[str, float]
    modifiers: Dict[str, Set[str]]  # modifier_type -> set of terms
    statistics: Dict[str, Any]
    computed_at: float
    
    def save_to_yaml(self, filepath: str):
        """Save configuration to YAML file"""
        output = {
            'config_name': self.config_name,
            'computed_at': self.computed_at,
            'weights': self.weights,
            'params': self.params,
            'modifiers': {k: list(v) for k, v in self.modifiers.items()},
            'statistics': self.statistics
        }
        
        os.makedirs(os.path.dirname(filepath) or '.', exist_ok=True)
        with open(filepath, 'w') as f:
            yaml.dump(output, f, default_flow_style=False, sort_keys=False)
        
        thread_safe_print(f"[Config] Saved to: {filepath}")
    
    @classmethod
    def load_from_yaml(cls, filepath: str) -> 'LearnedConfiguration':
        """Load configuration from YAML file"""
        with open(filepath, 'r') as f:
            data = yaml.safe_load(f)
        
        return cls(
            config_name=data['config_name'],
            weights=data['weights'],
            params=data['params'],
            modifiers={k: set(v) for k, v in data['modifiers'].items()},
            statistics=data.get('statistics', {}),
            computed_at=data.get('computed_at', time.time())
        )

# ========================= UMLS STATISTICS COMPUTER =========================

class UMLSStatisticsComputer:
    """
    Compute all parameters from UMLS tables.
    NO new tables created - only reads from MRCONSO, MRREL, MRSTY.
    """
    
    def __init__(
        self,
        bq_client: bigquery.Client,
        project_id: str,
        dataset_id: str
    ):
        self.bq = bq_client
        self.project_id = project_id
        self.dataset_id = dataset_id
    
    @monitor_performance("compute_configuration")
    def compute_configuration(
        self,
        config_name: str = "learned_from_umls_v1",
        sample_size: int = 10000
    ) -> LearnedConfiguration:
        """
        Compute complete configuration from UMLS.
        
        Args:
            config_name: Name for this configuration
            sample_size: Sample size for analysis
        
        Returns:
            LearnedConfiguration object
        """
        thread_safe_print(f"[UMLS Stats] Computing configuration from UMLS...")
        thread_safe_print(f"[UMLS Stats] Sample size: {sample_size}")
        
        start_time = time.time()
        
        try:
            # Compute statistics
            ic_stats = self._compute_ic_distribution(sample_size)
            relationship_stats = self._compute_relationship_stats(sample_size)
            hierarchy_stats = self._compute_hierarchy_stats(sample_size)
            modifier_stats = self._discover_modifiers()
            
            # Derive parameters from statistics
            params = self._derive_parameters(
                ic_stats,
                relationship_stats,
                hierarchy_stats
            )
            
            # Derive weights from correlations
            weights = self._derive_weights(sample_size)
            
            # Prepare modifiers dictionary
            modifiers = self._prepare_modifiers(modifier_stats)
            
            config = LearnedConfiguration(
                config_name=config_name,
                weights=weights,
                params=params,
                modifiers=modifiers,
                statistics={
                    'ic_distribution': ic_stats,
                    'relationship_patterns': relationship_stats,
                    'hierarchy_depth': hierarchy_stats,
                    'modifier_discovery': modifier_stats,
                    'sample_size': sample_size
                },
                computed_at=time.time()
            )
            
            elapsed = time.time() - start_time
            thread_safe_print(f"[UMLS Stats] Configuration computed in {elapsed:.1f}s")
            
            return config
            
        except Exception as e:
            logger.error(f"Failed to compute configuration: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _compute_ic_distribution(self, sample_size: int) -> Dict:
        """Compute IC score distribution from UMLS"""
        query = f"""
        WITH sampled_cuis AS (
          SELECT DISTINCT CUI
          FROM `{self.project_id}.{self.dataset_id}.MRCONSO`
          WHERE LAT = 'ENG' AND ISPREF = 'Y'
          LIMIT {sample_size}
        ),
        cui_frequency AS (
          SELECT 
            c.CUI,
            COUNT(DISTINCT c.STR) as term_count,
            (SELECT COUNT(DISTINCT STR) 
             FROM `{self.project_id}.{self.dataset_id}.MRCONSO` 
             WHERE LAT = 'ENG') as total_terms
          FROM `{self.project_id}.{self.dataset_id}.MRCONSO` c
          JOIN sampled_cuis s ON c.CUI = s.CUI
          WHERE c.LAT = 'ENG'
          GROUP BY c.CUI
        ),
        ic_scores AS (
          SELECT 
            CUI,
            -LOG(SAFE_DIVIDE(term_count, total_terms)) as ic
          FROM cui_frequency
        )
        SELECT 
          APPROX_QUANTILES(ic, 100) as percentiles,
          AVG(ic) as mean_ic,
          STDDEV(ic) as std_ic
        FROM ic_scores
        WHERE ic IS NOT NULL
        """
        
        try:
            result = self.bq.query(query, timeout=300).result()
            row = next(result)
            
            percentiles = [float(p) if p is not None else 0.0 for p in row.percentiles]
            
            return {
                'mean': float(row.mean_ic or 3.5),
                'std': float(row.std_ic or 1.5),
                'percentiles': percentiles
            }
        except Exception as e:
            logger.warning(f"IC computation failed, using defaults: {e}")
            # Sensible defaults based on typical UMLS
            return {
                'mean': 3.5,
                'std': 1.5,
                'percentiles': [i * 0.08 for i in range(101)]
            }
    
    def _compute_relationship_stats(self, sample_size: int) -> Dict:
        """Compute relationship statistics"""
        query = f"""
        WITH sampled_cuis AS (
          SELECT DISTINCT CUI
          FROM `{self.project_id}.{self.dataset_id}.MRCONSO`
          WHERE LAT = 'ENG' AND ISPREF = 'Y'
          LIMIT {sample_size}
        ),
        cui_relationships AS (
          SELECT 
            r.CUI1,
            COUNT(DISTINCT r.CUI2) as related_count
          FROM `{self.project_id}.{self.dataset_id}.MRREL` r
          JOIN sampled_cuis s ON r.CUI1 = s.CUI
          WHERE r.REL IN ('PAR', 'CHD', 'RB', 'RN')
          GROUP BY r.CUI1
        )
        SELECT 
          APPROX_QUANTILES(related_count, 100) as percentiles,
          AVG(related_count) as mean_count
        FROM cui_relationships
        """
        
        try:
            result = self.bq.query(query, timeout=300).result()
            row = next(result)
            
            percentiles = [int(p) if p is not None else 0 for p in row.percentiles]
            
            return {
                'mean_related': float(row.mean_count or 25.0),
                'percentiles': percentiles
            }
        except Exception as e:
            logger.warning(f"Relationship stats failed, using defaults: {e}")
            return {
                'mean_related': 25.0,
                'percentiles': [int(i * 0.5) for i in range(101)]
            }
    
    def _compute_hierarchy_stats(self, sample_size: int) -> Dict:
        """Compute hierarchy depth statistics"""
        query = f"""
        WITH sampled_cuis AS (
          SELECT DISTINCT CUI
          FROM `{self.project_id}.{self.dataset_id}.MRCONSO`
          WHERE LAT = 'ENG' AND ISPREF = 'Y'
          LIMIT {sample_size}
        ),
        parent_counts AS (
          SELECT 
            s.CUI,
            COUNT(DISTINCT r.CUI2) as parent_count
          FROM sampled_cuis s
          LEFT JOIN `{self.project_id}.{self.dataset_id}.MRREL` r 
            ON s.CUI = r.CUI1 AND r.REL = 'PAR'
          GROUP BY s.CUI
        )
        SELECT 
          AVG(parent_count) as mean_depth,
          MAX(parent_count) as max_depth
        FROM parent_counts
        """
        
        try:
            result = self.bq.query(query, timeout=300).result()
            row = next(result)
            
            return {
                'mean_depth': float(row.mean_depth or 3.5),
                'max_depth': int(row.max_depth or 10)
            }
        except Exception as e:
            logger.warning(f"Hierarchy stats failed, using defaults: {e}")
            return {
                'mean_depth': 3.5,
                'max_depth': 10
            }
    
    def _discover_modifiers(self) -> Dict:
        """Discover modifier terms from UMLS semantic types"""
        query = f"""
        WITH modifier_semantic_types AS (
          SELECT 
            s.TUI,
            s.STY,
            COUNT(DISTINCT c.CUI) as cui_count,
            COUNT(DISTINCT c.STR) as term_count
          FROM `{self.project_id}.{self.dataset_id}.MRSTY` s
          JOIN `{self.project_id}.{self.dataset_id}.MRCONSO` c ON s.CUI = c.CUI
          WHERE c.LAT = 'ENG'
            AND LENGTH(c.STR) BETWEEN 3 AND 20
          GROUP BY s.TUI, s.STY
          HAVING cui_count > 10 AND term_count > 20
          ORDER BY cui_count DESC
          LIMIT 20
        )
        SELECT 
          TUI as semantic_type,
          STY as semantic_type_name,
          cui_count,
          term_count
        FROM modifier_semantic_types
        """
        
        try:
            result = self.bq.query(query, timeout=300).result()
            
            discovered_types = []
            for row in result:
                discovered_types.append({
                    'tui': row.semantic_type,
                    'name': row.semantic_type_name,
                    'cui_count': row.cui_count,
                    'term_count': row.term_count
                })
            
            thread_safe_print(f"[Modifiers] Discovered {len(discovered_types)} modifier types")
            
            return {
                'types': discovered_types,
                'count': len(discovered_types)
            }
        except Exception as e:
            logger.warning(f"Modifier discovery failed: {e}")
            return {'types': [], 'count': 0}
    
    def _derive_parameters(
        self,
        ic_stats: Dict,
        relationship_stats: Dict,
        hierarchy_stats: Dict
    ) -> Dict[str, float]:
        """Derive parameters from statistics"""
        
        ic_percentiles = ic_stats['percentiles']
        rel_percentiles = relationship_stats['percentiles']
        
        params = {
            # IC threshold: 25th percentile
            'ic_min_threshold': ic_percentiles[25],
            
            # Prefilter target: 75th percentile × 15
            'prefilter_target_size': rel_percentiles[75] * 15,
            
            # Max clusters: 60th percentile × 3
            'max_clusters': rel_percentiles[60] * 3,
            
            # Hierarchy sample: mean depth × 15
            'hierarchy_sample_size': hierarchy_stats['mean_depth'] * 15,
            
            # Embedding sample: hierarchy sample × 0.5
            'embedding_sample_size': hierarchy_stats['mean_depth'] * 7.5,
            
            # Final CUI counts
            'final_cui_count_query': min(5, max(2, relationship_stats['mean_related'] * 0.15)),
            'final_cui_count_document': min(8, max(3, relationship_stats['mean_related'] * 0.25)),
            
            # Retention top K
            'retention_top_k': rel_percentiles[60] * 0.9
        }
        
        thread_safe_print(f"[Params] Derived parameters:")
        for k, v in params.items():
            thread_safe_print(f"  {k}: {v:.2f}")
        
        return params
    
    def _derive_weights(self, sample_size: int) -> Dict[str, float]:
        """Derive component weights from feature correlations"""
        query = f"""
        WITH sampled_cuis AS (
          SELECT DISTINCT CUI
          FROM `{self.project_id}.{self.dataset_id}.MRCONSO`
          WHERE LAT = 'ENG' AND ISPREF = 'Y'
          LIMIT {sample_size}
        ),
        cui_features AS (
          SELECT 
            c.CUI,
            ARRAY_LENGTH(SPLIT(c.STR, ' ')) as token_count,
            (SELECT COUNT(*) 
             FROM `{self.project_id}.{self.dataset_id}.MRREL` r 
             WHERE r.CUI1 = c.CUI AND r.REL IN ('PAR', 'CHD')) as relationship_count,
            (SELECT COUNT(DISTINCT TUI) 
             FROM `{self.project_id}.{self.dataset_id}.MRSTY` s 
             WHERE s.CUI = c.CUI) as semantic_diversity,
            -LOG(SAFE_DIVIDE(
              COUNT(*) OVER (PARTITION BY c.STR),
              {sample_size}
            )) as ic
          FROM `{self.project_id}.{self.dataset_id}.MRCONSO` c
          JOIN sampled_cuis s ON c.CUI = s.CUI
          WHERE c.LAT = 'ENG' AND c.ISPREF = 'Y'
        )
        SELECT 
          CORR(token_count, ic) as token_ic_corr,
          CORR(relationship_count, ic) as hierarchy_ic_corr,
          CORR(semantic_diversity, ic) as diversity_ic_corr
        FROM cui_features
        WHERE token_count IS NOT NULL 
          AND relationship_count IS NOT NULL
          AND semantic_diversity IS NOT NULL
          AND ic IS NOT NULL
        """
        
        try:
            result = self.bq.query(query, timeout=300).result()
            row = next(result)
            
            # Use absolute correlations
            correlations = {
                'token': max(0.1, abs(row.token_ic_corr or 0.3)),
                'hierarchy': max(0.1, abs(row.hierarchy_ic_corr or 0.3)),
                'diversity': max(0.1, abs(row.diversity_ic_corr or 0.2))
            }
            
            total_corr = sum(correlations.values())
            base_total = 0.85  # Reserve 15% for embeddings
            
            weights = {
                'token_subsumption': (correlations['token'] / total_corr) * base_total * 0.4,
                'umls_hierarchy': (correlations['hierarchy'] / total_corr) * base_total * 0.4,
                'term_specificity': (correlations['token'] / total_corr) * base_total * 0.3,
                'modifier_presence': (correlations['diversity'] / total_corr) * base_total * 0.3,
                'semantic_similarity': 0.15
            }
            
            # Normalize
            weight_sum = sum(weights.values())
            weights = {k: v / weight_sum for k, v in weights.items()}
            
            thread_safe_print(f"[Weights] Derived from correlations:")
            for k, v in weights.items():
                thread_safe_print(f"  {k}: {v:.3f}")
            
            return weights
            
        except Exception as e:
            logger.warning(f"Weight derivation failed, using equal weights: {e}")
            return {
                'token_subsumption': 0.20,
                'umls_hierarchy': 0.20,
                'term_specificity': 0.20,
                'modifier_presence': 0.20,
                'semantic_similarity': 0.20
            }
    
    def _prepare_modifiers(self, modifier_stats: Dict) -> Dict[str, Set[str]]:
        """Prepare modifier terms dictionary"""
        modifiers = {}
        
        for mod_type in modifier_stats['types'][:10]:  # Top 10 types
            query = f"""
            SELECT DISTINCT LOWER(c.STR) as term
            FROM `{self.project_id}.{self.dataset_id}.MRCONSO` c
            JOIN `{self.project_id}.{self.dataset_id}.MRSTY` s ON c.CUI = s.CUI
            WHERE s.TUI = '{mod_type['tui']}'
              AND c.LAT = 'ENG'
              AND LENGTH(c.STR) BETWEEN 3 AND 20
            LIMIT 100
            """
            
            try:
                result = self.bq.query(query, timeout=60).result()
                
                # Normalize modifier type name
                type_name = mod_type['name'].lower().replace(' ', '_')
                terms = {row.term for row in result}
                
                if terms:
                    modifiers[type_name] = terms
                    
            except Exception as e:
                logger.warning(f"Failed to fetch terms for {mod_type['name']}: {e}")
        
        thread_safe_print(f"[Modifiers] Prepared {len(modifiers)} modifier types")
        return modifiers

# ========================= CONFIGURATION MANAGER =========================

class SimpleConfigManager:
    """
    Manage configuration from YAML files.
    No BigQuery tables required.
    """
    
    def __init__(self, config_dir: str = "./config"):
        self.config_dir = config_dir
        os.makedirs(config_dir, exist_ok=True)
        self._cache = {}
    
    def get_configuration(
        self,
        config_name: str = "default",
        force_recompute: bool = False
    ) -> LearnedConfiguration:
        """
        Get configuration - load from YAML or use cached.
        
        Args:
            config_name: Configuration name
            force_recompute: Force recomputation from UMLS
        
        Returns:
            LearnedConfiguration
        """
        config_path = os.path.join(self.config_dir, f"{config_name}.yaml")
        
        # Check cache
        if not force_recompute and config_name in self._cache:
            thread_safe_print(f"[Config] Using cached: {config_name}")
            return self._cache[config_name]
        
        # Try to load from file
        if not force_recompute and os.path.exists(config_path):
            thread_safe_print(f"[Config] Loading from file: {config_path}")
            config = LearnedConfiguration.load_from_yaml(config_path)
            self._cache[config_name] = config
            return config
        
        raise FileNotFoundError(
            f"Configuration '{config_name}' not found at {config_path}. "
            f"Please run initialization first to compute configuration from UMLS."
        )
    
    def save_configuration(self, config: LearnedConfiguration):
        """Save configuration to YAML"""
        config_path = os.path.join(self.config_dir, f"{config.config_name}.yaml")
        config.save_to_yaml(config_path)
        self._cache[config.config_name] = config

# ========================= HIERARCHY CLIENT (Same as before) =========================

class HierarchyClient:
    """NetworkX-based hierarchy with caching"""
    
    def __init__(
        self,
        network_obj: nx.DiGraph,
        ic_scores: Optional[Dict[str, float]] = None
    ):
        self.network = network_obj
        self.ic_scores = ic_scores or {}
        self.ancestors_cache = FullyAdaptiveLRUCache()
        self.children_cache = FullyAdaptiveLRUCache()
        self.ic_cache = FullyAdaptiveLRUCache()
        self.hierarchy_relation_cache = FullyAdaptiveLRUCache()
        self.lock = threading.RLock()
    
    def get_children(self, cui: str) -> List[str]:
        cached = self.children_cache.get(cui)
        if cached is not None:
            return cached
        children = list(self.network.successors(cui)) if self.network.has_node(cui) else []
        self.children_cache.put(cui, children)
        return children
    
    def get_parents(self, cui: str) -> List[str]:
        return list(self.network.predecessors(cui)) if self.network.has_node(cui) else []
    
    def get_ancestors(self, cui: str, max_depth: int = 10) -> List[List[str]]:
        key = (cui, max_depth)
        cached = self.ancestors_cache.get(key)
        if cached is not None:
            return cached
        
        queue = [(0, [cui])]
        visited = set()
        paths = []
        
        while queue:
            depth, path = queue.pop(0)
            node = path[-1]
            
            if node in visited:
                continue
            visited.add(node)
            
            if depth >= max_depth or not self.network.has_node(node):
                paths.append(path)
                continue
            
            parents = self.get_parents(node)
            if not parents:
                paths.append(path)
            else:
                for parent in parents:
                    queue.append((depth + 1, path + [parent]))
        
        self.ancestors_cache.put(key, paths)
        return paths
    
    def get_ic_score(self, cui: str) -> float:
        cached = self.ic_cache.get(cui)
        if cached is not None:
            return cached
        
        if cui in self.ic_scores:
            ic = self.ic_scores[cui]
        else:
            paths = self.get_ancestors(cui)
            if paths:
                avg_depth = sum(len(p) for p in paths) / len(paths)
                ic = min(10.0, 2.0 + np.log1p(avg_depth) * 2.0)
            else:
                ic = 3.0
        
        self.ic_cache.put(cui, ic)
        return ic
    
    def is_hierarchical_descendant(self, cui_child: str, cui_parent: str) -> bool:
        cache_key = (cui_child, cui_parent)
        cached = self.hierarchy_relation_cache.get(cache_key)
        if cached is not None:
            return cached
        
        try:
            if self.network.has_node(cui_child) and self.network.has_node(cui_parent):
                has_path = nx.has_path(self.network, cui_child, cui_parent)
            else:
                has_path = False
        except Exception:
            has_path = False
        
        self.hierarchy_relation_cache.put(cache_key, has_path)
        return has_path
    
    def get_cache_stats(self) -> Dict:
        return {
            'ancestors': self.ancestors_cache.get_stats(),
            'children': self.children_cache.get_stats(),
            'ic': self.ic_cache.get_stats(),
            'hierarchy_relations': self.hierarchy_relation_cache.get_stats()
        }

# ========================= PRE-FILTER, CLUSTERER, SCORER =========================
# (Keep all the FastCUIPreFilter, FastHierarchicalClusterer, OptimizedCUIScorer 
#  classes from the previous version - they work the same)

class FastCUIPreFilter:
    """Stage 1: Pre-filtering"""
    
    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy
    
    @monitor_performance("prefilter")
    def prefilter(
        self,
        cuis: List[str],
        params: Dict[str, float]
    ) -> Tuple[List[str], Dict]:
        start_time = time.time()
        stats = {'input_count': len(cuis)}
        
        ic_min_threshold = params.get('ic_min_threshold', 2.0)
        target_size = int(params.get('prefilter_target_size', 1000))
        
        thread_safe_print(f"[PreFilter] Input: {len(cuis)}, Target: {target_size}")
        
        # Filter by IC
        cuis = [cui for cui in cuis if self.hierarchy.get_ic_score(cui) >= ic_min_threshold]
        stats['after_ic_filter'] = len(cuis)
        
        # Remove ancestors
        cui_set = set(cuis)
        to_remove = set()
        for cui in cuis:
            parents = self.hierarchy.get_parents(cui)
            for parent in parents:
                if parent in cui_set:
                    to_remove.add(parent)
        cuis = [cui for cui in cuis if cui not in to_remove]
        stats['after_ancestor_filter'] = len(cuis)
        
        # Sample if needed
        if len(cuis) > target_size:
            import random
            buckets = defaultdict(list)
            for cui in cuis:
                ic = self.hierarchy.get_ic_score(cui)
                buckets[int(ic)].append(cui)
            
            sampled = []
            for bucket_key in sorted(buckets.keys()):
                bucket_cuis = buckets[bucket_key]
                bucket_proportion = len(bucket_cuis) / len(cuis)
                bucket_sample_size = max(1, int(target_size * bucket_proportion))
                
                if len(bucket_cuis) <= bucket_sample_size:
                    sampled.extend(bucket_cuis)
                else:
                    sampled.extend(random.sample(bucket_cuis, bucket_sample_size))
            
            if len(sampled) > target_size:
                sampled.sort(key=lambda c: self.hierarchy.get_ic_score(c), reverse=True)
                sampled = sampled[:target_size]
            
            cuis = sampled
            stats['after_sampling'] = len(cuis)
        
        stats['processing_time_ms'] = (time.time() - start_time) * 1000
        thread_safe_print(f"[PreFilter] Output: {len(cuis)}")
        
        return cuis, stats

class FastHierarchicalClusterer:
    """Stage 2: Clustering"""
    
    def __init__(self, hierarchy: HierarchyClient):
        self.hierarchy = hierarchy
    
    @monitor_performance("clustering")
    def cluster(
        self,
        cuis: List[str],
        params: Dict[str, float]
    ) -> Dict[str, List[str]]:
        max_clusters = int(params.get('max_clusters', 200))
        
        ancestor_groups = defaultdict(list)
        for cui in cuis:
            paths = self.hierarchy.get_ancestors(cui, max_depth=5)
            if paths and len(paths[0]) >= 3:
                ancestor = paths[0][min(3, len(paths[0]) - 1)]
            else:
                ancestor = cui
            ancestor_groups[ancestor].append(cui)
        
        if len(ancestor_groups) > max_clusters:
            sorted_clusters = sorted(ancestor_groups.items(), key=lambda x: len(x[1]))
            large_clusters = {k: v for k, v in sorted_clusters[-(max_clusters - 1):]}
            small_cluster_cuis = []
            for k, v in sorted_clusters[:-(max_clusters - 1)]:
                small_cluster_cuis.extend(v)
            large_clusters['MERGED_SMALL'] = small_cluster_cuis
            ancestor_groups = large_clusters
        
        thread_safe_print(f"[Clustering] Created {len(ancestor_groups)} clusters")
        return dict(ancestor_groups)

class OptimizedCUIScorer:
    """Stage 3: Scoring"""
    
    def __init__(
        self,
        config: LearnedConfiguration,
        hierarchy: HierarchyClient,
        bq_client: bigquery.Client,
        project_id: str,
        dataset_id: str
    ):
        self.config = config
        self.hierarchy = hierarchy
        self.bq = bq_client
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.metadata_cache = FullyAdaptiveLRUCache()
        self.token_index = defaultdict(set)
    
    @monitor_performance("scoring")
    def score_batch(
        self,
        cuis: List[str],
        embeddings: Dict[str, np.ndarray]
    ) -> Dict[str, CUIScore]:
        start_time = time.time()
        
        cui_metadata = self._fetch_cui_metadata(cuis)
        self._build_token_index(cui_metadata)
        self._embeddings = embeddings
        
        scored = {}
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(self._score_single_cui, cui, cui_metadata): cui
                for cui in cuis
            }
            
            for future in as_completed(futures, timeout=300):
                cui = futures[future]
                try:
                    scored[cui] = future.result()
                except Exception as e:
                    logger.error(f"Error scoring {cui}: {e}")
        
        processing_time = (time.time() - start_time) * 1000
        thread_safe_print(f"[Scoring] Scored {len(scored)} CUIs in {processing_time:.0f}ms")
        
        return scored
    
    def _build_token_index(self, cui_metadata: Dict[str, CUIMetadata]):
        self.token_index.clear()
        for cui, metadata in cui_metadata.items():
            for token in metadata.tokens:
                self.token_index[token].add(cui)
    
    def _score_single_cui(
        self,
        cui: str,
        all_metadata: Dict[str, CUIMetadata]
    ) -> CUIScore:
        metadata = all_metadata[cui]
        components = []
        weights = self.config.weights
        
        # Token Subsumption
        token_sub = self._fast_token_subsumption(metadata, all_metadata)
        components.append(ComponentScore(
            name="Token Subsumption",
            score=token_sub['score'],
            weight=weights.get('token_subsumption', 0.2),
            explanation=token_sub['explanation'],
            details=token_sub['details']
        ))
        
        # UMLS Hierarchy
        umls_hier = self._sampled_hierarchy_check(metadata, all_metadata)
        components.append(ComponentScore(
            name="UMLS Hierarchy",
            score=umls_hier['score'],
            weight=weights.get('umls_hierarchy', 0.2),
            explanation=umls_hier['explanation'],
            details=umls_hier['details']
        ))
        
        # Term Specificity
        term_spec = self._calculate_term_specificity(metadata)
        components.append(ComponentScore(
            name="Term Specificity",
            score=term_spec['score'],
            weight=weights.get('term_specificity', 0.2),
            explanation=term_spec['explanation'],
            details=term_spec['details']
        ))
        
        # Modifier Presence
        modifier = self._calculate_modifier_presence(metadata)
        components.append(ComponentScore(
            name="Clinical Modifiers",
            score=modifier['score'],
            weight=weights.get('modifier_presence', 0.2),
            explanation=modifier['explanation'],
            details=modifier['details']
        ))
        
        # Semantic Similarity
        semantic = self._fast_semantic_similarity(metadata, all_metadata)
        components.append(ComponentScore(
            name="Semantic Similarity",
            score=semantic['score'],
            weight=weights.get('semantic_similarity', 0.2),
            explanation=semantic['explanation'],
            details=semantic['details']
        ))
        
        final_score = sum(comp.weighted_score for comp in components)
        
        return CUIScore(
            cui=cui,
            preferred_term=metadata.preferred_term,
            completeness_score=final_score,
            component_scores=components,
            retained_cuis=[]
        )
    
    def _fast_token_subsumption(self, cui_metadata, all_metadata) -> Dict:
        cui_tokens = set(cui_metadata.tokens)
        candidate_cuis = set()
        for token in cui_tokens:
            candidate_cuis.update(self.token_index.get(token, set()))
        candidate_cuis.discard(cui_metadata.cui)
        
        if not candidate_cuis:
            return {'score': 0.0, 'explanation': "No overlapping CUIs", 'details': {}}
        
        subsumed_count = sum(
            1 for other_cui in candidate_cuis
            if (other_tokens := set(all_metadata[other_cui].tokens))
            and other_tokens.issubset(cui_tokens) 
            and len(cui_tokens) > len(other_tokens)
        )
        
        total_other = len(all_metadata) - 1
        score = subsumed_count / total_other if total_other > 0 else 0.0
        
        return {
            'score': score,
            'explanation': f"Subsumes {subsumed_count}/{total_other}",
            'details': {'subsumed_count': subsumed_count}
        }
    
    def _sampled_hierarchy_check(self, cui_metadata, all_metadata) -> Dict:
        import random
        sample_size = int(self.config.params.get('hierarchy_sample_size', 100))
        cui = cui_metadata.cui
        other_cuis = [c for c in all_metadata.keys() if c != cui]
        
        if not other_cuis:
            return {'score': 0.0, 'explanation': "No other CUIs", 'details': {}}
        
        sampled_cuis = random.sample(other_cuis, min(sample_size, len(other_cuis)))
        
        hierarchical_count = sum(
            1 for other_cui in sampled_cuis
            if self.hierarchy.is_hierarchical_descendant(cui, other_cui)
        )
        
        score = hierarchical_count / len(sampled_cuis)
        
        return {
            'score': score,
            'explanation': f"Subsumes ~{score:.0%}",
            'details': {'sampled': len(sampled_cuis)}
        }
    
    def _calculate_term_specificity(self, cui_metadata) -> Dict:
        ic = cui_metadata.ic_score
        tokens = cui_metadata.token_count
        ic_norm = min(ic / 10.0, 1.0)
        token_norm = min(tokens / 6.0, 1.0)
        score = 0.6 * ic_norm + 0.4 * token_norm
        
        return {
            'score': score,
            'explanation': f"IC={ic:.1f}, {tokens} tokens",
            'details': {'ic': ic, 'tokens': tokens}
        }
    
    def _calculate_modifier_presence(self, cui_metadata) -> Dict:
        tokens = set(cui_metadata.tokens)
        detected_types = []
        
        for mod_type, terms in self.config.modifiers.items():
            if tokens & terms:
                detected_types.append(mod_type)
        
        total_types = len(self.config.modifiers)
        score = len(detected_types) / total_types if total_types > 0 else 0.0
        
        return {
            'score': score,
            'explanation': f"{len(detected_types)} modifier types",
            'details': {'modifiers': detected_types}
        }
    
    def _fast_semantic_similarity(self, cui_metadata, all_metadata) -> Dict:
        import random
        
        if not hasattr(self, '_embeddings') or cui_metadata.cui not in self._embeddings:
            return {'score': 0.5, 'explanation': "No embedding", 'details': {}}
        
        cui_emb = self._embeddings[cui_metadata.cui]
        sample_size = int(self.config.params.get('embedding_sample_size', 50))
        
        other_cuis = [c for c in all_metadata.keys() 
                      if c != cui_metadata.cui and c in self._embeddings]
        
        if not other_cuis:
            return {'score': 0.5, 'explanation': "No comparable embeddings", 'details': {}}
        
        sampled = random.sample(other_cuis, min(sample_size, len(other_cuis)))
        
        similarities = [
            np.dot(cui_emb, self._embeddings[other_cui]) / (
                np.linalg.norm(cui_emb) * np.linalg.norm(self._embeddings[other_cui])
            )
            for other_cui in sampled
        ]
        
        score = float(np.mean(similarities))
        
        return {
            'score': score,
            'explanation': f"Avg similarity: {score:.2f}",
            'details': {'sampled': len(sampled)}
        }
    
    def _fetch_cui_metadata(self, cui_list: List[str]) -> Dict[str, CUIMetadata]:
        cached = {}
        missing = []
        
        for cui in cui_list:
            cached_meta = self.metadata_cache.get(cui)
            if cached_meta:
                cached[cui] = cached_meta
            else:
                missing.append(cui)
        
        if missing:
            query = f"""
            SELECT 
                c.CUI as cui,
                c.STR as preferred_term,
                ARRAY_AGG(DISTINCT s.TUI) as semantic_types
            FROM `{self.project_id}.{self.dataset_id}.MRCONSO` c
            LEFT JOIN `{self.project_id}.{self.dataset_id}.MRSTY` s ON c.CUI = s.CUI
            WHERE c.CUI IN UNNEST(@cui_list)
              AND c.LAT = 'ENG'
              AND c.ISPREF = 'Y'
            GROUP BY c.CUI, c.STR
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[bigquery.ArrayQueryParameter("cui_list", "STRING", missing)]
            )
            
            try:
                results = self.bq.query(query, job_config=job_config, timeout=300).result()
                
                for row in results:
                    tokens = CUIMetadata.tokenize(row.preferred_term)
                    ic_score = self.hierarchy.get_ic_score(row.cui)
                    
                    metadata = CUIMetadata(
                        cui=row.cui,
                        preferred_term=row.preferred_term,
                        semantic_types=row.semantic_types or [],
                        tokens=tokens,
                        token_count=len(tokens),
                        ic_score=ic_score
                    )
                    
                    self.metadata_cache.put(row.cui, metadata)
                    cached[row.cui] = metadata
                    
            except Exception as e:
                logger.error(f"Error fetching metadata: {e}")
        
        return cached

# ========================= HIGH-PERFORMANCE PIPELINE =========================

class HighPerformanceReductionPipeline:
    """Complete pipeline"""
    
    def __init__(
        self,
        hierarchy: HierarchyClient,
        config: LearnedConfiguration,
        bq_client: bigquery.Client,
        project_id: str,
        dataset_id: str
    ):
        self.pre_filter = FastCUIPreFilter(hierarchy)
        self.clusterer = FastHierarchicalClusterer(hierarchy)
        self.scorer = OptimizedCUIScorer(config, hierarchy, bq_client, project_id, dataset_id)
        self.hierarchy = hierarchy
        self.config = config
    
    @monitor_performance("full_pipeline")
    def reduce(
        self,
        cuis: List[str],
        embeddings: Dict[str, np.ndarray],
        usage_context: UsageContext = UsageContext.QUERY
    ) -> ReductionResult:
        overall_start = time.time()
        
        thread_safe_print(f"\n{'='*70}")
        thread_safe_print(f"REDUCTION PIPELINE: {len(cuis)} CUIs")
        thread_safe_print(f"{'='*70}")
        
        try:
            # Stage 1
            filtered_cuis, prefilter_stats = self.pre_filter.prefilter(cuis, self.config.params)
            
            # Stage 2
            clusters = self.clusterer.cluster(filtered_cuis, self.config.params)
            cluster_representatives = []
            for cluster_cuis in clusters.values():
                if len(cluster_cuis) == 1:
                    cluster_representatives.append(cluster_cuis[0])
                else:
                    best_cui = max(cluster_cuis, key=lambda c: self.hierarchy.get_ic_score(c))
                    cluster_representatives.append(best_cui)
            
            # Stage 3
            scored_embeddings = {cui: emb for cui, emb in embeddings.items() if cui in cluster_representatives}
            scored_cuis = self.scorer.score_batch(cluster_representatives, scored_embeddings)
            
            # Stage 4
            retention_map = self._build_retention_map(scored_cuis)
            
            # Stage 5
            sorted_cuis = sorted(scored_cuis.keys(), key=lambda c: scored_cuis[c].completeness_score, reverse=True)
            max_final = int(self.config.params.get(f'final_cui_count_{usage_context.value}', 3))
            selected_cuis = self._select_final_cuis(sorted_cuis, scored_cuis, retention_map, max_final)
            
            total_time = (time.time() - overall_start) * 1000
            
            thread_safe_print(f"COMPLETE: {len(cuis)} → {len(selected_cuis)} in {total_time:.0f}ms")
            
            return ReductionResult(
                mode=ScoringMode.REDUCTION,
                usage_context=usage_context,
                input_cuis=cuis,
                output_cuis=selected_cuis,
                scores=scored_cuis,
                retention_map=retention_map,
                processing_time_ms=total_time,
                metadata={'prefilter_stats': prefilter_stats}
            )
            
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            raise
    
    def _build_retention_map(self, scored_cuis: Dict[str, CUIScore]) -> Dict[str, List[str]]:
        retention_top_k = int(self.config.params.get('retention_top_k', 50))
        top_cuis = sorted(scored_cuis.keys(), key=lambda c: scored_cuis[c].completeness_score, reverse=True)[:retention_top_k]
        
        retention_map = {}
        for cui_a in top_cuis:
            metadata_a = self.scorer.metadata_cache.get(cui_a)
            if not metadata_a:
                continue
            
            tokens_a = set(metadata_a.tokens)
            retained = []
            
            for cui_b in top_cuis:
                if cui_a == cui_b:
                    continue
                metadata_b = self.scorer.metadata_cache.get(cui_b)
                if not metadata_b:
                    continue
                
                tokens_b = set(metadata_b.tokens)
                if tokens_b and tokens_b.issubset(tokens_a) and len(tokens_a) > len(tokens_b):
                    retained.append(cui_b)
            
            retention_map[cui_a] = retained
        
        return retention_map
    
    def _select_final_cuis(
        self,
        sorted_cuis: List[str],
        scored_cuis: Dict[str, CUIScore],
        retention_map: Dict[str, List[str]],
        max_cuis: int
    ) -> List[str]:
        if len(sorted_cuis) <= max_cuis:
            return sorted_cuis
        
        selected = [sorted_cuis[0]]
        
        for cui in sorted_cuis[1:]:
            if len(selected) >= max_cuis:
                break
            
            is_retained = any(cui in retention_map.get(sel, []) for sel in selected)
            
            if not is_retained:
                selected.append(cui)
        
        return selected

# ========================= UTILITY FUNCTIONS =========================

class CUIExtractor:
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.session = requests.Session()
        
        try:
            tmp = subprocess.run(
                ['gcloud', 'auth', 'print-identity-token'],
                stdout=subprocess.PIPE,
                universal_newlines=True,
                timeout=30
            )
            token = tmp.stdout.strip()
            self.headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
        except Exception as e:
            logger.error(f"Failed to get auth token: {e}")
            self.headers = {"Content-Type": "application/json"}
        
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))
    
    @monitor_performance("cui_extraction")
    def extract_for_text(self, text: str) -> List[str]:
        try:
            payload = {"query_texts": [text], "top_k": 3}
            resp = self.session.post(self.api_url, headers=self.headers, json=payload, timeout=200)
            resp.raise_for_status()
            data = resp.json()
            
            cuis = []
            for v in data.values():
                if isinstance(v, list):
                    cuis.extend(v)
            
            return list(set(map(str, cuis)))
        except Exception as e:
            logger.error(f"CUI extraction failed: {e}")
            return []

def filter_by_sab(cuis: List[str], project_id: str, dataset_id: str, allowed_sabs: List[str]) -> List[str]:
    if not cuis:
        return []
    
    try:
        client = bigquery.Client(project=project_id)
        filtered = []
        
        for i in range(0, len(cuis), 2000):
            batch = cuis[i:i+2000]
            query = f"""
            SELECT DISTINCT CUI
            FROM `{project_id}.{dataset_id}.MRCONSO`
            WHERE CUI IN UNNEST(@cuis) AND SAB IN UNNEST(@sabs)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", batch),
                    bigquery.ArrayQueryParameter("sabs", "STRING", allowed_sabs)
                ]
            )
            
            results = client.query(query, job_config=job_config, timeout=200).result()
            filtered.extend([row.CUI for row in results])
        
        return filtered
    except Exception as e:
        logger.error(f"SAB filter failed: {e}")
        return []

def fetch_cui_embeddings(
    cuis: List[str],
    project_id: str,
    dataset_id: str,
    table_name: str
) -> Dict[str, np.ndarray]:
    client = bigquery.Client(project=project_id)
    embeddings = {}
    
    try:
        for i in range(0, len(cuis), 2000):
            batch = cuis[i:i+2000]
            query = f"""
            SELECT CUI, embedding
            FROM `{project_id}.{dataset_id}.{table_name}`
            WHERE CUI IN UNNEST(@cuis)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", batch)]
            )
            
            results = client.query(query, job_config=job_config, timeout=200).result()
            
            for row in results:
                embeddings[row.CUI] = np.array(row.embedding)
    except Exception as e:
        logger.error(f"Embedding fetch failed: {e}")
    
    return embeddings

# ========================= INITIALIZATION & USAGE =========================

def initialize_system(
    project_id: str,
    dataset_id: str,
    config_dir: str = "./config",
    sample_size: int = 10000
):
    """
    Initialize system by computing configuration from UMLS.
    Run this ONCE to create configuration files.
    """
    thread_safe_print("="*70)
    thread_safe_print("INITIALIZING CUI REDUCTION SYSTEM")
    thread_safe_print("="*70)
    
    # Compute configuration from UMLS
    bq_client = bigquery.Client(project=project_id)
    stats_computer = UMLSStatisticsComputer(bq_client, project_id, dataset_id)
    
    config = stats_computer.compute_configuration(
        config_name="default",
        sample_size=sample_size
    )
    
    # Save to YAML
    config_manager = SimpleConfigManager(config_dir)
    config_manager.save_configuration(config)
    
    thread_safe_print("="*70)
    thread_safe_print("INITIALIZATION COMPLETE")
    thread_safe_print(f"Configuration saved to: {config_dir}/default.yaml")
    thread_safe_print("="*70)
    
    return config

def main():
    """Example usage"""
    
    # Configuration
    PROJECT_ID = "your-project"
    DATASET_ID = "umls_dataset"
    API_URL = "your-api-url"
    EMBEDDING_TABLE = "cui_embeddings"
    NETWORK_PKL_PATH = "/path/to/network.pkl"
    ALLOWED_SABS = ['ICD10CM', 'ICD10PCS', 'ICD9CM', 'SNOMEDCT_US', 'LOINC']
    CONFIG_DIR = "./config"
    
    # Step 1: Initialize (RUN ONCE)
    # Uncomment to compute configuration from UMLS
    # initialize_system(PROJECT_ID, DATASET_ID, CONFIG_DIR, sample_size=10000)
    
    # Step 2: Load network
    thread_safe_print("Loading UMLS network...")
    with open(NETWORK_PKL_PATH, "rb") as f:
        UMLS_NETWORK_OBJ = pickle.load(f)
    thread_safe_print(f"Network loaded: {UMLS_NETWORK_OBJ.number_of_nodes()} nodes")
    
    # Step 3: Load configuration
    bq_client = bigquery.Client(project=PROJECT_ID)
    hierarchy = HierarchyClient(UMLS_NETWORK_OBJ)
    config_manager = SimpleConfigManager(CONFIG_DIR)
    config = config_manager.get_configuration("default")
    
    # Step 4: Initialize pipeline
    pipeline = HighPerformanceReductionPipeline(
        hierarchy=hierarchy,
        config=config,
        bq_client=bq_client,
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID
    )
    
    extractor = CUIExtractor(API_URL)
    
    # Step 5: Process
    texts = [
        "Patient has severe pain in left knee with swelling",
        "Chest pain radiating to left arm"
    ]
    
    for text in texts:
        thread_safe_print(f"\nProcessing: {text}")
        
        extracted = extractor.extract_for_text(text)
        filtered = filter_by_sab(extracted, PROJECT_ID, DATASET_ID, ALLOWED_SABS)
        embeddings = fetch_cui_embeddings(filtered, PROJECT_ID, DATASET_ID, EMBEDDING_TABLE)
        
        result = pipeline.reduce(filtered, embeddings, UsageContext.QUERY)
        
        thread_safe_print(f"\nFinal CUIs:")
        for cui in result.output_cuis:
            score_obj = result.scores[cui]
            thread_safe_print(f"  {score_obj.preferred_term}: {score_obj.completeness_score:.3f}")
    
    # Performance summary
    thread_safe_print("\n" + "="*70)
    thread_safe_print("PERFORMANCE SUMMARY")
    thread_safe_print("="*70)
    summary = perf_monitor.get_summary()
    for metric, stats in summary.items():
        thread_safe_print(f"{metric}: Mean={stats['mean']:.0f}ms, P95={stats['p95']:.0f}ms")

if __name__ == "__main__":
    main()
