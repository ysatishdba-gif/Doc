import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel

# ======================================================
# TRULY LLM-DRIVEN SYSTEM - MINIMAL HARDCODING
# ======================================================

class AdaptiveLLMPipeline:
    """
    A truly LLM-driven clinical intent extraction system.
    
    Philosophy:
    - LLM decides its own processing steps
    - LLM determines its own output format
    - LLM adapts its approach based on query complexity
    - Code provides infrastructure, not logic
    - Prompts are minimal scaffolding, not prescriptive rules
    """
    
    def __init__(self, 
                 project: str,
                 location: Optional[str] = None,
                 model: Optional[str] = None,
                 config: Optional[Dict[str, Any]] = None):
        """
        Initialize with flexible configuration.
        
        Args:
            project: GCP project ID
            location: GCP region (None = let LLM decide)
            model: Model name (None = let LLM decide)
            config: Custom configuration dict
        """
        # Let LLM decide defaults through meta-prompting
        if location is None:
            location = self._ask_llm_for_default("location", ["us-central1", "us-east1", "europe-west1"])
        if model is None:
            model = self._ask_llm_for_default("model", ["gemini-2.0-flash-exp", "gemini-1.5-pro", "gemini-1.5-flash"])
            
        aiplatform.init(project=project, location=location)
        self.model = GenerativeModel(model)
        self.config = config or {}
        
        # Let LLM define its own system behavior
        self.system_instructions = self._initialize_system()
        
    def _ask_llm_for_default(self, param_type: str, options: List[str]) -> str:
        """Let LLM choose optimal default configuration."""
        prompt = f"""You are a system configuration expert.
        
Choose the optimal {param_type} for a clinical intent extraction system.

Options: {json.dumps(options)}

Consider:
- Performance vs cost
- Speed vs accuracy
- Current best practices

Return ONLY the chosen option as plain text, nothing else."""

        try:
            temp_model = GenerativeModel("gemini-2.0-flash-exp")
            response = temp_model.generate_content(prompt)
            choice = response.text.strip()
            # Validate choice is in options
            return choice if choice in options else options[0]
        except:
            return options[0]  # Fallback to first option
    
    def _initialize_system(self) -> str:
        """Let LLM define its own system instructions."""
        prompt = """You are an adaptive AI system for medical information extraction.

Your task: Define your OWN system instructions for processing clinical queries.

Consider:
1. What principles should guide your processing?
2. How should you adapt to different query types?
3. What output format would be most useful?
4. How should you handle errors and edge cases?
5. What level of detail is appropriate?

Write clear, comprehensive system instructions for yourself.
These instructions will guide all future query processing.

Return your self-defined system instructions as plain text."""

        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except:
            # Minimal fallback
            return "Process clinical queries intelligently and adaptively."
    
    def _call_llm(self, 
                  prompt: str, 
                  context: Optional[Dict[str, Any]] = None,
                  adapt_params: bool = True) -> str:
        """
        Call LLM with adaptive parameters.
        
        Args:
            prompt: The prompt to send
            context: Optional context for the LLM to consider
            adapt_params: If True, let LLM choose its own generation params
        """
        # Let LLM decide its own generation parameters
        if adapt_params and context:
            param_prompt = f"""Based on this task context:
{json.dumps(context, indent=2)}

Choose optimal generation parameters:
- temperature (0.0-2.0): How creative should I be?
- max_tokens (1000-8000): How long should my response be?
- top_p (0.0-1.0): How focused should I be?
- top_k (1-100): How diverse should I be?

Return ONLY valid JSON:
{{"temperature": float, "max_tokens": int, "top_p": float, "top_k": int}}"""
            
            try:
                param_response = self.model.generate_content(param_prompt)
                params = self._extract_json(param_response.text)
            except:
                params = {}
        else:
            params = {}
        
        # Use LLM-chosen params or reasonable defaults
        generation_config = {
            "temperature": params.get("temperature", 0.3),
            "max_output_tokens": params.get("max_tokens", 8192),
            "top_p": params.get("top_p", 0.95),
            "top_k": params.get("top_k", 40)
        }
        
        try:
            # Include system instructions
            full_prompt = f"""System Instructions:
{self.system_instructions}

Task:
{prompt}"""
            
            response = self.model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            return response.text.strip()
        except Exception as e:
            return json.dumps({"error": str(e), "fallback": True})
    
    def _extract_json(self, text: str) -> Dict[str, Any]:
        """Flexibly extract JSON from text using LLM assistance if needed."""
        # Try standard extraction first
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'```\s*', '', text)
        text = text.strip()
        
        try:
            return json.loads(text)
        except:
            pass
        
        # Try to find JSON in text
        for pattern in [r'\{.*\}', r'\[.*\]']:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(0))
                except:
                    pass
        
        # Ask LLM to help extract JSON
        extraction_prompt = f"""Extract valid JSON from this text:

{text[:1000]}

Return ONLY valid JSON, nothing else."""
        
        try:
            response = self.model.generate_content(extraction_prompt)
            return json.loads(response.text.strip())
        except:
            return {"error": "Failed to extract JSON", "raw_text": text[:500]}
    
    def process_query(self, query: str, verbose: bool = False) -> Dict[str, Any]:
        """
        Process a query with full LLM autonomy.
        
        The LLM decides:
        - Whether this is clinical
        - What processing steps are needed
        - What output format to use
        - How deep to analyze
        - What information to extract
        """
        start_time = datetime.utcnow()
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"Query: {query}")
            print(f"{'='*80}\n")
        
        # Stage 1: Let LLM decide how to process this query
        planning_prompt = f"""You are processing this query:
"{query}"

Your task: Decide how to process this query.

Consider:
1. Is this a clinical/medical query? If not, how should you respond?
2. What information needs to be extracted?
3. What processing steps would be most effective?
4. What level of detail is appropriate?
5. What output format would be most useful?

Return a JSON processing plan:
{{
  "is_clinical": boolean,
  "reasoning": "why you made this determination",
  "recommended_steps": ["step1", "step2", ...],
  "output_structure": {{"suggested": "structure"}},
  "complexity_level": "simple|moderate|complex",
  "estimated_processing_time": "estimated time"
}}"""
        
        planning_response = self._call_llm(planning_prompt, adapt_params=True)
        plan = self._extract_json(planning_response)
        
        if verbose:
            print(f"Processing Plan:")
            print(f"  Clinical: {plan.get('is_clinical', 'unknown')}")
            print(f"  Complexity: {plan.get('complexity_level', 'unknown')}")
            print(f"  Steps: {len(plan.get('recommended_steps', []))}")
            print()
        
        # If not clinical, let LLM handle it appropriately
        if not plan.get("is_clinical", True):
            response_prompt = f"""This query is not clinical:
"{query}"

Reasoning: {plan.get('reasoning', 'Not medical in nature')}

Provide an appropriate response to the user.
Be helpful, but clarify this is outside the clinical domain."""
            
            response = self._call_llm(response_prompt, adapt_params=False)
            
            return {
                "query": query,
                "is_clinical": False,
                "reasoning": plan.get("reasoning"),
                "response": response,
                "processing_time_seconds": (datetime.utcnow() - start_time).total_seconds(),
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # Stage 2: Let LLM process the query its own way
        processing_prompt = f"""Process this clinical query according to your processing plan:

Query: "{query}"

Processing Plan:
{json.dumps(plan, indent=2)}

Your task: Extract all relevant clinical information from this query.

You have complete freedom to:
- Determine what information is important
- Decide how to structure the output
- Choose the level of detail
- Define any taxonomies or classifications
- Generate any queries or breakdowns you think are useful

Be thorough but adaptive. The complexity level is: {plan.get('complexity_level', 'moderate')}

Return your analysis in whatever JSON structure you think is most appropriate and useful."""
        
        processing_context = {
            "query_type": "clinical",
            "complexity": plan.get("complexity_level", "moderate"),
            "recommended_steps": plan.get("recommended_steps", [])
        }
        
        processing_response = self._call_llm(processing_prompt, context=processing_context, adapt_params=True)
        result = self._extract_json(processing_response)
        
        if verbose:
            print(f"Processing Complete:")
            print(f"  Keys in result: {list(result.keys())}")
            print(f"  Result size: {len(json.dumps(result))} chars")
            print()
        
        # Stage 3: Let LLM optionally refine its own output
        refinement_prompt = f"""Review your own output for this query:

Original Query: "{query}"

Your Output:
{json.dumps(result, indent=2)[:2000]}...

Questions to consider:
1. Is this output clear and well-structured?
2. Is anything missing or should be added?
3. Could anything be more concise or better organized?
4. Is the level of detail appropriate?

If refinement would improve quality, provide the refined version.
If the output is already good, return it as-is with a note that no changes are needed.

Return JSON:
{{
  "needs_refinement": boolean,
  "reasoning": "why or why not",
  "refined_output": {{...}} or null if no changes needed
}}"""
        
        refinement_response = self._call_llm(refinement_prompt, adapt_params=False)
        refinement = self._extract_json(refinement_response)
        
        if verbose:
            print(f"Refinement:")
            print(f"  Needed: {refinement.get('needs_refinement', False)}")
            print(f"  Reasoning: {refinement.get('reasoning', 'N/A')[:100]}...")
            print()
        
        # Use refined output if LLM decided to refine
        if refinement.get("needs_refinement") and refinement.get("refined_output"):
            final_result = refinement["refined_output"]
        else:
            final_result = result
        
        # Stage 4: Let LLM add any metadata it thinks is useful
        metadata_prompt = f"""You've just processed this clinical query:
"{query}"

What metadata would be useful to include with the results?

Consider:
- Confidence scores
- Quality indicators  
- Suggestions for the user
- Warnings or caveats
- Related information
- Next steps

Return JSON with any metadata you think would be valuable:
{{"metadata": {{"key": "value", ...}}}}"""
        
        metadata_response = self._call_llm(metadata_prompt, adapt_params=False)
        metadata = self._extract_json(metadata_response)
        
        # Combine everything
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        output = {
            "query": query,
            "is_clinical": True,
            "processing_plan": plan,
            "result": final_result,
            "refinement": {
                "applied": refinement.get("needs_refinement", False),
                "reasoning": refinement.get("reasoning")
            },
            "metadata": metadata.get("metadata", {}),
            "processing_time_seconds": processing_time,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if verbose:
            print(f"Final Output:")
            print(f"  Processing time: {processing_time:.2f}s")
            print(f"  Total size: {len(json.dumps(output))} chars")
            print(f"{'='*80}\n")
        
        return output
    
    def batch_process(self, queries: List[str], verbose: bool = False) -> List[Dict[str, Any]]:
        """
        Process multiple queries with LLM-driven batch optimization.
        """
        # Let LLM decide batch processing strategy
        batch_prompt = f"""You need to process {len(queries)} queries:

{json.dumps(queries[:5], indent=2)}
{"..." if len(queries) > 5 else ""}

Decide the optimal batch processing strategy:
1. Should they be processed sequentially or can some be parallelized?
2. Should similar queries be grouped?
3. Are there any dependencies between queries?
4. What order would be most efficient?

Return JSON:
{{
  "strategy": "sequential|grouped|parallel",
  "reasoning": "why this strategy",
  "groupings": [[query_indices]] or null,
  "processing_order": [indices]
}}"""
        
        batch_response = self._call_llm(batch_prompt, adapt_params=False)
        strategy = self._extract_json(batch_response)
        
        if verbose:
            print(f"Batch Strategy: {strategy.get('strategy', 'sequential')}")
            print(f"Reasoning: {strategy.get('reasoning', 'N/A')}\n")
        
        # For now, implement sequential processing
        # (parallel/grouped would require more infrastructure)
        results = []
        for idx, query in enumerate(queries):
            if verbose:
                print(f"Processing {idx + 1}/{len(queries)}...")
            result = self.process_query(query, verbose=False)
            results.append(result)
        
        return results
    
    def evolve_system(self, feedback: str) -> None:
        """
        Allow LLM to evolve its own system instructions based on feedback.
        """
        evolution_prompt = f"""Current system instructions:
{self.system_instructions}

User feedback:
{feedback}

Based on this feedback, how should you evolve your system instructions?
Consider:
- What's working well that should be kept?
- What needs improvement?
- What new capabilities should be added?
- What should be removed or simplified?

Return your evolved system instructions as plain text."""
        
        response = self._call_llm(evolution_prompt, adapt_params=False)
        self.system_instructions = response
        
        print("System instructions evolved based on feedback.")
    
    def explain_reasoning(self, query: str, result: Dict[str, Any]) -> str:
        """
        Let LLM explain its own reasoning for a result.
        """
        explanation_prompt = f"""Explain your reasoning for this processing:

Query: "{query}"

Your Result:
{json.dumps(result, indent=2)[:1000]}...

Provide a clear explanation of:
1. Why you structured the output this way
2. What guided your decisions
3. What tradeoffs you made
4. What the user should understand about the results

Write your explanation naturally."""
        
        return self._call_llm(explanation_prompt, adapt_params=False)


# ======================================================
# EXAMPLE USAGE
# ======================================================

if __name__ == "__main__":
    PROJECT_ID = "your-gcp-project-id"  # Replace with your actual project ID
    
    # Initialize truly LLM-driven pipeline
    # The LLM will choose its own defaults and system instructions
    pipeline = AdaptiveLLMPipeline(
        project=PROJECT_ID,
        location=None,  # Let LLM decide
        model=None,     # Let LLM decide
        config={}
    )
    
    print("System Instructions (LLM-defined):")
    print("="*80)
    print(pipeline.system_instructions)
    print("="*80)
    print()
    
    # Test queries
    test_queries = [
        "Family history of diabetes",
        "Patient with severe chest pain radiating to left arm",
        "What's the weather today?",  # Non-clinical
        "45M with HTN, DM, presents with SOB on exertion"
    ]
    
    # Process each query - LLM has full autonomy
    for idx, query in enumerate(test_queries, 1):
        result = pipeline.process_query(query, verbose=True)
        
        # Save result
        output_filename = f"adaptive_result_{idx}.json"
        with open(output_filename, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"Saved to: {output_filename}\n")
        
        # Optional: Get LLM's explanation of its reasoning
        if result.get("is_clinical"):
            explanation = pipeline.explain_reasoning(query, result.get("result", {}))
            print("LLM's Explanation:")
            print("-" * 80)
            print(explanation)
            print("-" * 80)
            print()
    
    # Example: Batch processing with LLM-driven optimization
    print("\nBatch Processing Example:")
    print("="*80)
    batch_results = pipeline.batch_process(test_queries[:2], verbose=True)
    
    with open("batch_results.json", 'w') as f:
        json.dump(batch_results, f, indent=2)
    print("Saved batch results to: batch_results.json\n")
    
    # Example: System evolution based on feedback
    feedback = """
    The processing is good but sometimes too verbose.
    Can you be more concise while maintaining accuracy?
    Also, add confidence scores to clinical determinations.
    """
    
    print("Evolving system based on feedback...")
    pipeline.evolve_system(feedback)
    
    print("\nEvolved System Instructions:")
    print("="*80)
    print(pipeline.system_instructions)
    print("="*80)
