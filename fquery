"""
CUI Reduction System - Simple Version (expects pre-defined variables)
Expects these variables to be defined before running:
- project_id
- dataset_id  
- api_url
- mrrel_table
- cui_description_table
- cui_embeddings_table
- cui_narrower_table
"""

import os
import time
import numpy as np
import logging
import subprocess
import json
from typing import List, Set, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import threading

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ReductionStats:
    """Statistics for the reduction process"""
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    
    def to_dict(self):
        return asdict(self)


class CUIAPIClient:
    """Thread-safe client for GCP-based CUI extraction API"""
    
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0
    
    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3, max_retries: int = 3):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.max_retries = max_retries
        
        # Configure session with connection pooling
        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Get initial token
        self._update_gcp_token()
    
    def _update_gcp_token(self, force: bool = False):
        """Get GCP identity token with caching (thread-safe)"""
        with self._token_lock:
            current_time = time.time()
            
            if not force and self._cached_token and current_time < self._token_expiry:
                return self._cached_token
            
            try:
                result = subprocess.run(
                    ['gcloud', 'auth', 'print-identity-token'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    universal_newlines=True,
                    timeout=10,
                    check=False
                )
                
                if result.returncode != 0:
                    raise Exception(f"gcloud auth failed: {result.stderr}")
                
                identity_token = result.stdout.strip()
                if not identity_token:
                    raise Exception("Empty token received from gcloud")
                
                self._cached_token = {
                    "Authorization": f"Bearer {identity_token}",
                    "Content-Type": "application/json"
                }
                self._token_expiry = current_time + 3300  # 55 minutes
                
                logger.info("GCP authentication token updated")
                return self._cached_token
                
            except Exception as e:
                raise Exception(f"Failed to get GCP token: {str(e)}")
    
    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        """Extract CUIs from multiple texts in a single batch request"""
        if not texts:
            return set()
        
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = self._update_gcp_token()
        
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            
            if response.status_code == 401 and retry_auth:
                logger.warning("Token expired, refreshing...")
                self._update_gcp_token(force=True)
                return self.extract_cuis_batch(texts, retry_auth=False)
            
            response.raise_for_status()
            data = response.json()
            
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            
            logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
            return all_cuis
            
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return set()


class EnhancedCUIReducer:
    """Production-grade CUI reducer with error handling"""
    
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        mrrel_table: str = "MRREL",
        cui_description_table: str = "cui_descriptions",
        cui_embeddings_table: str = "cui_embeddings",
        cui_narrower_table: str = "cui_narrower_concepts",
        max_hierarchy_depth: int = 3,
        query_timeout: int = 300
    ):
        """Initialize with BigQuery client"""
        try:
            self.client = bigquery.Client(project=project_id)
        except Exception as e:
            raise Exception(f"Failed to initialize BigQuery client: {str(e)}")
        
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.mrrel_table = mrrel_table
        self.cui_description_table = cui_description_table
        self.cui_embeddings_table = cui_embeddings_table
        self.cui_narrower_table = cui_narrower_table
        self.max_hierarchy_depth = max_hierarchy_depth
        self.query_timeout = query_timeout
        
        # Cache
        self._hierarchy_cache = None
        self._ic_scores_cache = None
        self._description_cache = {}
        
        logger.info(f"Initialized CUIReducer for {project_id}.{dataset_id}")
    
    def reduce(
        self,
        input_cuis: List[str],
        target_reduction: float = 0.85,
        ic_threshold: Optional[float] = None,
        ic_percentile: float = 50.0,
        semantic_threshold: float = 0.88,
        use_semantic_clustering: bool = True,
        adaptive_threshold: bool = False
    ) -> Tuple[List[str], ReductionStats]:
        """Main reduction pipeline"""
        start_time = time.time()
        initial_count = len(input_cuis)
        
        if initial_count == 0:
            logger.warning("Empty input CUI list")
            return [], self._create_empty_stats(start_time)
        
        # Remove duplicates
        input_cuis = list(set(str(c).strip() for c in input_cuis if c and str(c).strip()))
        initial_count = len(input_cuis)
        
        logger.info(f"Starting reduction: {initial_count} CUIs → target {target_reduction*100:.1f}%")
        
        try:
            # Build hierarchy
            hierarchy = self._build_hierarchy_safe(input_cuis)
            ic_scores = self._compute_ic_scores_safe(hierarchy)
            
            # Determine IC threshold
            ic_threshold = self._determine_threshold(
                ic_scores, ic_threshold, ic_percentile, 
                adaptive_threshold, input_cuis, hierarchy, target_reduction
            )
            
            # Stage 1: IC-based rollup
            rolled_up_cuis = self._semantic_rollup_with_ic_safe(
                input_cuis, hierarchy, ic_scores, ic_threshold
            )
            after_rollup = len(rolled_up_cuis)
            rollup_reduction = self._safe_percentage(initial_count - after_rollup, initial_count)
            
            logger.info(f"Stage 1 complete: {after_rollup} CUIs ({rollup_reduction:.1f}% reduction)")
            
            # Stage 2: Semantic clustering
            clustering_reduction = 0.0
            if use_semantic_clustering and rollup_reduction < target_reduction * 100:
                final_cuis = self._semantic_clustering_safe(
                    rolled_up_cuis, ic_scores, semantic_threshold
                )
                final_count = len(final_cuis)
                clustering_reduction = self._safe_percentage(after_rollup - final_count, initial_count)
                logger.info(f"Stage 2 complete: {final_count} CUIs ({clustering_reduction:.1f}% additional)")
            else:
                final_cuis = rolled_up_cuis
                final_count = after_rollup
            
            # Calculate stats
            total_reduction = self._safe_percentage(initial_count - final_count, initial_count)
            processing_time = time.time() - start_time
            
            stats = ReductionStats(
                initial_count=initial_count,
                after_ic_rollup=after_rollup,
                final_count=final_count,
                ic_rollup_reduction_pct=rollup_reduction,
                semantic_clustering_reduction_pct=clustering_reduction,
                total_reduction_pct=total_reduction,
                processing_time=processing_time,
                ic_threshold_used=ic_threshold,
                hierarchy_size=len(hierarchy.get('all_cuis', set()))
            )
            
            logger.info(f"✓ Reduction complete: {initial_count} → {final_count} ({total_reduction:.1f}%)")
            return final_cuis, stats
            
        except Exception as e:
            logger.error(f"Critical error in reduction pipeline: {str(e)}")
            return input_cuis, self._create_error_stats(initial_count, start_time, str(e))
    
    def _build_hierarchy_safe(self, relevant_cuis: List[str]) -> Dict:
        """Build hierarchy with error handling"""
        if self._hierarchy_cache is not None:
            return self._hierarchy_cache
        
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(relevant_cuis)
        
        try:
            visited = set()
            frontier = set(relevant_cuis)
            
            for depth in range(self.max_hierarchy_depth):
                if not frontier:
                    break
                
                logger.info(f"Fetching hierarchy depth {depth + 1}: {len(frontier)} CUIs")
                
                query = f"""
                SELECT DISTINCT cui1, cui2, rel
                FROM `{self.project_id}.{self.dataset_id}.{self.mrrel_table}`
                WHERE (cui1 IN UNNEST(@frontier) OR cui2 IN UNNEST(@frontier))
                  AND rel IN ('PAR', 'CHD')
                LIMIT 100000
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("frontier", "STRING", list(frontier))
                    ]
                )
                
                try:
                    query_job = self.client.query(query, job_config=job_config)
                    # FIXED: Use timeout in result() method, not in job_config
                    df = query_job.result(timeout=self.query_timeout).to_dataframe()
                    
                    if len(df) == 0:
                        logger.warning(f"No relationships found at depth {depth + 1}")
                        break
                    
                    logger.info(f"  Retrieved {len(df)} relationships")
                    
                    next_frontier = set()
                    for _, row in df.iterrows():
                        cui1, cui2, rel = str(row['cui1']), str(row['cui2']), str(row['rel'])
                        
                        if rel == 'PAR':
                            parent_to_children[cui1].append(cui2)
                            child_to_parents[cui2].append(cui1)
                        elif rel == 'CHD':
                            parent_to_children[cui2].append(cui1)
                            child_to_parents[cui1].append(cui2)
                        
                        all_cuis.update([cui1, cui2])
                        
                        if cui1 not in visited:
                            next_frontier.add(cui1)
                        if cui2 not in visited:
                            next_frontier.add(cui2)
                    
                    visited.update(frontier)
                    frontier = next_frontier - visited
                    
                except Exception as e:
                    logger.warning(f"MRREL query failed at depth {depth + 1}: {str(e)}")
                    break
            
        except Exception as e:
            logger.error(f"Failed to build hierarchy: {str(e)}")
        
        hierarchy = {
            'child_to_parents': dict(child_to_parents),
            'parent_to_children': dict(parent_to_children),
            'all_cuis': all_cuis
        }
        
        self._hierarchy_cache = hierarchy
        logger.info(f"Built hierarchy: {len(all_cuis)} CUIs total")
        
        return hierarchy
    
    def _compute_ic_scores_safe(self, hierarchy: Dict) -> Dict[str, float]:
        """Compute IC scores with error handling"""
        if self._ic_scores_cache is not None:
            return self._ic_scores_cache
        
        parent_to_children = hierarchy.get('parent_to_children', {})
        all_cuis = hierarchy.get('all_cuis', set())
        total = len(all_cuis)
        
        if total == 0:
            return {}
        
        try:
            descendant_counts = {}
            
            def count_descendants(cui: str, visited: Set[str] = None) -> int:
                if visited is None:
                    visited = set()
                if cui in visited or cui in descendant_counts:
                    return descendant_counts.get(cui, 0)
                
                visited.add(cui)
                children = parent_to_children.get(cui, [])
                count = len(children)
                
                for child in children:
                    count += count_descendants(child, visited)
                
                descendant_counts[cui] = count
                return count
            
            logger.info("Computing IC scores...")
            for cui in all_cuis:
                if cui not in descendant_counts:
                    try:
                        count_descendants(cui)
                    except RecursionError:
                        logger.warning(f"Recursion limit for {cui}")
                        descendant_counts[cui] = 0
            
            # Compute IC
            ic_scores = {}
            for cui in all_cuis:
                desc_count = descendant_counts.get(cui, 0)
                ic = -np.log((desc_count + 1) / total)
                ic_scores[cui] = max(0.0, ic)
            
            self._ic_scores_cache = ic_scores
            
            if ic_scores:
                values = list(ic_scores.values())
                logger.info(f"IC scores: {len(ic_scores)} CUIs, range [{min(values):.2f}, {max(values):.2f}]")
            
            return ic_scores
            
        except Exception as e:
            logger.error(f"Failed to compute IC scores: {str(e)}")
            return {cui: 5.0 for cui in all_cuis}
    
    def _determine_threshold(
        self, 
        ic_scores: Dict, 
        ic_threshold: Optional[float],
        ic_percentile: float,
        adaptive: bool,
        input_cuis: List[str],
        hierarchy: Dict,
        target_reduction: float
    ) -> float:
        """Determine IC threshold"""
        try:
            if ic_threshold is not None:
                return float(ic_threshold)
            
            ic_values = list(ic_scores.values())
            if not ic_values:
                return 5.0
            
            if adaptive:
                return self._find_adaptive_threshold_safe(
                    input_cuis, hierarchy, ic_scores, target_reduction
                )
            else:
                return float(np.percentile(ic_values, ic_percentile))
        except Exception as e:
            logger.warning(f"Threshold determination failed: {str(e)}, using default 5.0")
            return 5.0
    
    def _semantic_rollup_with_ic_safe(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        ic_threshold: float
    ) -> List[str]:
        """Semantic rollup with error handling"""
        try:
            child_to_parents = hierarchy.get('child_to_parents', {})
            rolled_up = {}
            
            for cui in cui_list:
                try:
                    # Get ancestors with BFS
                    ancestors = []
                    visited = set()
                    queue = deque([cui])
                    
                    while queue and len(visited) < 100:
                        current = queue.popleft()
                        if current in visited:
                            continue
                        visited.add(current)
                        
                        for parent in child_to_parents.get(current, []):
                            if parent not in visited:
                                ancestors.append(parent)
                                queue.append(parent)
                    
                    # Find LIA
                    candidates = [cui] + ancestors
                    valid = [c for c in candidates if ic_scores.get(c, 0) >= ic_threshold]
                    
                    if valid:
                        rolled_up[cui] = min(valid, key=lambda c: ic_scores.get(c, float('inf')))
                    else:
                        rolled_up[cui] = cui
                        
                except Exception as e:
                    logger.debug(f"Rollup failed for {cui}: {str(e)}")
                    rolled_up[cui] = cui
            
            return list(set(rolled_up.values()))
            
        except Exception as e:
            logger.error(f"Semantic rollup failed: {str(e)}")
            return cui_list
    
    def _semantic_clustering_safe(
        self,
        cui_list: List[str],
        ic_scores: Dict[str, float],
        similarity_threshold: float
    ) -> List[str]:
        """Semantic clustering with error handling"""
        if len(cui_list) <= 1:
            return cui_list
        
        try:
            logger.info(f"Fetching embeddings for {len(cui_list)} CUIs...")
            
            query = f"""
            SELECT REF_CUI as cui, REF_Embedding as embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
            WHERE REF_CUI IN UNNEST(@cuis)
            LIMIT 10000
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)
                ]
            )
            
            # FIXED: Use timeout in result() method
            query_job = self.client.query(query, job_config=job_config)
            df = query_job.result(timeout=60).to_dataframe()
            
            if len(df) == 0:
                logger.warning("No embeddings found, skipping clustering")
                return cui_list
            
            embeddings = np.vstack(df['embedding'].values)
            cuis = df['cui'].values
            
            logger.info(f"Clustering {len(cuis)} CUIs...")
            
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - similarity_threshold,
                metric='cosine',
                linkage='average'
            )
            
            labels = clustering.fit_predict(embeddings)
            
            # Select representatives
            final_cuis = []
            for cluster_id in np.unique(labels):
                cluster_cuis = cuis[labels == cluster_id].tolist()
                if len(cluster_cuis) == 1:
                    final_cuis.append(cluster_cuis[0])
                else:
                    rep = min(cluster_cuis, key=lambda c: ic_scores.get(c, float('inf')))
                    final_cuis.append(rep)
            
            logger.info(f"Clustered into {len(final_cuis)} groups")
            return final_cuis
            
        except Exception as e:
            logger.error(f"Clustering failed: {str(e)}")
            return cui_list
    
    def _find_adaptive_threshold_safe(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        target_reduction: float
    ) -> float:
        """Find adaptive threshold"""
        try:
            ic_values = sorted(ic_scores.values())
            if not ic_values:
                return 5.0
            
            for percentile in [50, 40, 30, 25, 20, 15, 10]:
                try:
                    threshold = float(np.percentile(ic_values, percentile))
                    rolled_up = self._semantic_rollup_with_ic_safe(
                        cui_list, hierarchy, ic_scores, threshold
                    )
                    reduction = 1 - len(rolled_up) / len(cui_list)
                    
                    if reduction >= target_reduction * 0.9:
                        logger.info(f"Adaptive threshold: {threshold:.3f} (p{percentile})")
                        return threshold
                except Exception:
                    continue
            
            return float(np.percentile(ic_values, 10))
        except Exception as e:
            logger.warning(f"Adaptive threshold failed: {str(e)}")
            return 5.0
    
    def get_cui_descriptions(self, cui_list: List[str]) -> Dict[str, str]:
        """Retrieve descriptions with caching"""
        if not cui_list:
            return {}
        
        uncached = [c for c in cui_list if c not in self._description_cache]
        
        if uncached:
            try:
                query = f"""
                SELECT CUI as cui, Definition as description
                FROM `{self.project_id}.{self.dataset_id}.{self.cui_description_table}`
                WHERE CUI IN UNNEST(@cuis)
                LIMIT 50000
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("cuis", "STRING", uncached)
                    ]
                )
                
                # FIXED: Use timeout in result() method
                query_job = self.client.query(query, job_config=job_config)
                df = query_job.result(timeout=30).to_dataframe()
                new_descriptions = dict(zip(df['cui'], df['description']))
                self._description_cache.update(new_descriptions)
                
            except Exception as e:
                logger.error(f"Failed to fetch descriptions: {str(e)}")
        
        return {cui: self._description_cache.get(cui, "N/A") for cui in cui_list}
    
    def get_ic_scores(self, cui_list: List[str]) -> Dict[str, float]:
        """Get IC scores for CUIs"""
        if self._ic_scores_cache is None:
            return {cui: 0.0 for cui in cui_list}
        return {cui: self._ic_scores_cache.get(cui, 0.0) for cui in cui_list}
    
    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        """Safe percentage calculation"""
        return (numerator / denominator * 100) if denominator > 0 else 0.0
    
    @staticmethod
    def _create_empty_stats(start_time: float) -> ReductionStats:
        """Create stats for empty input"""
        return ReductionStats(
            initial_count=0,
            after_ic_rollup=0,
            final_count=0,
            ic_rollup_reduction_pct=0.0,
            semantic_clustering_reduction_pct=0.0,
            total_reduction_pct=0.0,
            processing_time=time.time() - start_time,
            ic_threshold_used=0.0
        )
    
    @staticmethod
    def _create_error_stats(initial_count: int, start_time: float, error: str) -> ReductionStats:
        """Create stats for error scenario"""
        logger.error(f"Returning original CUIs due to error: {error}")
        return ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=initial_count,
            final_count=initial_count,
            ic_rollup_reduction_pct=0.0,
            semantic_clustering_reduction_pct=0.0,
            total_reduction_pct=0.0,
            processing_time=time.time() - start_time,
            ic_threshold_used=0.0
        )


# Main function to use the classes
def run_cui_reduction(
    texts: List[str],
    project_id: str,
    dataset_id: str,
    api_url: str,
    mrrel_table: str = "MRREL",
    cui_description_table: str = "cui_descriptions",
    cui_embeddings_table: str = "cui_embeddings",
    cui_narrower_table: str = "cui_narrower_concepts",
    target_reduction: float = 0.85,
    ic_percentile: float = 50.0,
    semantic_threshold: float = 0.88,
    use_semantic_clustering: bool = True,
    adaptive_threshold: bool = False
) -> Tuple[List[str], Dict[str, str], Optional[ReductionStats]]:
    """
    Run CUI reduction on texts
    
    Args:
        texts: List of medical texts
        project_id: GCP project ID
        dataset_id: BigQuery dataset ID
        api_url: NER API endpoint URL
        Other parameters for tables and reduction settings
    
    Returns:
        Tuple of (reduced_cuis, descriptions, stats)
    """
    try:
        # Initialize API client
        api_client = CUIAPIClient(
            api_base_url=api_url,
            timeout=60,
            top_k=3
        )
        
        # Initialize reducer
        cui_reducer = EnhancedCUIReducer(
            project_id=project_id,
            dataset_id=dataset_id,
            mrrel_table=mrrel_table,
            cui_description_table=cui_description_table,
            cui_embeddings_table=cui_embeddings_table,
            cui_narrower_table=cui_narrower_table,
            max_hierarchy_depth=3,
            query_timeout=300
        )
        
        # Extract CUIs from texts
        logger.info(f"Extracting CUIs from {len(texts)} texts...")
        initial_cuis = api_client.extract_cuis_batch(texts)
        
        if not initial_cuis:
            logger.warning("No CUIs extracted from texts")
            return [], {}, None
        
        # Reduce CUIs
        start_time = time.time()
        reduced_cuis, stats = cui_reducer.reduce(
            list(initial_cuis),
            target_reduction=target_reduction,
            ic_percentile=ic_percentile,
            semantic_threshold=semantic_threshold,
            use_semantic_clustering=use_semantic_clustering,
            adaptive_threshold=adaptive_threshold
        )
        
        # Add API call time to stats
        stats.api_call_time = time.time() - start_time - stats.processing_time
        
        # Get descriptions
        logger.info("Fetching descriptions...")
        descriptions = cui_reducer.get_cui_descriptions(reduced_cuis)
        
        return reduced_cuis, descriptions, stats
        
    except Exception as e:
        logger.error(f"Pipeline error: {str(e)}")
        return [], {}, None


# Example usage (if running as standalone script)
if __name__ == "__main__":
    # These variables should be defined before running
    # Example:
    # project_id = "my-project"
    # dataset_id = "umls_data"
    # api_url = "https://my-api.run.app/extract"
    # mrrel_table = "MRREL"
    # cui_description_table = "cui_descriptions"
    # cui_embeddings_table = "cui_embeddings"
    # cui_narrower_table = "cui_narrower_concepts"
    
    texts = ["History of Type 2 Diabetes Mellitus and hypertension."]
    
    reduced_cuis, descriptions, stats = run_cui_reduction(
        texts=texts,
        project_id=project_id,  # Should be defined
        dataset_id=dataset_id,  # Should be defined
        api_url=api_url,  # Should be defined
        mrrel_table=mrrel_table,  # Should be defined
        cui_description_table=cui_description_table,  # Should be defined
        cui_embeddings_table=cui_embeddings_table,  # Should be defined
        cui_narrower_table=cui_narrower_table  # Should be defined
    )
    
    if stats:
        print(f"Reduction complete: {stats.initial_count} → {stats.final_count} ({stats.total_reduction_pct:.1f}%)")
