# ============================================================
# FINAL CLINICALLY SAFE CUI ABSTRACTION PIPELINE
# ============================================================

import asyncio
import aiohttp
import subprocess
import re
import time
import threading
import logging
import math
from typing import List, Dict, Set, Tuple
from collections import defaultdict
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd
import networkx as nx
import requests
from requests.adapters import HTTPAdapter, Retry
from google.cloud import bigquery
from sklearn.metrics.pairwise import cosine_distances
import nest_asyncio

nest_asyncio.apply()

# ------------------------------------------------------------
# Logging
# ------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------
# Utilities
# ------------------------------------------------------------
def extract_base_cui(cui: str) -> str | None:
    if not cui:
        return None
    m = re.match(r"^(C\d{7})", cui.upper())
    return m.group(1) if m else None

def safe_percentage(n, d):
    return (n / d * 100) if d else 0.0

# ------------------------------------------------------------
# GCP Token Provider
# ------------------------------------------------------------
class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force=False):
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token

            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)

            token = proc.stdout.strip()
            cls._token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            cls._expiry = now + 3300
            return cls._token

# ------------------------------------------------------------
# Stats
# ------------------------------------------------------------
@dataclass
class ReductionStats:
    initial_count: int
    after_ic: int
    final_count: int
    total_reduction_pct: float
    coverage_score: float
    processing_time: float

    def to_dict(self):
        return asdict(self)

# ------------------------------------------------------------
# CUI Extraction API Client
# ------------------------------------------------------------
class CUIAPIClient:
    def __init__(self, api_url: str, top_k: int = 3):
        self.api_url = api_url
        self.top_k = top_k
        self.session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429,500,502,503])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract_cuis(self, texts: List[str]) -> Set[str]:
        r = self.session.post(
            self.api_url,
            headers=GCPTokenProvider.get_headers(),
            json={"query_texts": texts, "top_k": self.top_k},
            timeout=60
        )
        r.raise_for_status()
        out = set()
        for v in r.json().values():
            out.update(map(str, v))
        return out

# ------------------------------------------------------------
# Vocabulary Filter
# ------------------------------------------------------------
def filter_allowed_cuis(cuis: Set[str], project: str, dataset: str) -> Set[str]:
    if not cuis:
        return set()

    client = bigquery.Client(project=project)
    bases = list({extract_base_cui(c) for c in cuis if extract_base_cui(c)})

    query = f"""
    SELECT DISTINCT CUI
    FROM `{project}.{dataset}.MRCONSO`
    WHERE CUI IN UNNEST(@cuis)
      AND SAB IN ('ICD9CM','ICD10','ICD10CM','SNOMEDCT_US','LOINC')
    """

    df = client.query(
        query,
        job_config=bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", bases)
            ]
        )
    ).to_dataframe()

    allowed = set(df["CUI"])
    return {c for c in cuis if extract_base_cui(c) in allowed}

# ------------------------------------------------------------
# Clinical Reducer
# ------------------------------------------------------------
class ClinicalCUIReducer:
    """
    TEXT-FAITHFUL CLINICAL ABSTRACTION ENGINE
    """

    def __init__(
        self,
        project: str,
        dataset: str,
        subnet_api_url: str,
        mrsty_table: str,
        embedding_table: str
    ):
        self.client = bigquery.Client(project=project)
        self.project = project
        self.dataset = dataset
        self.subnet_api_url = subnet_api_url
        self.mrsty_table = mrsty_table
        self.embedding_table = embedding_table

        self.embedding_cache = {}
        self.hierarchy_cache = {}

    # --------------------------------------------------------
    # Semantic grouping (MRSTY, deterministic)
    # --------------------------------------------------------
    def _semantic_groups(self, cuis: Set[str]) -> Dict[str, Set[str]]:
        query = f"""
        SELECT CUI, STY
        FROM `{self.project}.{self.dataset}.{self.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        df = self.client.query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))
                ]
            )
        ).to_dataframe()

        groups = defaultdict(set)
        for _, r in df.iterrows():
            groups[r.STY].add(r.CUI)

        return groups if groups else {"UNKNOWN": set(cuis)}

    # --------------------------------------------------------
    # Batched, parallel, unbounded hierarchy fetch (READ-ONLY)
    # --------------------------------------------------------
    async def _fetch_hierarchy(self, base_cuis: Set[str]) -> nx.DiGraph:
        key = frozenset(base_cuis)
        if key in self.hierarchy_cache:
            return self.hierarchy_cache[key]

        graph = nx.DiGraph()
        headers = GCPTokenProvider.get_headers()
        batch_size = 100
        semaphore = asyncio.Semaphore(10)

        async def fetch_batch(session, batch):
            async with semaphore:
                for attempt in range(3):
                    try:
                        async with session.post(
                            f"{self.subnet_api_url}/subnet/",
                            headers=headers,
                            json={"cuis": batch, "cross_context": False}
                        ) as resp:
                            if resp.status == 401:
                                headers.update(GCPTokenProvider.get_headers(force=True))
                                continue
                            resp.raise_for_status()
                            data = await resp.json()
                            return data.get("output", [[], []])[1]
                    except Exception:
                        await asyncio.sleep(2 ** attempt)
                return []

        timeout = aiohttp.ClientTimeout(total=120)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            tasks = []
            bases = list(base_cuis)
            for i in range(0, len(bases), batch_size):
                tasks.append(fetch_batch(session, bases[i:i + batch_size]))
            results = await asyncio.gather(*tasks)

        for edges in results:
            for p, c in edges:
                p = extract_base_cui(str(p))
                c = extract_base_cui(str(c))
                if p and c:
                    graph.add_edge(p, c)

        self.hierarchy_cache[key] = graph
        return graph

    # --------------------------------------------------------
    # IC computation (FULL TRANSITIVE CLOSURE)
    # --------------------------------------------------------
    def _compute_ic(self, group_cuis: Set[str], graph: nx.DiGraph) -> Dict[str, float]:
        N = len(group_cuis)
        ic = {}
        for c in group_cuis:
            descendants = nx.descendants(graph, c) & group_cuis
            ic[c] = -math.log((len(descendants) + 1) / N)
        return ic

    # --------------------------------------------------------
    # Embedding fetch (OBSERVED ONLY)
    # --------------------------------------------------------
    def _fetch_embeddings(self, cuis: Set[str]):
        assert cuis, "Embedding fetch called with empty set"

        bases = list({extract_base_cui(c) for c in cuis})
        query = f"""
        SELECT REF_CUI, REF_Embedding
        FROM `{self.project}.{self.dataset}.{self.embedding_table}`
        WHERE REF_CUI IN UNNEST(@cuis)
        """

        df = self.client.query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", bases)
                ]
            )
        ).to_dataframe()

        base_map = {
            r.REF_CUI: np.asarray(r.REF_Embedding, dtype=np.float32)
            for _, r in df.iterrows()
        }

        for c in cuis:
            b = extract_base_cui(c)
            if b in base_map:
                self.embedding_cache[c] = base_map[b]

    # --------------------------------------------------------
    # Similarity pruning (HIERARCHY DOMINANT)
    # --------------------------------------------------------
    def _similarity_prune(self, cuis: Set[str], graph: nx.DiGraph) -> Set[str]:
        X, labels = [], []
        for c in cuis:
            if c in self.embedding_cache:
                X.append(self.embedding_cache[c])
                labels.append(c)

        if len(X) < 2:
            return set(cuis)

        X = np.vstack(X)
        sim = 1 - cosine_distances(X, X)

        dropped = set()
        for i, c1 in enumerate(labels):
            for j in range(i + 1, len(labels)):
                c2 = labels[j]
                if sim[i, j] >= 0.88:
                    if nx.has_path(graph, c1, c2):
                        dropped.add(c2)
                    elif nx.has_path(graph, c2, c1):
                        dropped.add(c1)

        return set(cuis) - dropped

    # --------------------------------------------------------
    # Coverage (OBSERVED ONLY)
    # --------------------------------------------------------
    def _coverage(self, observed: Set[str], retained: Set[str], graph: nx.DiGraph) -> float:
        covered = set()
        for c in retained:
            covered |= nx.descendants(graph, c)
        return len(covered & observed) / len(observed) if observed else 0.0

    # --------------------------------------------------------
    # FULL REDUCTION
    # --------------------------------------------------------
    def reduce(self, input_cuis: Set[str]) -> Tuple[List[str], ReductionStats]:
        start = time.time()
        observed_cuis = set(input_cuis)

        semantic_groups = self._semantic_groups(observed_cuis)
        retained_all = set()

        for _, group_cuis in semantic_groups.items():
            bases = {extract_base_cui(c) for c in group_cuis if extract_base_cui(c)}
            graph = asyncio.run(self._fetch_hierarchy(bases))

            ic = self._compute_ic(group_cuis, graph)
            threshold = np.percentile(list(ic.values()), 50)
            selected = {c for c, v in ic.items() if v >= threshold}

            # ðŸ”´ CLINICAL SAFETY GATE
            selected &= observed_cuis

            if not selected:
                continue

            self._fetch_embeddings(selected)
            final_group = self._similarity_prune(selected, graph)

            retained_all |= final_group

        # ðŸ”’ HARD SAFETY ASSERTION
        assert retained_all.issubset(observed_cuis), \
            "CLINICAL ERROR: non-observed CUI leaked"

        stats = ReductionStats(
            initial_count=len(observed_cuis),
            after_ic=len(retained_all),
            final_count=len(retained_all),
            total_reduction_pct=safe_percentage(
                len(observed_cuis) - len(retained_all), len(observed_cuis)
            ),
            coverage_score=self._coverage(observed_cuis, retained_all, graph),
            processing_time=time.time() - start
        )

        return sorted(retained_all), stats

# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------
def main():
    PROJECT_ID = "your_project"
    DATASET_ID = "your_dataset"
    CUI_API_URL = "your_cui_api"
    SUBNET_API_URL = "your_subnet_api"
    MRSTY_TABLE = "MRSTY"
    EMBEDDING_TABLE = "EMBEDDINGS"

    TEXTS = ["diabetes mellitus type 2 with complications"]

    api = CUIAPIClient(CUI_API_URL)
    extracted = api.extract_cuis(TEXTS)
    filtered = filter_allowed_cuis(extracted, PROJECT_ID, DATASET_ID)

    reducer = ClinicalCUIReducer(
        project=PROJECT_ID,
        dataset=DATASET_ID,
        subnet_api_url=SUBNET_API_URL,
        mrsty_table=MRSTY_TABLE,
        embedding_table=EMBEDDING_TABLE
    )

    final_cuis, stats = reducer.reduce(filtered)
    logger.info(f"FINAL CUIs: {final_cuis}")
    logger.info(f"STATS: {stats.to_dict()}")

if __name__ == "__main__":
    main()
