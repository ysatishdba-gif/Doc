# ============================================================
# CLINICALLY SAFE CUI REDUCTION PIPELINE (PICKLE-BASED HIERARCHY)
# ============================================================

import subprocess
import re
import logging
import json
from typing import List, Dict, Set, Tuple
from collections import defaultdict
import time
from dataclasses import dataclass, asdict
import threading
import pickle
import numpy as np

import networkx as nx
from google.cloud import bigquery
from sklearn.metrics.pairwise import cosine_distances
import requests
from requests.adapters import HTTPAdapter, Retry

# --------------------- Logging ---------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --------------------- Utilities ---------------------
def extract_base_cui(cui: str) -> str | None:
    if not cui:
        return None
    m = re.match(r"^(C\d{7})", cui.upper())
    return m.group(1) if m else None

def safe_percentage(n, d):
    return (n / d * 100) if d else 0.0

# --------------------- GCP Token ---------------------
class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force=False):
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token

            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)

            token = proc.stdout.strip()
            cls._token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            cls._expiry = now + 3300
            return cls._token

# --------------------- Stats ---------------------
@dataclass
class ReductionStats:
    initial_count: int
    after_ic_rollup: int
    after_similarity_stage1: int
    after_similarity_stage2: int
    after_similarity_stage3: int
    final_count: int
    ic_rollup_reduction_pct: float
    similarity_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    coverage_score: float
    group_stats: Dict

    def to_dict(self):
        return asdict(self)

# --------------------- Vocabulary Filter ---------------------
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []

    client = bigquery.Client(project=project_id)
    bases = list({extract_base_cui(c) for c in cuis if extract_base_cui(c)})

    query = f"""
    SELECT DISTINCT CUI
    FROM `{project_id}.{dataset_id}.MRCONSO`
    WHERE CUI IN UNNEST(@cuis)
      AND SAB IN ('ICD10','ICD10CM','ICD9CM','SNOMEDCT_US','LOINC')
    """

    df = client.query(
        query,
        job_config=bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", bases)
            ]
        )
    ).to_dataframe()

    allowed = set(df["CUI"])
    return [c for c in cuis if extract_base_cui(c) in allowed]

# --------------------- CUI API Client ---------------------
class CUIAPIClient:
    def __init__(self, api_url: str, top_k: int = 3):
        self.api_url = api_url
        self.top_k = top_k
        self.session = requests.Session()
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429,500,502,503])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract_cuis_batch(self, texts: List[str]) -> Set[str]:
        r = self.session.post(
            self.api_url,
            headers=GCPTokenProvider.get_headers(),
            json={"query_texts": texts, "top_k": self.top_k},
            timeout=60
        )
        r.raise_for_status()
        out = set()
        for v in r.json().values():
            out.update(map(str, v))
        return out

# --------------------- Enhanced Reducer ---------------------
class EnhancedCUIReducer:
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        embedding_table: str,
        mrsty_table: str,
        umls_network: nx.DiGraph,
        similarity_thresholds=(0.95, 0.90, 0.85)
    ):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.embedding_table = embedding_table
        self.mrsty_table = mrsty_table
        self.graph = umls_network
        self.similarity_thresholds = similarity_thresholds
        self.embedding_cache = {}

    # ---------------- Semantic Groups ----------------
    def _semantic_groups(self, cuis: List[str]) -> Dict[str, List[str]]:
        query = f"""
        SELECT CUI, STY
        FROM `{self.project_id}.{self.dataset_id}.{self.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        df = self.client.query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", cuis)
                ]
            )
        ).to_dataframe()

        groups = defaultdict(list)
        for _, r in df.iterrows():
            groups[r.STY].append(r.CUI)

        return groups if groups else {"UNKNOWN": cuis}

    # ---------------- IC Computation ----------------
    def _compute_ic(self, group: List[str]) -> Dict[str, float]:
        N = len(group)
        group_set = set(group)
        ic = {}

        for c in group:
            base = extract_base_cui(c)
            if base not in self.graph:
                ic[c] = 0.0
                continue

            descendants = {
                x for x in group_set
                if extract_base_cui(x) in nx.descendants(self.graph, base)
            }
            ic[c] = -np.log((len(descendants) + 1) / N)

        return ic

    # ---------------- Embeddings ----------------
    def _fetch_embeddings(self, cuis: Set[str]):
        uncached = [c for c in cuis if c not in self.embedding_cache]
        if not uncached:
            return

        query = f"""
        SELECT CUI, embedding
        FROM `{self.project_id}.{self.dataset_id}.{self.embedding_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        df = self.client.query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", uncached)
                ]
            )
        ).to_dataframe()

        for _, r in df.iterrows():
            if r.embedding is not None:
                self.embedding_cache[r.CUI] = np.asarray(r.embedding, dtype=np.float32)

    # ---------------- Similarity Pruning ----------------
    def _similarity_prune(self, cuis: Set[str]) -> Set[str]:
        X, labels = [], []
        for c in cuis:
            if c in self.embedding_cache:
                X.append(self.embedding_cache[c])
                labels.append(c)

        if len(X) < 2:
            return cuis

        X = np.vstack(X)
        sim = 1 - cosine_distances(X, X)

        dropped = set()
        for i, c1 in enumerate(labels):
            b1 = extract_base_cui(c1)
            for j in range(i + 1, len(labels)):
                if sim[i, j] >= self.similarity_thresholds[2]:
                    c2 = labels[j]
                    b2 = extract_base_cui(c2)
                    if nx.has_path(self.graph, b1, b2):
                        dropped.add(c2)
                    elif nx.has_path(self.graph, b2, b1):
                        dropped.add(c1)

        return cuis - dropped

    # ---------------- Coverage ----------------
    def _coverage(self, observed: Set[str], retained: Set[str]) -> float:
        covered = set()
        for c in retained:
            base = extract_base_cui(c)
            if base in self.graph:
                covered |= nx.descendants(self.graph, base)
        return len({c for c in observed if extract_base_cui(c) in covered}) / len(observed)

    # ---------------- Reduce ----------------
    def reduce(self, input_cuis: List[str]) -> Tuple[List[str], ReductionStats]:
        start = time.time()
        observed = set(input_cuis)
        groups = self._semantic_groups(input_cuis)

        retained_all = set()
        group_stats = {}

        for sty, group in groups.items():
            ic = self._compute_ic(group)
            threshold = np.percentile(list(ic.values()), 50)
            selected = {c for c, v in ic.items() if v >= threshold}

            # ðŸ”´ CLINICAL SAFETY
            selected &= observed
            self._fetch_embeddings(selected)
            selected = self._similarity_prune(selected)

            retained_all |= selected
            group_stats[sty] = {
                "original": len(group),
                "retained": len(selected)
            }

        assert retained_all.issubset(observed)

        stats = ReductionStats(
            initial_count=len(observed),
            after_ic_rollup=len(retained_all),
            after_similarity_stage1=len(retained_all),
            after_similarity_stage2=len(retained_all),
            after_similarity_stage3=len(retained_all),
            final_count=len(retained_all),
            ic_rollup_reduction_pct=safe_percentage(len(observed)-len(retained_all), len(observed)),
            similarity_reduction_pct=0.0,
            total_reduction_pct=safe_percentage(len(observed)-len(retained_all), len(observed)),
            processing_time=time.time()-start,
            coverage_score=self._coverage(observed, retained_all),
            group_stats=group_stats
        )

        return sorted(retained_all), stats

# --------------------- MAIN ---------------------
if __name__ == "__main__":

    # Load pickle
    # with open("UMLS_NETWORK_OBJ.pkl", "rb") as f:
    #     UMLS_NETWORK_OBJ = pickle.load(f)

    project_id = project_id
    dataset_id = dataset
    embedding_table = embedding_table
    mrsty_table = "MRSTY"
    api_url = url

    texts = ["grams"]

    api = CUIAPIClient(api_url)
    extracted = api.extract_cuis_batch(texts)
    filtered = filter_allowed_cuis(extracted, project_id, dataset_id)

    reducer = EnhancedCUIReducer(
        project_id=project_id,
        dataset_id=dataset_id,
        embedding_table=embedding_table,
        mrsty_table=mrsty_table,
        umls_network=UMLS_NETWORK_OBJ
    )

    final_cuis, stats = reducer.reduce(filtered)

    logger.info(json.dumps(stats.to_dict(), indent=2))
    logger.info(f"FINAL CUIs: {final_cuis}")
