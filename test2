"""
Automatic Semantic Type Compatibility Learning
Discovers compatibility rules from data instead of manual definition
"""

import pickle
import numpy as np
import networkx as nx
from typing import Dict, List, Tuple, Set, Optional
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd


class AutoSemanticTypeValidator:
    """
    Automatically learns semantic type compatibility from data
    Uses three strategies:
    1. Graph co-occurrence (types appearing together in hierarchies)
    2. Embedding similarity (types with similar embeddings are compatible)
    3. Transitivity (if A->B and B->C compatible, then A->C compatible)
    """
    
    def __init__(
        self,
        cui_semantic_types: Optional[Dict[str, Set[str]]] = None,
        learn_from_graph: bool = True,
        learn_from_embeddings: bool = True,
        similarity_threshold: float = 0.6,
        min_cooccurrence: int = 5
    ):
        """
        Initialize automatic validator
        
        Args:
            cui_semantic_types: Dict mapping CUI -> set of semantic types
            learn_from_graph: Learn compatibility from graph structure
            learn_from_embeddings: Learn compatibility from embedding similarity
            similarity_threshold: Min embedding similarity to consider compatible
            min_cooccurrence: Min co-occurrences to establish compatibility
        """
        self.cui_semantic_types = cui_semantic_types or {}
        self.learn_from_graph = learn_from_graph
        self.learn_from_embeddings = learn_from_embeddings
        self.similarity_threshold = similarity_threshold
        self.min_cooccurrence = min_cooccurrence
        
        # Learned compatibility
        self.compatible_types = {}
        self.compatibility_scores = defaultdict(lambda: defaultdict(float))
        self.is_learned = False
    
    def learn_from_graph_structure(
        self,
        graph: nx.DiGraph,
        max_depth: int = 3
    ):
        """
        Learn compatibility from graph co-occurrence
        If two types appear in parent-child relationships, they're compatible
        """
        print("Learning semantic type compatibility from graph structure...")
        
        cooccurrence = defaultdict(lambda: defaultdict(int))
        
        # For each CUI, look at its ancestors and descendants
        for cui in graph.nodes():
            if cui not in self.cui_semantic_types:
                continue
            
            cui_types = self.cui_semantic_types[cui]
            
            # Get ancestors and descendants
            neighbors = set()
            
            # Ancestors (up to max_depth)
            try:
                ancestors = nx.ancestors(graph, cui)
                # Limit depth
                close_ancestors = set()
                for anc in ancestors:
                    try:
                        depth = nx.shortest_path_length(graph, cui, anc)
                        if depth <= max_depth:
                            close_ancestors.add(anc)
                    except:
                        pass
                neighbors.update(close_ancestors)
            except:
                pass
            
            # Descendants (up to max_depth)
            try:
                descendants = nx.descendants(graph, cui)
                close_descendants = set()
                for desc in descendants:
                    try:
                        depth = nx.shortest_path_length(graph, desc, cui)
                        if depth <= max_depth:
                            close_descendants.add(desc)
                    except:
                        pass
                neighbors.update(close_descendants)
            except:
                pass
            
            # Record co-occurrences
            for neighbor in neighbors:
                if neighbor not in self.cui_semantic_types:
                    continue
                
                neighbor_types = self.cui_semantic_types[neighbor]
                
                # Record all type pairs
                for t1 in cui_types:
                    for t2 in neighbor_types:
                        cooccurrence[t1][t2] += 1
                        cooccurrence[t2][t1] += 1  # Symmetric
        
        # Convert to compatibility
        for t1, t2_counts in cooccurrence.items():
            compatible_set = set()
            for t2, count in t2_counts.items():
                if count >= self.min_cooccurrence:
                    compatible_set.add(t2)
                    # Store score (normalized by max count)
                    max_count = max(t2_counts.values())
                    self.compatibility_scores[t1][t2] = count / max_count
            
            if compatible_set:
                self.compatible_types[t1] = compatible_set
        
        print(f"  Learned compatibility for {len(self.compatible_types)} semantic types")
        print(f"  Average compatible types per type: {np.mean([len(v) for v in self.compatible_types.values()]):.1f}")
    
    def learn_from_embedding_similarity(
        self,
        embeddings: Dict[str, np.ndarray]
    ):
        """
        Learn compatibility from embedding similarity
        Types whose CUIs have similar embeddings are compatible
        """
        print("Learning semantic type compatibility from embeddings...")
        
        # Group CUIs by semantic type
        type_to_cuis = defaultdict(list)
        for cui, types in self.cui_semantic_types.items():
            if cui in embeddings:
                for t in types:
                    type_to_cuis[t].append(cui)
        
        # Compute average embedding per type
        type_embeddings = {}
        for sem_type, cuis in type_to_cuis.items():
            if len(cuis) > 0:
                type_embs = [embeddings[cui] for cui in cuis if cui in embeddings]
                if type_embs:
                    type_embeddings[sem_type] = np.mean(type_embs, axis=0)
        
        # Compute pairwise similarities
        types_list = list(type_embeddings.keys())
        if len(types_list) < 2:
            print("  Not enough semantic types with embeddings")
            return
        
        emb_matrix = np.array([type_embeddings[t] for t in types_list])
        similarities = cosine_similarity(emb_matrix)
        
        # Build compatibility based on similarity
        for i, t1 in enumerate(types_list):
            compatible_set = self.compatible_types.get(t1, set())
            
            for j, t2 in enumerate(types_list):
                if similarities[i, j] >= self.similarity_threshold:
                    compatible_set.add(t2)
                    # Update score (take max of graph and embedding)
                    current_score = self.compatibility_scores[t1][t2]
                    self.compatibility_scores[t1][t2] = max(current_score, similarities[i, j])
            
            self.compatible_types[t1] = compatible_set
        
        print(f"  Updated compatibility for {len(self.compatible_types)} semantic types")
    
    def apply_transitivity(self, max_iterations: int = 3):
        """
        Apply transitivity: if A->B and B->C compatible, then A->C compatible
        """
        print("Applying transitivity to expand compatibility...")
        
        for iteration in range(max_iterations):
            added = 0
            
            for t1 in list(self.compatible_types.keys()):
                compatible_t1 = set(self.compatible_types[t1])
                
                # For each compatible type
                for t2 in list(compatible_t1):
                    if t2 in self.compatible_types:
                        # Add transitive compatibilities
                        new_compatible = self.compatible_types[t2] - compatible_t1 - {t1}
                        
                        for t3 in new_compatible:
                            # Compute transitive score (min of the two paths)
                            score_t1_t2 = self.compatibility_scores[t1][t2]
                            score_t2_t3 = self.compatibility_scores[t2][t3]
                            transitive_score = min(score_t1_t2, score_t2_t3) * 0.8  # Decay
                            
                            # Only add if score is reasonable
                            if transitive_score >= 0.3:
                                compatible_t1.add(t3)
                                # Update score if better
                                if self.compatibility_scores[t1][t3] < transitive_score:
                                    self.compatibility_scores[t1][t3] = transitive_score
                                added += 1
                
                self.compatible_types[t1] = compatible_t1
            
            print(f"  Iteration {iteration + 1}: Added {added} transitive compatibilities")
            
            if added == 0:
                break
    
    def learn_compatibility(
        self,
        graphs: List[nx.DiGraph] = None,
        embeddings: Dict[str, np.ndarray] = None
    ):
        """
        Main method to learn compatibility from data
        
        Args:
            graphs: List of graphs to learn from (e.g., [reduced_graph, topic_graph])
            embeddings: Combined embeddings dict
        """
        print("\n" + "="*70)
        print("AUTOMATIC SEMANTIC TYPE COMPATIBILITY LEARNING")
        print("="*70)
        
        if not self.cui_semantic_types:
            print("No semantic types provided - skipping compatibility learning")
            return
        
        # Learn from graph structure
        if self.learn_from_graph and graphs:
            for i, graph in enumerate(graphs):
                print(f"\nGraph {i+1}/{len(graphs)}:")
                self.learn_from_graph_structure(graph, max_depth=3)
        
        # Learn from embeddings
        if self.learn_from_embeddings and embeddings:
            self.learn_from_embedding_similarity(embeddings)
        
        # Apply transitivity
        if self.compatible_types:
            self.apply_transitivity(max_iterations=2)
        
        self.is_learned = True
        
        # Summary
        print("\n" + "="*70)
        print("LEARNING SUMMARY")
        print("="*70)
        print(f"Total semantic types: {len(self.compatible_types)}")
        if self.compatible_types:
            avg_compatible = np.mean([len(v) for v in self.compatible_types.values()])
            print(f"Avg compatible types per type: {avg_compatible:.1f}")
            
            # Show examples
            print("\nExample compatibilities:")
            for sem_type in list(self.compatible_types.keys())[:5]:
                compatible = self.compatible_types[sem_type]
                scores = [self.compatibility_scores[sem_type][t] for t in compatible]
                avg_score = np.mean(scores) if scores else 0
                print(f"  {sem_type}: {len(compatible)} compatible types (avg score: {avg_score:.3f})")
    
    def are_compatible(self, cui1: str, cui2: str, strict: bool = False) -> Optional[bool]:
        """
        Check if two CUIs are semantically compatible
        
        Args:
            cui1: First CUI
            cui2: Second CUI
            strict: If True, require compatibility score > 0.5
        
        Returns:
            True if compatible, False if incompatible, None if no info
        """
        if cui1 not in self.cui_semantic_types or cui2 not in self.cui_semantic_types:
            return None  # No semantic type info
        
        types1 = self.cui_semantic_types[cui1]
        types2 = self.cui_semantic_types[cui2]
        
        # If no learned compatibility, assume compatible (permissive)
        if not self.is_learned or not self.compatible_types:
            # Fallback: at least one common type
            return len(types1 & types2) > 0 if strict else None
        
        # Check learned compatibility
        max_score = 0.0
        for t1 in types1:
            if t1 not in self.compatible_types:
                continue
            
            compatible_t1 = self.compatible_types[t1]
            for t2 in types2:
                if t2 in compatible_t1:
                    score = self.compatibility_scores[t1][t2]
                    max_score = max(max_score, score)
        
        if strict:
            return max_score > 0.5
        else:
            return max_score > 0.0
    
    def get_compatibility_score(self, cui1: str, cui2: str) -> float:
        """Get compatibility score between two CUIs"""
        if cui1 not in self.cui_semantic_types or cui2 not in self.cui_semantic_types:
            return 0.0
        
        types1 = self.cui_semantic_types[cui1]
        types2 = self.cui_semantic_types[cui2]
        
        max_score = 0.0
        for t1 in types1:
            for t2 in types2:
                score = self.compatibility_scores[t1].get(t2, 0.0)
                max_score = max(max_score, score)
        
        return max_score
    
    def get_semantic_types(self, cui: str) -> Set[str]:
        """Get semantic types for a CUI"""
        return self.cui_semantic_types.get(cui, set())
    
    def export_compatibility_matrix(self, output_path: str):
        """Export learned compatibility for inspection"""
        rows = []
        
        for t1 in sorted(self.compatible_types.keys()):
            for t2 in sorted(self.compatible_types[t1]):
                score = self.compatibility_scores[t1][t2]
                rows.append({
                    'type1': t1,
                    'type2': t2,
                    'compatibility_score': score
                })
        
        df = pd.DataFrame(rows)
        df.to_csv(output_path, index=False)
        print(f"\nExported compatibility matrix to: {output_path}")
        
        return df
    
    def save_learned_compatibility(self, output_path: str):
        """Save learned compatibility for reuse"""
        data = {
            'compatible_types': {k: list(v) for k, v in self.compatible_types.items()},
            'compatibility_scores': {
                k: dict(v) for k, v in self.compatibility_scores.items()
            },
            'is_learned': self.is_learned
        }
        
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"Saved learned compatibility to: {output_path}")
    
    def load_learned_compatibility(self, input_path: str):
        """Load previously learned compatibility"""
        with open(input_path, 'rb') as f:
            data = pickle.load(f)
        
        self.compatible_types = {k: set(v) for k, v in data['compatible_types'].items()}
        self.compatibility_scores = defaultdict(
            lambda: defaultdict(float),
            {k: defaultdict(float, v) for k, v in data['compatibility_scores'].items()}
        )
        self.is_learned = data['is_learned']
        
        print(f"Loaded learned compatibility from: {input_path}")
        print(f"  {len(self.compatible_types)} semantic types with compatibility rules")


def create_auto_validator_from_data(
    cui_semantic_types: Dict[str, Set[str]],
    reduced_graph: nx.DiGraph,
    topic_graph: nx.DiGraph,
    reduced_embeddings: Dict[str, np.ndarray],
    topic_embeddings: Dict[str, np.ndarray],
    similarity_threshold: float = 0.6,
    min_cooccurrence: int = 5
) -> AutoSemanticTypeValidator:
    """
    Convenience function to create and train validator from data
    
    Args:
        cui_semantic_types: CUI to semantic types mapping
        reduced_graph: Reduced CUI graph
        topic_graph: Topic CUI graph
        reduced_embeddings: Reduced embeddings
        topic_embeddings: Topic embeddings
        similarity_threshold: Min similarity for embedding-based compatibility
        min_cooccurrence: Min co-occurrences for graph-based compatibility
    
    Returns:
        Trained AutoSemanticTypeValidator
    """
    # Create validator
    validator = AutoSemanticTypeValidator(
        cui_semantic_types=cui_semantic_types,
        learn_from_graph=True,
        learn_from_embeddings=True,
        similarity_threshold=similarity_threshold,
        min_cooccurrence=min_cooccurrence
    )
    
    # Combine embeddings
    all_embeddings = {**reduced_embeddings, **topic_embeddings}
    
    # Learn compatibility
    validator.learn_compatibility(
        graphs=[reduced_graph, topic_graph],
        embeddings=all_embeddings
    )
    
    return validator


# Example usage
if __name__ == "__main__":
    print("Automatic Semantic Type Compatibility Learning")
    print("="*70)
    print("\nThis module automatically discovers semantic type compatibility from:")
    print("  1. Graph structure (co-occurrence in hierarchies)")
    print("  2. Embedding similarity (semantic similarity)")
    print("  3. Transitivity (if A→B and B→C, then A→C)")
    print("\nNo manual rules needed!")
