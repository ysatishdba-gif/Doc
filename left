"""
LEFT HAND SIDE: Query CUI Reduction Pipeline
============================================
INPUT:  User query text (e.g., "grams", "diabetes patient")
OUTPUT: Reduced high-quality CUI set + Graph + Embeddings

YOUR WORK: Semantic grouping + IC filtering + Embedding clustering
"""

import pickle
import subprocess
import logging
import time
import threading
from typing import List, Dict, Set, Tuple
from collections import defaultdict
import numpy as np
from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import requests
from requests.adapters import HTTPAdapter, Retry
import networkx as nx
from dataclasses import dataclass, asdict

# =============================================================================
# CONFIGURATION
# =============================================================================

CONFIG = {
    'project_id': 'your-gcp-project',
    'dataset_id': 'umls_2024',
    'cui_embeddings_table': 'cui_embeddings_v1',
    'mrsty_table': 'MRSTY',
    'api_url': 'https://your-api.run.app/extract',
    'network_pkl': '/home/jupyter/CUIreduction/networkx_cui_context_v1_1_0.pkl',
    'output_graph': 'lhs_reduced_graph.pkl',
    'output_embeddings': 'lhs_embeddings.pkl',
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# GCP TOKEN PROVIDER
# =============================================================================

class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force: bool = False) -> Dict[str, str]:
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token
            
            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)
            
            token = proc.stdout.strip()
            cls._token = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
            cls._expiry = now + 3300
            return cls._token

# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class ReductionStats:
    initial_count: int
    final_count: int
    reduction_pct: float
    processing_time: float
    group_stats: Dict = None

# =============================================================================
# CUI API CLIENT
# =============================================================================

class CUIAPIClient:
    def __init__(self, api_url: str, timeout: int = 60, top_k: int = 3):
        self.api_url = api_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("http://", HTTPAdapter(max_retries=retry))
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract_cuis(self, texts: List[str]) -> Set[str]:
        if not texts:
            return set()
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = GCPTokenProvider.get_headers()
        
        try:
            response = self.session.post(self.api_url, json=payload, headers=headers, timeout=self.timeout)
            if response.status_code == 401:
                GCPTokenProvider.get_headers(force=True)
                response = self.session.post(self.api_url, json=payload, headers=GCPTokenProvider.get_headers(), timeout=self.timeout)
            response.raise_for_status()
            
            data = response.json()
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            return all_cuis
        except Exception as e:
            logger.error(f"API call failed: {e}")
            return set()

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def filter_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    """Filter CUIs to retain only ICD, SNOMED, LOINC"""
    if not cuis:
        return []
    
    client = bigquery.Client(project=project_id)
    query = f"""
    SELECT DISTINCT CUI
    FROM `{project_id}.{dataset_id}.MRCONSO`
    WHERE CUI IN UNNEST(@cuis)
      AND SAB IN ('CCSR_ICD10CM','CCSR_ICD10PCS','DMDICD10','ICD10','ICD10AE','ICD10AM',
                  'ICD10CM','ICD10PCS','ICD9CM','SNOMEDCT_US','LOINC')
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
    )
    df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
    return df['CUI'].tolist()

# =============================================================================
# CUI REDUCER
# =============================================================================

class EnhancedCUIReducer:
    def __init__(self, project_id: str, dataset_id: str, cui_embeddings_table: str, 
                 mrsty_table: str, umls_graph: nx.DiGraph):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.cui_embeddings_table = cui_embeddings_table
        self.mrsty_table = mrsty_table
        self.umls_graph = umls_graph
        self._hierarchy_cache = {}
        self._ic_scores_cache = {}

    def reduce(self, input_cuis: List[str], ic_percentile: int = 75, 
               similarity_threshold: float = 0.75) -> Tuple[List[str], ReductionStats]:
        
        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        
        # Semantic grouping
        semantic_groups = self._group_by_semantic_type(input_cuis)
        
        all_reduced_cuis = []
        group_stats = {}
        
        for group_name, group_cuis in semantic_groups.items():
            if not group_cuis:
                continue
            
            # Build hierarchy
            hierarchy = self._build_hierarchy(group_cuis)
            
            # IC filtering
            ic_scores = self._compute_ic_scores(hierarchy, group_cuis, group_name)
            ic_threshold = np.percentile(list(ic_scores.values()), ic_percentile) if ic_scores else 0.0
            high_ic_cuis = [cui for cui in group_cuis if ic_scores.get(cui, 0) >= ic_threshold]
            
            # Embedding clustering
            if len(high_ic_cuis) > 1:
                group_reduced = self._cluster_cuis(high_ic_cuis, similarity_threshold)
            else:
                group_reduced = high_ic_cuis
            
            all_reduced_cuis.extend(group_reduced)
            group_stats[group_name] = {
                'original': len(group_cuis),
                'final': len(group_reduced),
                'reduction_pct': (1 - len(group_reduced)/len(group_cuis)) * 100 if group_cuis else 0
            }
        
        final_cuis = list(set(all_reduced_cuis))
        
        stats = ReductionStats(
            initial_count=initial_count,
            final_count=len(final_cuis),
            reduction_pct=(1 - len(final_cuis)/initial_count) * 100 if initial_count else 0,
            processing_time=time.time() - start_time,
            group_stats=group_stats
        )
        
        return final_cuis, stats

    def _group_by_semantic_type(self, cuis: List[str]) -> Dict[str, List[str]]:
        query = f"""
        SELECT DISTINCT CUI, STY
        FROM `{self.project_id}.{self.dataset_id}.{self.mrsty_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cuis)]
        )
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        
        if df.empty:
            return {'UNKNOWN': cuis}
        
        cui_to_types = defaultdict(set)
        for _, row in df.iterrows():
            cui_to_types[row['CUI']].add(row['STY'])
        
        final_groups = defaultdict(list)
        for cui in cuis:
            types = cui_to_types.get(cui, {'UNKNOWN'})
            primary_type = sorted(types, key=lambda x: (-len(x), x))[0]
            final_groups[primary_type].append(cui)
        
        return dict(final_groups)

    def _build_hierarchy(self, cuis: List[str]) -> Dict:
        key = tuple(sorted(cuis))
        if key in self._hierarchy_cache:
            return self._hierarchy_cache[key]
        
        parent_to_children = defaultdict(list)
        child_to_parents = defaultdict(list)
        
        for cui in cuis:
            if not self.umls_graph.has_node(cui):
                continue
            for child in self.umls_graph.successors(cui):
                parent_to_children[cui].append(child)
                child_to_parents[child].append(cui)
            for parent in self.umls_graph.predecessors(cui):
                child_to_parents[cui].append(parent)
                parent_to_children[parent].append(cui)
        
        hierarchy = {
            "parent_to_children": dict(parent_to_children),
            "child_to_parents": dict(child_to_parents)
        }
        self._hierarchy_cache[key] = hierarchy
        return hierarchy

    def _compute_ic_scores(self, hierarchy: Dict, group_cuis: List[str], group_name: str) -> Dict[str, float]:
        cache_key = f"{group_name}_{tuple(sorted(group_cuis))}"
        if cache_key in self._ic_scores_cache:
            return self._ic_scores_cache[cache_key]
        
        parent_to_children = hierarchy.get("parent_to_children", {})
        group_set = set(group_cuis)
        descendant_counts = {}
        
        def count_descendants(cui: str, visited: Set[str] = None) -> int:
            if visited is None:
                visited = set()
            if cui in visited or cui not in group_set:
                return 0
            visited.add(cui)
            children = [c for c in parent_to_children.get(cui, []) if c in group_set]
            count = len(children)
            for child in children:
                count += count_descendants(child, visited)
            descendant_counts[cui] = count
            return count
        
        for cui in group_cuis:
            if cui not in descendant_counts:
                try:
                    count_descendants(cui)
                except RecursionError:
                    descendant_counts[cui] = 0
        
        group_size = len(group_cuis)
        ic_scores = {
            cui: max(0.0, -np.log((descendant_counts.get(cui, 0) + 1) / group_size))
            for cui in group_cuis
        }
        self._ic_scores_cache[cache_key] = ic_scores
        return ic_scores

    def _cluster_cuis(self, cui_list: List[str], similarity_threshold: float) -> List[str]:
        if len(cui_list) <= 1:
            return cui_list
        
        query = f"""
        SELECT CUI AS cui, embedding
        FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)]
        )
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        
        if df.empty:
            return cui_list
        
        valid = [(row["cui"], np.asarray(row["embedding"], dtype=np.float32)) 
                 for _, row in df.iterrows() if row["embedding"] is not None]
        
        if len(valid) < 2:
            return cui_list
        
        cuis, embeddings = zip(*valid)
        embeddings = np.vstack(embeddings)
        
        clustering = AgglomerativeClustering(
            n_clusters=None, distance_threshold=1-similarity_threshold, 
            metric="cosine", linkage="average"
        )
        labels = clustering.fit_predict(embeddings)
        
        selected = set()
        for label in np.unique(labels):
            idx = np.where(labels == label)[0]
            cluster_emb = embeddings[idx]
            centroid = np.mean(cluster_emb, axis=0)
            distances = cosine_distances([centroid], cluster_emb)[0]
            selected.add(cuis[idx[np.argmax(distances)]])
        
        return list(selected)

# =============================================================================
# MAIN LEFT HAND SIDE PIPELINE
# =============================================================================

class LeftHandSide:
    """Complete LHS pipeline: Query text â†’ Reduced CUIs + Graph + Embeddings"""
    
    def __init__(self, config: Dict):
        self.config = config
        
        # Load UMLS graph
        logger.info(f"Loading UMLS graph from {config['network_pkl']}")
        with open(config['network_pkl'], 'rb') as f:
            self.umls_graph = pickle.load(f)
        logger.info(f"Loaded graph: {self.umls_graph.number_of_nodes():,} nodes")
        
        # Initialize components
        self.api_client = CUIAPIClient(config['api_url'])
        self.bq_client = bigquery.Client(project=config['project_id'])
        self.reducer = EnhancedCUIReducer(
            project_id=config['project_id'],
            dataset_id=config['dataset_id'],
            cui_embeddings_table=config['cui_embeddings_table'],
            mrsty_table=config['mrsty_table'],
            umls_graph=self.umls_graph
        )
    
    def process(self, query_text: str) -> Dict:
        """
        Main processing function
        
        Args:
            query_text: User query (e.g., "grams", "diabetes patient")
        
        Returns:
            dict with keys:
                - reduced_cuis: List[str]
                - reduced_graph: nx.DiGraph
                - embeddings: Dict[str, np.ndarray]
                - stats: ReductionStats
        """
        print("\n" + "="*80)
        print("LEFT HAND SIDE: QUERY CUI REDUCTION")
        print("="*80)
        print(f"Input Query: '{query_text}'")
        print("-"*80)
        
        # Step 1: Extract CUIs
        print("\n[1/5] Extracting CUIs from query...")
        extracted_cuis = self.api_client.extract_cuis([query_text])
        print(f"      Extracted: {len(extracted_cuis)} CUIs")
        
        # Step 2: Filter vocabularies
        print("[2/5] Filtering to allowed vocabularies (ICD, SNOMED, LOINC)...")
        filtered_cuis = filter_cuis(extracted_cuis, self.config['project_id'], self.config['dataset_id'])
        print(f"      After filter: {len(filtered_cuis)} CUIs")
        
        # Step 3: Reduce CUIs
        print("[3/5] Reducing CUIs (Semantic + IC + Clustering)...")
        reduced_cuis, stats = self.reducer.reduce(filtered_cuis)
        print(f"      Initial: {stats.initial_count} â†’ Final: {stats.final_count}")
        print(f"      Reduction: {stats.reduction_pct:.1f}%")
        print(f"      Time: {stats.processing_time:.2f}s")
        
        # Step 4: Build graph
        print("[4/5] Building reduced graph with parent anchors...")
        reduced_graph = self._build_graph_with_parents(reduced_cuis)
        print(f"      Nodes: {reduced_graph.number_of_nodes():,}")
        print(f"      Edges: {reduced_graph.number_of_edges():,}")
        
        # Step 5: Fetch embeddings
        print("[5/5] Fetching embeddings...")
        embeddings = self._fetch_embeddings(reduced_cuis)
        print(f"      Embeddings: {len(embeddings)}")
        
        # Save outputs
        with open(self.config['output_graph'], 'wb') as f:
            pickle.dump(reduced_graph, f)
        with open(self.config['output_embeddings'], 'wb') as f:
            pickle.dump(embeddings, f)
        
        print("\nâœ“ LEFT HAND SIDE COMPLETE")
        print(f"  Saved graph: {self.config['output_graph']}")
        print(f"  Saved embeddings: {self.config['output_embeddings']}")
        print("="*80 + "\n")
        
        return {
            'reduced_cuis': reduced_cuis,
            'reduced_graph': reduced_graph,
            'embeddings': embeddings,
            'stats': stats
        }
    
    def _build_graph_with_parents(self, cuis: List[str]) -> nx.DiGraph:
        G = nx.DiGraph()
        G.add_nodes_from(cuis)
        for cui in cuis:
            if self.umls_graph.has_node(cui):
                for parent in self.umls_graph.predecessors(cui):
                    G.add_node(parent)
                    G.add_edge(cui, parent)
        return G
    
    def _fetch_embeddings(self, cuis: List[str]) -> Dict[str, np.ndarray]:
        if not cuis:
            return {}
        query = f"""
        SELECT CUI AS cui, embedding
        FROM `{self.config['project_id']}.{self.config['dataset_id']}.{self.config['cui_embeddings_table']}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cuis)]
        )
        df = self.bq_client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        embeddings = {}
        for _, row in df.iterrows():
            if row["embedding"] is not None:
                embeddings[row["cui"]] = np.asarray(row["embedding"], dtype=np.float32)
        return embeddings

# =============================================================================
# USAGE
# =============================================================================

if __name__ == "__main__":
    # Initialize LHS pipeline
    lhs = LeftHandSide(CONFIG)
    
    # Process query
    result = lhs.process("grams")
    
    # Display results
    print("\nðŸ“Š RESULTS:")
    print(f"   Reduced CUIs: {len(result['reduced_cuis'])}")
    print(f"   Graph nodes: {result['reduced_graph'].number_of_nodes()}")
    print(f"   Embeddings: {len(result['embeddings'])}")
    print(f"\n   Top 10 CUIs: {result['reduced_cuis'][:10]}")
