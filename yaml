import asyncio
import aiohttp
import subprocess
import re
import logging
from pathlib import Path
import json
import pickle
from typing import List, Dict, Set, Any, Optional, Tuple
from collections import defaultdict, deque
import time
from dataclasses import dataclass, asdict
import hashlib
import numpy as np
from google.cloud import bigquery
import nest_asyncio
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances

nest_asyncio.apply()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================
# SUBNET API HIERARCHY BUILDER (Replacement for MRREL)
# ============================================

class SubnetAPIHierarchyBuilder:
    """
    Replaces MRREL table queries with subnet API calls for building hierarchy
    """
    
    def __init__(self, api_url: str, project_id: str, dataset_id: str, timeout: int = 60):
        self.api_url = api_url.rstrip('/')
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.timeout = timeout
        self.cache = {}
        self._token = None
        self._token_expiry = 0
    
    def _get_auth_token(self) -> str:
        """Get GCP authentication token"""
        current_time = time.time()
        if self._token and current_time < self._token_expiry:
            return self._token
        
        try:
            result = subprocess.run(
                ['gcloud', 'auth', 'print-identity-token'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            
            if result.returncode != 0:
                raise Exception(f"gcloud auth failed: {result.stderr}")
            
            self._token = result.stdout.strip()
            self._token_expiry = current_time + 3300  # 55 minutes
            return self._token
            
        except Exception as e:
            raise Exception(f"Failed to get GCP token: {str(e)}")
    
    async def build_hierarchy_from_api(
        self, 
        relevant_cuis: List[str],
        limit_context: List[str] = None,
        max_hierarchy_depth: int = 1
    ) -> Dict:
        """
        Build hierarchy using subnet API instead of MRREL table
        This replaces _build_hierarchy_safe() method
        """
        
        if not limit_context:
            limit_context = ['SNOMEDCT_US.isa', 'ICD10.Empty', 'LOINC.Empty']
        
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(relevant_cuis)
        
        try:
            # Use async API calls to fetch hierarchy
            headers = {
                "Authorization": f"Bearer {self._get_auth_token()}",
                "Content-Type": "application/json"
            }
            
            async with aiohttp.ClientSession() as session:
                visited = set()
                frontier = set(relevant_cuis)
                
                for depth in range(max_hierarchy_depth):
                    if not frontier:
                        break
                    
                    logger.info(f"Fetching hierarchy depth {depth + 1}: {len(frontier)} CUIs")
                    
                    # Process frontier in batches
                    batch_size = 100
                    frontier_list = list(frontier)
                    all_edges = []
                    
                    for i in range(0, len(frontier_list), batch_size):
                        batch = frontier_list[i:i+batch_size]
                        
                        # Call subnet API for batch
                        payload = {
                            "cuis": batch,
                            "cross_context": False
                        }
                        if limit_context:
                            payload["limit_context"] = limit_context
                        
                        try:
                            async with session.post(
                                f"{self.api_url}/subnet/",
                                json=payload,
                                headers=headers,
                                timeout=aiohttp.ClientTimeout(total=self.timeout)
                            ) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    nodes, edges = data.get("output", ([], []))
                                    all_edges.extend(edges)
                                    all_cuis.update(nodes)
                        except Exception as e:
                            logger.warning(f"API call failed for batch at depth {depth + 1}: {e}")
                    
                    # Process edges to build parent-child relationships
                    next_frontier = set()
                    for parent, child in all_edges:
                        # API returns (parent, child) pairs
                        parent_to_children[parent].append(child)
                        child_to_parents[child].append(parent)
                        
                        if parent not in visited:
                            next_frontier.add(parent)
                        if child not in visited:
                            next_frontier.add(child)
                    
                    visited.update(frontier)
                    frontier = next_frontier - visited
                    
                    logger.info(f"Depth {depth + 1}: Found {len(all_edges)} relationships")
            
        except Exception as e:
            logger.error(f"Failed to build hierarchy from API: {str(e)}")
        
        hierarchy = {
            'child_to_parents': dict(child_to_parents),
            'parent_to_children': dict(parent_to_children),
            'all_cuis': all_cuis
        }
        
        logger.info(f"Built hierarchy from API: {len(all_cuis)} CUIs total")
        return hierarchy

# ============================================
# MODIFIED EnhancedCUIReducer
# ============================================

class EnhancedCUIReducerWithAPI:
    """
    Modified CUI Reducer that uses subnet API instead of MRREL table
    """
    
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        subnet_api_url: str,  # NEW: API URL for subnet
        cui_description_table: str = "cui_descriptions",
        cui_embeddings_table: str = "cui_embeddings",
        cui_narrower_table: str = "cui_narrower_concepts",
        max_hierarchy_depth: int = 1,
        query_timeout: int = 300
    ):
        try:
            self.client = bigquery.Client(project=project_id)
        except Exception as e:
            raise Exception(f"Failed to initialize BigQuery client: {str(e)}")
        
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.cui_description_table = cui_description_table
        self.cui_embeddings_table = cui_embeddings_table
        self.cui_narrower_table = cui_narrower_table
        self.max_hierarchy_depth = max_hierarchy_depth
        self.query_timeout = query_timeout
        
        # NEW: Initialize subnet API hierarchy builder
        self.hierarchy_builder = SubnetAPIHierarchyBuilder(
            api_url=subnet_api_url,
            project_id=project_id,
            dataset_id=dataset_id
        )
        
        # Cache
        self._hierarchy_cache = None
        self._ic_scores_cache = None
        self._description_cache = {}
    
    def reduce(
        self,
        input_cuis: List[str],
        target_reduction: float = 0.85,
        ic_threshold: Optional[float] = None,
        ic_percentile: float = 50.0,
        semantic_threshold: float = 0.88,
        use_semantic_clustering: bool = True,
        adaptive_threshold: bool = False,
        limit_context: List[str] = None  # NEW: context for API
    ) -> Tuple[List[str], 'ReductionStats']:
        """
        Main reduction method using API-based hierarchy
        """
        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        
        # Build hierarchy using API instead of MRREL
        api_start = time.time()
        hierarchy = asyncio.run(
            self.hierarchy_builder.build_hierarchy_from_api(
                input_cuis, 
                limit_context=limit_context,
                max_hierarchy_depth=self.max_hierarchy_depth
            )
        )
        api_time = time.time() - api_start
        
        # Rest of the logic remains the same
        ic_scores = self._compute_ic_scores_safe(hierarchy)
        
        if ic_threshold is None:
            ic_threshold = float(np.median(list(ic_scores.values())))
            logger.info(f"Using MEDIAN IC threshold: {ic_threshold:.3f}")
        
        rolled_up = self._semantic_rollup_with_ic_safe(
            input_cuis, hierarchy, ic_scores, ic_threshold
        )
        after_rollup = len(rolled_up)
        
        final_cuis = rolled_up
        if use_semantic_clustering:
            final_cuis = self._semantic_clustering_safe(rolled_up, ic_scores)
        
        final_count = len(final_cuis)
        
        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=after_rollup,
            final_count=final_count,
            ic_rollup_reduction_pct=self._safe_percentage(
                initial_count - after_rollup, initial_count
            ),
            semantic_clustering_reduction_pct=self._safe_percentage(
                after_rollup - final_count, initial_count
            ),
            total_reduction_pct=self._safe_percentage(
                initial_count - final_count, initial_count
            ),
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_threshold,
            hierarchy_size=len(hierarchy.get("all_cuis", [])),
            api_call_time=api_time
        )
        
        return final_cuis, stats
    
    # All other methods remain exactly the same
    def _compute_ic_scores_safe(self, hierarchy: Dict) -> Dict[str, float]:
        """Same as original"""
        if self._ic_scores_cache is not None:
            return self._ic_scores_cache
        
        parent_to_children = hierarchy.get('parent_to_children', {})
        all_cuis = hierarchy.get('all_cuis', set())
        total = len(all_cuis)
        
        if total == 0:
            return {}
        
        try:
            descendant_counts = {}
            
            def count_descendants(cui: str, visited: Set[str] = None) -> int:
                if visited is None:
                    visited = set()
                if cui in visited or cui in descendant_counts:
                    return descendant_counts.get(cui, 0)
                
                visited.add(cui)
                children = parent_to_children.get(cui, [])
                count = len(children)
                
                for child in children:
                    count += count_descendants(child, visited)
                
                descendant_counts[cui] = count
                return count
            
            logger.info("Computing IC scores...")
            for cui in all_cuis:
                if cui not in descendant_counts:
                    try:
                        count_descendants(cui)
                    except RecursionError:
                        logger.warning(f"Recursion limit for {cui}")
                        descendant_counts[cui] = 0
            
            ic_scores = {}
            for cui in all_cuis:
                desc_count = descendant_counts.get(cui, 0)
                ic = -np.log((desc_count + 1) / total)
                ic_scores[cui] = max(0.0, ic)
            
            self._ic_scores_cache = ic_scores
            
            if ic_scores:
                values = list(ic_scores.values())
                logger.info(f"IC scores: {len(ic_scores)} CUIs, range [{min(values):.2f}, {max(values):.2f}]")
            
            return ic_scores
            
        except Exception as e:
            logger.error(f"Failed to compute IC scores: {str(e)}")
            return {cui: 5.0 for cui in all_cuis}
    
    def _semantic_rollup_with_ic_safe(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        ic_threshold: float
    ) -> List[str]:
        """Same as original"""
        try:
            child_to_parents = hierarchy.get('child_to_parents', {})
            rolled_up = {}
            
            for cui in cui_list:
                try:
                    ancestors = []
                    visited = set()
                    queue = deque([cui])
                    
                    while queue and len(visited) < 100:
                        current = queue.popleft()
                        if current in visited:
                            continue
                        visited.add(current)
                        
                        for parent in child_to_parents.get(current, []):
                            if parent not in visited:
                                ancestors.append(parent)
                                queue.append(parent)
                    
                    candidates = [cui] + ancestors
                    valid = [c for c in candidates if ic_scores.get(c, 0) >= ic_threshold]
                    
                    if valid:
                        rolled_up[cui] = min(valid, key=lambda c: ic_scores.get(c, float('inf')))
                    else:
                        rolled_up[cui] = cui
                        
                except Exception as e:
                    logger.debug(f"Rollup failed for {cui}: {str(e)}")
                    rolled_up[cui] = cui
            
            return list(set(rolled_up.values()))
            
        except Exception as e:
            logger.error(f"Semantic rollup failed: {str(e)}")
            return cui_list
    
    def _semantic_clustering_safe(
        self,
        cui_list: List[str],
        ic_scores: Dict[str, float],
        similarity_threshold: float = None,
        min_intra_cluster_distance: float = 0.25
    ) -> List[str]:
        """Same as original"""
        if len(cui_list) <= 1:
            return cui_list
        
        try:
            logger.info(f"Fetching embeddings for {len(cui_list)} CUIs...")
            
            query = f"""
            SELECT REF_CUI as cui, REF_Embedding as embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
            WHERE REF_CUI IN UNNEST(@cuis)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)
                ]
            )
            
            df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            
            if df.empty:
                logger.warning("No embeddings found, skipping clustering")
                return cui_list
            
            embeddings = np.vstack(df['embedding'].values)
            cuis = np.array(df['cui'].values)
            
            sim_matrix = 1 - cosine_distances(embeddings)
            upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]
            perc95 = np.percentile(upper_tri, 95)
            
            logger.info(f"Dynamic similarity threshold: {perc95:.4f}")
            
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - perc95,
                metric='cosine',
                linkage='average'
            )
            
            labels = clustering.fit_predict(embeddings)
            
            final_cuis = []
            
            for cluster_id in np.unique(labels):
                idx = np.where(labels == cluster_id)[0]
                cluster_cuis = cuis[idx]
                cluster_embeddings = embeddings[idx]
                
                if len(cluster_cuis) == 1:
                    final_cuis.append(cluster_cuis[0])
                    continue
                
                dist_matrix = cosine_distances(cluster_embeddings)
                
                order = sorted(
                    range(len(cluster_cuis)),
                    key=lambda i: ic_scores.get(cluster_cuis[i], 0),
                    reverse=True
                )
                
                kept = []
                for i in order:
                    too_close = any(dist_matrix[i, j] < min_intra_cluster_distance for j in kept)
                    if not too_close:
                        kept.append(i)
                
                final_cuis.extend(cluster_cuis[kept])
            
            logger.info(
                f"Distance-based clustering reduced {len(cui_list)} → {len(final_cuis)} CUIs"
            )
            
            return list(set(final_cuis))
            
        except Exception as e:
            logger.error(f"Clustering failed: {str(e)}")
            return cui_list
    
    def get_cui_descriptions(self, cui_list: List[str]) -> Dict[str, str]:
        """Same as original"""
        if not cui_list:
            return {}
        
        uncached = [c for c in cui_list if c not in self._description_cache]
        
        if uncached:
            try:
                query = f"""
                SELECT CUI as cui, Definition as description
                FROM `{self.project_id}.{self.dataset_id}.{self.cui_description_table}`
                WHERE CUI IN UNNEST(@cuis)
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("cuis", "STRING", uncached)
                    ]
                )
                
                query_job = self.client.query(query, job_config=job_config)
                df = query_job.result(timeout=30).to_dataframe()
                new_descriptions = dict(zip(df['cui'], df['description']))
                self._description_cache.update(new_descriptions)
                
            except Exception as e:
                logger.error(f"Failed to fetch descriptions: {str(e)}")
        
        return {cui: self._description_cache.get(cui, "N/A") for cui in cui_list}
    
    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        return (numerator / denominator * 100) if denominator > 0 else 0.0

# ============================================
# MAIN FUNCTION WITH API-BASED HIERARCHY
# ============================================

def run_cui_reduction_with_api(
    texts: List[str],
    project_id: str,
    dataset_id: str,
    api_url: str,
    subnet_api_url: str,  # NEW: URL for subnet API
    cui_description_table: str = "cui_descriptions",
    cui_embeddings_table: str = "cui_embeddings",
    cui_narrower_table: str = "cui_narrower_concepts",
    target_reduction: float = 0.85,
    ic_percentile: float = 50.0,
    semantic_threshold: float = 0.88,
    use_semantic_clustering: bool = True,
    adaptive_threshold: bool = False,
    limit_context: List[str] = None
) -> Tuple[List[str], List[str], Dict[str, str], Optional['ReductionStats']]:
    """
    Main function using subnet API for hierarchy instead of MRREL table
    """
    
    try:
        # Initialize API client for CUI extraction
        api_client = CUIAPIClient(
            api_base_url=api_url,
            timeout=60,
            top_k=3
        )
        
        # Initialize reducer with subnet API
        cui_reducer = EnhancedCUIReducerWithAPI(
            project_id=project_id,
            dataset_id=dataset_id,
            subnet_api_url=subnet_api_url,  # Pass subnet API URL
            cui_description_table=cui_description_table,
            cui_embeddings_table=cui_embeddings_table,
            cui_narrower_table=cui_narrower_table,
            max_hierarchy_depth=1,
            query_timeout=300
        )
        
        # Extract CUIs from texts
        logger.info(f"Extracting CUIs from {len(texts)} text(s)...")
        initial_cuis = api_client.extract_cuis_batch(texts)
        
        if not initial_cuis:
            logger.warning("No CUIs extracted from texts")
            return [], [], {}, None
        
        # Filter CUIs to only ICD, SNOMED, and LOINC
        logger.info(f"Filtering {len(initial_cuis)} CUIs to retain only ICD, SNOMED, and LOINC...")
        initial_cuis = filter_allowed_cuis(initial_cuis, project_id, dataset_id)
        
        if not initial_cuis:
            logger.warning("No CUIs remain after filtering")
            return [], [], {}, None
        
        # Set default context if not provided
        if not limit_context:
            limit_context = [
                'LNC.Empty', 'SNOMEDCT_US.Empty', 'SNOMEDCT_US.isa',
                'ICD10.Empty', 'ICD10AM.Empty', 'ICD10CM.Empty',
                'ICD10PCS.Empty', 'CCSR_ICD10PCS.Empty'
            ]
        
        # Reduce CUIs using API-based hierarchy
        start_time = time.time()
        reduced_cuis, stats = cui_reducer.reduce(
            list(initial_cuis),
            target_reduction=target_reduction,
            ic_percentile=ic_percentile,
            semantic_threshold=semantic_threshold,
            use_semantic_clustering=use_semantic_clustering,
            adaptive_threshold=adaptive_threshold,
            limit_context=limit_context
        )
        
        # Add total API call time to stats
        stats.api_call_time = time.time() - start_time - stats.processing_time
        
        # Get descriptions
        logger.info("Fetching descriptions...")
        descriptions = cui_reducer.get_cui_descriptions(reduced_cuis)
        
        return list(initial_cuis), reduced_cuis, descriptions, stats
        
    except Exception as e:
        logger.error(f"Pipeline error: {str(e)}")
        return [], [], {}, None

# ============================================
# EXAMPLE USAGE
# ============================================

if __name__ == "__main__":
    # Configuration
    project_id = "your_project"
    dataset_id = "your_dataset"
    api_url = "your_cui_extraction_api_url"
    subnet_api_url = "your_subnet_api_url"  # NEW: subnet API URL
    
    texts = ["pharmacotherapy depression type 2 diabetes"]
    
    # Run reduction with API-based hierarchy
    initial_cuis, reduced_cuis, descriptions, stats = run_cui_reduction_with_api(
        texts=texts,
        project_id=project_id,
        dataset_id=dataset_id,
        api_url=api_url,
        subnet_api_url=subnet_api_url,  # Pass subnet API
        cui_description_table="cui_descriptions",
        cui_embeddings_table="cui_embeddings",
        cui_narrower_table="cui_narrower_concepts"
    )
    
    if stats:
        print(f"Reduction complete: {stats.initial_count} → {stats.final_count} ({stats.total_reduction_pct:.1f}%)")
        print(f"API call time: {stats.api_call_time:.2f}s")
        print(f"Hierarchy size: {stats.hierarchy_size} CUIs")
