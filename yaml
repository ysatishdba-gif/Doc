import asyncio
import aiohttp
import subprocess
import re
import logging
from pathlib import Path
import json
import pickle
from typing import List, Dict, Set, Any, Optional, Tuple
from collections import defaultdict, deque
import time
from dataclasses import dataclass, asdict
import hashlib
import numpy as np
from google.cloud import bigquery
import nest_asyncio
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import requests
from requests.adapters import HTTPAdapter, Retry
import threading

# Apply nest_asyncio for Jupyter compatibility
nest_asyncio.apply()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================
# DATA CLASSES (FROM CODE 1)
# ============================================

@dataclass
class ReductionStats:
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    
    def to_dict(self):
        return asdict(self)

# ============================================
# FILTER FUNCTION (FROM CODE 1)
# ============================================

def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    """only ICD, LOINC, or SNOMED based on MRCONSO table."""
    if not cuis:
        return []

    try:
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('CCSR_ICD10CM','CCSR_ICD10PCS','DMDICD10','ICD10','ICD10AE','ICD10AM','ICD10AMAE','ICD10CM','ICD10DUT','ICD10PCS','ICD9CM','ICPC2ICD10DUT',
          'ICPC2ICD10ENG','SNOMEDCT_US', 'LOINC')
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
        )
        query_job = client.query(query, job_config=job_config)
        df = query_job.result(timeout=60).to_dataframe()
        allowed_cuis = df['CUI'].tolist()
        logger.info(f"{len(allowed_cuis)} CUIs after filter")
        return allowed_cuis
    except Exception as e:
        logger.error(f"Failed to filter CUIs by SAB: {str(e)}")
        return []

# ============================================
# CUI API CLIENT (FROM CODE 1)
# ============================================

class CUIAPIClient:
    
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0
    
    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3, max_retries: int = 3):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.max_retries = max_retries
        
        # Configure session with connection pooling
        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Get initial token
        self._update_gcp_token()
    
    def _update_gcp_token(self, force: bool = False):
        
        with self._token_lock:
            current_time = time.time()
            
            if not force and self._cached_token and current_time < self._token_expiry:
                return self._cached_token
            
            try:
                result = subprocess.run(
                    ['gcloud', 'auth', 'print-identity-token'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    universal_newlines=True,
                    timeout=10,
                    check=False
                )
                
                if result.returncode != 0:
                    raise Exception(f"gcloud auth failed: {result.stderr}")
                
                identity_token = result.stdout.strip()
                if not identity_token:
                    raise Exception("Empty token received from gcloud")
                
                self._cached_token = {
                    "Authorization": f"Bearer {identity_token}",
                    "Content-Type": "application/json"
                }
                self._token_expiry = current_time + 3300  # 55 minutes
                
                return self._cached_token
                
            except Exception as e:
                raise Exception(f"Failed to get GCP token: {str(e)}")
    
    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        if not texts:
            return set()
        
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = self._update_gcp_token()
        
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            
            if response.status_code == 401 and retry_auth:
                logger.warning("Token expired, refreshing...")
                self._update_gcp_token(force=True)
                return self.extract_cuis_batch(texts, retry_auth=False)
            
            response.raise_for_status()
            data = response.json()
            
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            
            logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
            return all_cuis
            
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return set()

# ============================================
# SUBNET API HIERARCHY BUILDER (FROM CODE 2)
# ============================================

class SubnetAPIHierarchyBuilder:
    """Build hierarchy using subnet API instead of MRREL"""
    
    def __init__(self, subnet_api_url: str, timeout: int = 60):
        self.url = subnet_api_url.rstrip('/')
        self.timeout = timeout
        self._token = None
        self._token_expiry = 0
        self._token_lock = threading.Lock()
        
    def _get_auth_headers(self) -> Dict[str, str]:
        """Get authentication headers"""
        with self._token_lock:
            current_time = time.time()
            if self._token and current_time < self._token_expiry:
                return {"Authorization": f"Bearer {self._token}", "Content-Type": "application/json"}
            
            proc = subprocess.run(["gcloud", "auth", "print-identity-token"],
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)
            
            self._token = proc.stdout.strip()
            self._token_expiry = current_time + 3300
            return {"Authorization": f"Bearer {self._token}", "Content-Type": "application/json"}
    
    def _normalize_cui(self, cui: str) -> Optional[str]:
        """Normalize CUI format"""
        if not cui:
            return None
        m = re.match(r"^(C\d{7})(?:-\d+)?$", str(cui))
        return m.group(1) if m else None
    
    async def build_hierarchy_from_api(self, relevant_cuis: List[str], limit_context: List[str] = None) -> Dict:
        """Build hierarchy using subnet API - REPLACES _build_hierarchy_safe"""
        
        if not limit_context:
            limit_context = ['LNC.Empty', 'SNOMEDCT_US.Empty', 'SNOMEDCT_US.isa',
                           'ICD10.Empty', 'ICD10AM.Empty', 'ICD10CM.Empty',
                           'ICD10PCS.Empty', 'CCSR_ICD10PCS.Empty']
        
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(relevant_cuis)
        
        try:
            headers = self._get_auth_headers()
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout)) as session:
                # Process CUIs in batches
                batch_size = 50
                for i in range(0, len(relevant_cuis), batch_size):
                    batch = relevant_cuis[i:i+batch_size]
                    
                    payload = {"cuis": batch, "cross_context": False}
                    if limit_context:
                        payload["limit_context"] = limit_context
                    
                    try:
                        async with session.post(f"{self.url}/subnet/", json=payload,
                                              headers=headers) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                nodes, edges = data.get("output", ([], []))
                                
                                # Process nodes
                                for node in nodes:
                                    normalized = self._normalize_cui(node)
                                    if normalized:
                                        all_cuis.add(normalized)
                                
                                # Process edges (parent -> child relationships)
                                for edge in edges:
                                    if isinstance(edge, (list, tuple)) and len(edge) == 2:
                                        parent = self._normalize_cui(edge[0])
                                        child = self._normalize_cui(edge[1])
                                        if parent and child:
                                            parent_to_children[parent].append(child)
                                            child_to_parents[child].append(parent)
                                            all_cuis.update([parent, child])
                    
                    except Exception as e:
                        logger.warning(f"API batch failed: {e}")
        
        except Exception as e:
            logger.error(f"Failed to build hierarchy from API: {str(e)}")
        
        hierarchy = {
            'child_to_parents': dict(child_to_parents),
            'parent_to_children': dict(parent_to_children),
            'all_cuis': all_cuis
        }
        
        logger.info(f"Built hierarchy from API: {len(all_cuis)} CUIs total")
        return hierarchy

# ============================================
# ENHANCED CUI REDUCER (MODIFIED FROM CODE 1)
# ============================================

class EnhancedCUIReducer:
    
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        subnet_api_url: str = None,  # NEW: for subnet API
        mrrel_table: str = "MRREL",
        cui_description_table: str = "cui_descriptions",
        cui_embeddings_table: str = "cui_embeddings",
        cui_narrower_table: str = "cui_narrower_concepts",
        max_hierarchy_depth: int = 1,
        query_timeout: int = 300
    ):
        try:
            self.client = bigquery.Client(project=project_id)
        except Exception as e:
            raise Exception(f"Failed to initialize BigQuery client: {str(e)}")
        
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.mrrel_table = mrrel_table
        self.cui_description_table = cui_description_table
        self.cui_embeddings_table = cui_embeddings_table
        self.cui_narrower_table = cui_narrower_table
        self.max_hierarchy_depth = max_hierarchy_depth
        self.query_timeout = query_timeout
        
        # NEW: Initialize subnet API builder if URL provided
        self.subnet_api_url = subnet_api_url
        if subnet_api_url:
            self.hierarchy_builder = SubnetAPIHierarchyBuilder(subnet_api_url, query_timeout)
        
        # Cache
        self._hierarchy_cache = None
        self._ic_scores_cache = None
        self._description_cache = {}
    
    def reduce(
        self,
        input_cuis: List[str],
        target_reduction: float = 0.85,
        ic_threshold: Optional[float] = None,
        ic_percentile: float = 50.0,
        semantic_threshold: float = 0.88,
        use_semantic_clustering: bool = True,
        adaptive_threshold: bool = False
    ) -> Tuple[List[str], ReductionStats]:

        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)

        # Use API or MRREL based on configuration
        api_start = time.time()
        if self.subnet_api_url:
            # Use subnet API for hierarchy
            hierarchy = asyncio.run(self.hierarchy_builder.build_hierarchy_from_api(input_cuis))
        else:
            # Use original MRREL method
            hierarchy = self._build_hierarchy_safe(input_cuis)
        api_time = time.time() - api_start
        
        ic_scores = self._compute_ic_scores_safe(hierarchy)

        # MEDIAN IC THRESHOLD ONLY
        if ic_threshold is None:
            ic_threshold = float(np.median(list(ic_scores.values())))
            logger.info(f"Using MEDIAN IC threshold: {ic_threshold:.3f}")

        rolled_up = self._semantic_rollup_with_ic_safe(
            input_cuis, hierarchy, ic_scores, ic_threshold
        )
        after_rollup = len(rolled_up)

        final_cuis = rolled_up
        if use_semantic_clustering:
            final_cuis = self._semantic_clustering_safe(rolled_up, ic_scores)

        final_count = len(final_cuis)

        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=after_rollup,
            final_count=final_count,
            ic_rollup_reduction_pct=self._safe_percentage(
                initial_count - after_rollup, initial_count
            ),
            semantic_clustering_reduction_pct=self._safe_percentage(
                after_rollup - final_count, initial_count
            ),
            total_reduction_pct=self._safe_percentage(
                initial_count - final_count, initial_count
            ),
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_threshold,
            hierarchy_size=len(hierarchy.get("all_cuis", [])),
            api_call_time=api_time if self.subnet_api_url else 0.0
        )

        return final_cuis, stats
    
    def _build_hierarchy_safe(self, relevant_cuis: List[str]) -> Dict:
        """Original MRREL-based hierarchy building (fallback)"""
        if self._hierarchy_cache is not None:
            return self._hierarchy_cache
        
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(relevant_cuis)
        
        try:
            visited = set()
            frontier = set(relevant_cuis)
            
            for depth in range(self.max_hierarchy_depth):
                if not frontier:
                    break
                
                logger.info(f"Fetching hierarchy depth {depth + 1}: {len(frontier)} CUIs")
                
                query = f"""
                SELECT DISTINCT cui1, cui2, rel
                FROM `{self.project_id}.{self.dataset_id}.{self.mrrel_table}`
                WHERE (cui1 IN UNNEST(@frontier) OR cui2 IN UNNEST(@frontier))
                  AND rel IN ('PAR','CHD')
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("frontier", "STRING", list(frontier))
                    ]
                )
                
                try:
                    query_job = self.client.query(query, job_config=job_config)
                    df = query_job.result(timeout=self.query_timeout).to_dataframe()
                    
                    if len(df) == 0:
                        logger.warning(f"No relationships found at depth {depth + 1}")
                        break
                    
                    logger.info(f"Retrieved {len(df)} relationships")
                    
                    next_frontier = set()
                    for _, row in df.iterrows():
                        cui1, cui2, rel = str(row['cui1']), str(row['cui2']), str(row['rel'])
                        
                        if rel == 'PAR':
                            parent_to_children[cui1].append(cui2)
                            child_to_parents[cui2].append(cui1)
                        elif rel == 'CHD':
                            parent_to_children[cui2].append(cui1)
                            child_to_parents[cui1].append(cui2)
                        
                        all_cuis.update([cui1, cui2])
                        
                        if cui1 not in visited:
                            next_frontier.add(cui1)
                        if cui2 not in visited:
                            next_frontier.add(cui2)
                    
                    visited.update(frontier)
                    frontier = next_frontier - visited
                    
                except Exception as e:
                    logger.warning(f"MRREL query failed at depth {depth + 1}: {str(e)}")
                    break
            
        except Exception as e:
            logger.error(f"Failed to build hierarchy: {str(e)}")
        
        hierarchy = {
            'child_to_parents': dict(child_to_parents),
            'parent_to_children': dict(parent_to_children),
            'all_cuis': all_cuis
        }
        
        self._hierarchy_cache = hierarchy
        logger.info(f"Built hierarchy: {len(all_cuis)} CUIs total")
        
        return hierarchy
    
    # ALL OTHER METHODS REMAIN EXACTLY THE SAME AS CODE 1
    def _compute_ic_scores_safe(self, hierarchy: Dict) -> Dict[str, float]:
        if self._ic_scores_cache is not None:
            return self._ic_scores_cache
        
        parent_to_children = hierarchy.get('parent_to_children', {})
        all_cuis = hierarchy.get('all_cuis', set())
        total = len(all_cuis)
        
        if total == 0:
            return {}
        
        try:
            descendant_counts = {}
            
            def count_descendants(cui: str, visited: Set[str] = None) -> int:
                if visited is None:
                    visited = set()
                if cui in visited or cui in descendant_counts:
                    return descendant_counts.get(cui, 0)
                
                visited.add(cui)
                children = parent_to_children.get(cui, [])
                count = len(children)
                
                for child in children:
                    count += count_descendants(child, visited)
                
                descendant_counts[cui] = count
                return count
            
            logger.info("Computing IC scores...")
            for cui in all_cuis:
                if cui not in descendant_counts:
                    try:
                        count_descendants(cui)
                    except RecursionError:
                        logger.warning(f"Recursion limit for {cui}")
                        descendant_counts[cui] = 0
            
            ic_scores = {}
            for cui in all_cuis:
                desc_count = descendant_counts.get(cui, 0)
                ic = -np.log((desc_count + 1) / total)
                ic_scores[cui] = max(0.0, ic)
            
            self._ic_scores_cache = ic_scores
            
            if ic_scores:
                values = list(ic_scores.values())
                logger.info(f"IC scores: {len(ic_scores)} CUIs, range [{min(values):.2f}, {max(values):.2f}]")
            
            return ic_scores
            
        except Exception as e:
            logger.error(f"Failed to compute IC scores: {str(e)}")
            return {cui: 5.0 for cui in all_cuis}
    
    def _semantic_rollup_with_ic_safe(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        ic_threshold: float
    ) -> List[str]:
        try:
            child_to_parents = hierarchy.get('child_to_parents', {})
            rolled_up = {}
            
            for cui in cui_list:
                try:
                    ancestors = []
                    visited = set()
                    queue = deque([cui])
                    
                    while queue and len(visited) < 100:
                        current = queue.popleft()
                        if current in visited:
                            continue
                        visited.add(current)
                        
                        for parent in child_to_parents.get(current, []):
                            if parent not in visited:
                                ancestors.append(parent)
                                queue.append(parent)
                    
                    candidates = [cui] + ancestors
                    valid = [c for c in candidates if ic_scores.get(c, 0) >= ic_threshold]
                    
                    if valid:
                        rolled_up[cui] = min(valid, key=lambda c: ic_scores.get(c, float('inf')))
                    else:
                        rolled_up[cui] = cui
                        
                except Exception as e:
                    logger.debug(f"Rollup failed for {cui}: {str(e)}")
                    rolled_up[cui] = cui
            
            return list(set(rolled_up.values()))
            
        except Exception as e:
            logger.error(f"Semantic rollup failed: {str(e)}")
            return cui_list
    
    def _semantic_clustering_safe(
        self,
        cui_list: List[str],
        ic_scores: Dict[str, float],
        similarity_threshold: float = None,
        min_intra_cluster_distance: float = 0.25
    ) -> List[str]:
        """Clusters CUIs, then prunes CLOSE CUIs within each cluster"""
        
        if len(cui_list) <= 1:
            return cui_list
        
        try:
            logger.info(f"Fetching embeddings for {len(cui_list)} CUIs...")
            
            query = f"""
            SELECT REF_CUI as cui, REF_Embedding as embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
            WHERE REF_CUI IN UNNEST(@cuis)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)
                ]
            )
            
            df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            
            if df.empty:
                logger.warning("No embeddings found, skipping clustering")
                return cui_list
            
            embeddings = np.vstack(df['embedding'].values)
            cuis = np.array(df['cui'].values)
            
            # STEP 1: COMPUTE 95th PERCENTILE THRESHOLD
            sim_matrix = 1 - cosine_distances(embeddings)
            upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]
            perc95 = np.percentile(upper_tri, 95)
            
            logger.info(f"Dynamic similarity threshold : {perc95:.4f}")
            
            # STEP 2: COARSE CLUSTERING
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - perc95,
                metric='cosine',
                linkage='average'
            )
            
            labels = clustering.fit_predict(embeddings)
            
            # STEP 3: INTRA-CLUSTER DISTANCE FILTERING
            final_cuis = []
            
            for cluster_id in np.unique(labels):
                idx = np.where(labels == cluster_id)[0]
                cluster_cuis = cuis[idx]
                cluster_embeddings = embeddings[idx]
                
                if len(cluster_cuis) == 1:
                    final_cuis.append(cluster_cuis[0])
                    continue
                
                dist_matrix = cosine_distances(cluster_embeddings)
                
                order = sorted(
                    range(len(cluster_cuis)),
                    key=lambda i: ic_scores.get(cluster_cuis[i], 0),
                    reverse=True
                )
                
                kept = []
                for i in order:
                    too_close = any(dist_matrix[i, j] < min_intra_cluster_distance for j in kept)
                    if not too_close:
                        kept.append(i)
                
                final_cuis.extend(cluster_cuis[kept])
            
            logger.info(
                f"Distance-based clustering reduced {len(cui_list)} → {len(final_cuis)} CUIs"
            )
            
            return list(set(final_cuis))
            
        except Exception as e:
            logger.error(f"Clustering failed: {str(e)}")
            return cui_list
    
    def get_cui_descriptions(self, cui_list: List[str]) -> Dict[str, str]:
        if not cui_list:
            return {}
        
        uncached = [c for c in cui_list if c not in self._description_cache]
        
        if uncached:
            try:
                query = f"""
                SELECT CUI as cui, Definition as description
                FROM `{self.project_id}.{self.dataset_id}.{self.cui_description_table}`
                WHERE CUI IN UNNEST(@cuis)
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("cuis", "STRING", uncached)
                    ]
                )
                
                query_job = self.client.query(query, job_config=job_config)
                df = query_job.result(timeout=30).to_dataframe()
                new_descriptions = dict(zip(df['cui'], df['description']))
                self._description_cache.update(new_descriptions)
                
            except Exception as e:
                logger.error(f"Failed to fetch descriptions: {str(e)}")
        
        return {cui: self._description_cache.get(cui, "N/A") for cui in cui_list}
    
    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        return (numerator / denominator * 100) if denominator > 0 else 0.0

# ============================================
# MAIN FUNCTION (MODIFIED FROM CODE 1)
# ============================================

def run_cui_reduction(
    texts: List[str],
    project_id: str,
    dataset_id: str,
    api_url: str,
    subnet_api_url: str = None,  # NEW: optional subnet API URL
    mrrel_table: str = "MRREL",
    cui_description_table: str = "cui_descriptions",
    cui_embeddings_table: str = "cui_embeddings",
    cui_narrower_table: str = "cui_narrower_concepts",
    target_reduction: float = 0.85,
    ic_percentile: float = 50.0,
    semantic_threshold: float = 0.88,
    use_semantic_clustering: bool = True,
    adaptive_threshold: bool = False
) -> Tuple[List[str], List[str], Dict[str, str], Optional[ReductionStats]]:
    
    try:
        # Initialize API client
        api_client = CUIAPIClient(
            api_base_url=api_url,
            timeout=60,
            top_k=3
        )
        
        # Initialize reducer (will use subnet API if URL provided)
        cui_reducer = EnhancedCUIReducer(
            project_id=project_id,
            dataset_id=dataset_id,
            subnet_api_url=subnet_api_url,  # Pass subnet API URL
            mrrel_table=mrrel_table,
            cui_description_table=cui_description_table,
            cui_embeddings_table=cui_embeddings_table,
            cui_narrower_table=cui_narrower_table,
            max_hierarchy_depth=1,
            query_timeout=300
        )
        
        # Extract CUIs from texts
        logger.info(f"Extracting CUIs from {len(texts)} text(s)...")
        initial_cuis = api_client.extract_cuis_batch(texts)
        
        if not initial_cuis:
            logger.warning("No CUIs extracted from texts")
            return [], [], {}, None
        
        # Filter CUIs to only ICD, SNOMED, and LOINC
        logger.info(f"Filtering {len(initial_cuis)} CUIs to retain only ICD, SNOMED, and LOINC...")
        filtered_cuis = filter_allowed_cuis(initial_cuis, project_id, dataset_id)
        
        if not filtered_cuis:
            logger.warning("No CUIs remain after filtering")
            return [], [], {}, None
        
        # Reduce CUIs
        start_time = time.time()
        reduced_cuis, stats = cui_reducer.reduce(
            filtered_cuis,
            target_reduction=target_reduction,
            ic_percentile=ic_percentile,
            semantic_threshold=semantic_threshold,
            use_semantic_clustering=use_semantic_clustering,
            adaptive_threshold=adaptive_threshold
        )
        
        # Add API call time to stats if not using subnet API
        if not subnet_api_url:
            stats.api_call_time = time.time() - start_time - stats.processing_time
        
        # Get descriptions
        logger.info("Fetching descriptions...")
        descriptions = cui_reducer.get_cui_descriptions(reduced_cuis)
        
        return list(initial_cuis), reduced_cuis, descriptions, stats
        
    except Exception as e:
        logger.error(f"Pipeline error: {str(e)}")
        import traceback
        traceback.print_exc()
        return [], [], {}, None

# ============================================
# EXAMPLE USAGE
# ============================================

if __name__ == "__main__":
    # Configuration
    project_id = "your_project"
    dataset_id = "your_dataset"
    api_url = "your_cui_extraction_api_url"
    subnet_api_url = "your_subnet_api_url"  # Optional: set to None to use MRREL
    
    texts = ["pharmacotherapy depression type 2 diabetes"]
    
    # Run with subnet API for hierarchy
    initial_cuis, reduced_cuis, descriptions, stats = run_cui_reduction(
        texts=texts,
        project_id=project_id,
        dataset_id=dataset_id,
        api_url=api_url,
        subnet_api_url=subnet_api_url,  # Use subnet API
        mrrel_table="MRREL",
        cui_description_table="cui_descriptions",
        cui_embeddings_table="cui_embeddings",
        cui_narrower_table="cui_narrower_concepts"
    )
    
    if stats:
        print(f"Reduction complete: {stats.initial_count} → {stats.final_count} ({stats.total_reduction_pct:.1f}%)")
        if stats.api_call_time > 0:
            print(f"Hierarchy API time: {stats.api_call_time:.2f}s")
