import asyncio
import aiohttp
import subprocess
import re
import logging
from pathlib import Path
import json
from typing import List, Dict, Set, Optional, Tuple
from collections import defaultdict, deque
import time
from dataclasses import dataclass, asdict
import hashlib
import numpy as np
from google.cloud import bigquery
import nest_asyncio
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import requests
from requests.adapters import HTTPAdapter, Retry
import threading

nest_asyncio.apply()


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GCPTokenProvider:
    _lock = threading.Lock()
    _token = None
    _expiry = 0

    @classmethod
    def get_headers(cls, force: bool = False) -> Dict[str, str]:
        with cls._lock:
            now = time.time()
            if not force and cls._token and now < cls._expiry:
                return cls._token

            proc = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=10
            )
            if proc.returncode != 0:
                raise RuntimeError(proc.stderr)

            token = proc.stdout.strip()
            if not token:
                raise RuntimeError("Empty token from gcloud")

            cls._token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            cls._expiry = now + 3300  # ~55 minutes
            return cls._token

# Data Classes

@dataclass
class ReductionStats:
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    group_stats: Dict = None
    missing_embeddings_count: int = 0

    def to_dict(self):
        return asdict(self)


# Filter CUIs

def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    try:
        logger.info(f"Filtering {len(cuis)} CUIs to retain only ICD, SNOMED, and LOINC...")
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('CCSR_ICD10CM','CCSR_ICD10PCS','DMDICD10','ICD10','ICD10AE','ICD10AM','ICD10AMAE',
                      'ICD10CM','ICD10DUT','ICD10PCS','ICD9CM','ICPC2ICD10DUT','ICPC2ICD10ENG',
                      'SNOMEDCT_US', 'LOINC')
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
        )
        df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        allowed_cuis = df['CUI'].tolist()
        logger.info(f"{len(allowed_cuis)} CUIs after filter")
        return allowed_cuis
    except Exception as e:
        logger.error(f"Failed to filter CUIs by SAB: {str(e)}")
        return []


# CUI API

class CUIAPIClient:

    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3, max_retries: int = 3):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.max_retries = max_retries

        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        if not texts:
            return set()
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = GCPTokenProvider.get_headers()
        logger.info(f"Extracting CUIs from {len(texts)} text(s)...")
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            if response.status_code == 401 and retry_auth:
                logger.warning("Token expired, refreshing...")
                GCPTokenProvider.get_headers(force=True)
                return self.extract_cuis_batch(texts, retry_auth=False)
            response.raise_for_status()
            data = response.json()
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
            return all_cuis
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return set()


# Enhanced Reducer with Dynamic Semantic Type Grouping

class EnhancedCUIReducer:
    
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        subnet_api_url: str,
        cui_description_table: str,
        cui_embeddings_table: str,
        cui_narrower_table: str,
        max_hierarchy_depth: int = 5,
        query_timeout: int = 300
    ):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.subnet_api_url = subnet_api_url
        self.cui_description_table = cui_description_table
        self.cui_embeddings_table = cui_embeddings_table
        self.cui_narrower_table = cui_narrower_table
        self.max_hierarchy_depth = max_hierarchy_depth
        self.query_timeout = query_timeout

        self._hierarchy_cache = {}
        self._ic_scores_cache = {}
        self._description_cache = {}
        self._missing_embeddings_total = 0
     

    # Public API - Modified for semantic grouping
    def reduce(
        self,
        input_cuis: List[str],
        ic_percentile: int = 75,
        similarity_threshold: float = 0.88,
        distance_from_centroid_threshold: float = 0.3
    ) -> Tuple[List[str], ReductionStats]:

        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        
        logger.info(f"Starting semantic-group based reduction for {initial_count} CUIs")

        # Step 3: Group by semantic type (dynamically from MRSTY)
        semantic_groups = self._group_by_semantic_type_dynamic(input_cuis)
        
        logger.info(f"Created {len(semantic_groups)} semantic groups")
        
        all_reduced_cuis = []
        group_stats = {}
        total_after_ic = 0
        
        for group_name, group_cuis in semantic_groups.items():
            if not group_cuis:
                continue
            
            # Step 4: Build hierarchy within group
            group_hierarchy = self._build_hierarchy_depthwise(group_cuis)
            
            # Step 5: Compute IC scores within group
            group_ic_scores = self._compute_ic_scores_within_group(
                group_hierarchy, 
                group_cuis,
                group_name
            )
            
            # Step 6: Keep only CUIs >= 75th percentile IC
            if group_ic_scores:
                ic_threshold = np.percentile(
                    list(group_ic_scores.values()), 
                    ic_percentile
                )
                
                high_ic_cuis = [
                    cui for cui in group_cuis 
                    if group_ic_scores.get(cui, 0) >= ic_threshold
                ]
            else:
                # If no IC scores computed (e.g., single CUI), keep all
                high_ic_cuis = group_cuis
            
            total_after_ic += len(high_ic_cuis)
            
            # Step 7-8: Semantic clustering and diversity selection
            if len(high_ic_cuis) > 1:
                group_reduced = self._cluster_and_select_diverse(
                    high_ic_cuis,
                    similarity_threshold,
                    distance_from_centroid_threshold
                )
            else:
                group_reduced = high_ic_cuis
            
            all_reduced_cuis.extend(group_reduced)
            
            group_stats[group_name] = {
                'original': len(group_cuis),
                'after_ic_filter': len(high_ic_cuis),
                'final': len(group_reduced),
                'reduction_pct': (1 - len(group_reduced)/len(group_cuis)) * 100 if group_cuis else 0
            }
        
        # Step 9: Remove duplicates
        final_cuis = list(set(all_reduced_cuis))
        final_count = len(final_cuis)
        
        # Step 10: Validate coverage
        coverage_score = self._calculate_coverage(input_cuis, final_cuis)
        logger.info(f"Coverage score: {coverage_score:.2%} of original CUIs have representative")

        # Fetch descriptions for final CUIs
        self._fetch_descriptions(final_cuis)

        # Stats
        logger.info(f"Total missing embeddings: {self._missing_embeddings_total}")
        
        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=total_after_ic,
            final_count=final_count,
            ic_rollup_reduction_pct=self._safe_percentage(initial_count - total_after_ic, initial_count),
            semantic_clustering_reduction_pct=self._safe_percentage(total_after_ic - final_count, initial_count),
            total_reduction_pct=self._safe_percentage(initial_count - final_count, initial_count),
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_percentile,
            hierarchy_size=sum(len(h.get("all_cuis", [])) for h in self._hierarchy_cache.values()),
            api_call_time=0.0,
            group_stats=group_stats,
            missing_embeddings_count=self._missing_embeddings_total
        )

        return final_cuis, stats

    # FIXED INDENTATION - This method was incorrectly indented
    def _group_by_semantic_type_dynamic(self, cuis: List[str]) -> Dict[str, List[str]]:
        """
        Dynamically group CUIs by their semantic type names (STY) from MRSTY table
        This creates natural groupings based on the actual semantic types present
        """
        
        # Query to get all CUI-STY mappings for input CUIs
        query = f"""
        WITH cui_types AS (
            SELECT DISTINCT 
                CUI,
                TUI,
                STY  -- Semantic Type Name
            FROM `{self.project_id}.{self.dataset_id}.MRSTY`
            WHERE CUI IN UNNEST(@cuis)
        )
        SELECT * FROM cui_types
        ORDER BY STY, CUI
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", cuis)
            ]
        )
        
        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        
        if df.empty:
            logger.warning("No semantic type information found in MRSTY")
            return {'UNKNOWN': cuis}
        
        # Group CUIs by STY (Semantic Type Name)
        semantic_groups = defaultdict(set)
        cui_to_types = defaultdict(set)
        
        for _, row in df.iterrows():
            cui = row['CUI']
            sty = row['STY']  # Using STY (semantic type name) as group key
            
            semantic_groups[sty].add(cui)
            cui_to_types[cui].add(sty)
        
        # Handle CUIs with multiple semantic types
        # Assign each CUI to only one group (the most specific or first alphabetically)
        final_groups = defaultdict(list)
        
        for cui in cuis:
            if cui not in cui_to_types:
                # CUI not found in MRSTY
                final_groups['UNKNOWN'].append(cui)
            else:
                types = cui_to_types[cui]
                if len(types) == 1:
                    # Single semantic type - straightforward assignment
                    final_groups[list(types)[0]].append(cui)
                else:
                    # Multiple types - choose based on priority
                    # Priority: More specific types (longer names) over general ones
                    sorted_types = sorted(types, key=lambda x: (-len(x), x))
                    final_groups[sorted_types[0]].append(cui)
        
        # Convert to regular dict
        result = dict(final_groups)
        
        return result

    # Modified IC computation for within-group
    def _compute_ic_scores_within_group(
        self, 
        hierarchy: Dict, 
        group_cuis: List[str],
        group_name: str
    ) -> Dict[str, float]:
        """Compute IC scores relative to the group, not global hierarchy"""
        
        cache_key = f"{group_name}_{tuple(sorted(group_cuis))}"
        if cache_key in self._ic_scores_cache:
            return self._ic_scores_cache[cache_key]
        
        parent_to_children = hierarchy.get("parent_to_children", {})
        group_set = set(group_cuis)
        
        # Count descendants ONLY within this semantic group
        descendant_counts = {}
        
        def count_group_descendants(cui: str, visited: Set[str] = None) -> int:
            if visited is None:
                visited = set()
            if cui in visited or cui not in group_set:
                return 0
            visited.add(cui)
            
            children = [c for c in parent_to_children.get(cui, []) if c in group_set]
            count = len(children)
            for child in children:
                count += count_group_descendants(child, visited)
            
            descendant_counts[cui] = count
            return count
        
        for cui in group_cuis:
            if cui not in descendant_counts:
                try:
                    count_group_descendants(cui)
                except RecursionError:
                    descendant_counts[cui] = 0
        
        # Calculate IC scores normalized by group size
        group_size = len(group_cuis)
        ic_scores = {}
        for cui in group_cuis:
            desc_count = descendant_counts.get(cui, 0)
            # IC = -log(P) where P = (descendants + 1) / group_size
            ic_scores[cui] = max(0.0, -np.log((desc_count + 1) / group_size))
        
        self._ic_scores_cache[cache_key] = ic_scores
        return ic_scores

    # Fixed: Better handling of missing embeddings
    def _cluster_and_select_diverse(
        self,
        cui_list: List[str],
        similarity_threshold: float,
        distance_threshold: float
    ) -> List[str]:
        """
        Clusters CUIs and selects all CUIs that are far from cluster centroids
        """
        
        if len(cui_list) <= 1:
            return cui_list
        
        try:
            # Fetch embeddings
            query = f"""
            SELECT REF_CUI as cui, REF_Embedding as embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
            WHERE REF_CUI IN UNNEST(@cuis)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)
                ]
            )
            
            df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            
            if df.empty:
                # No embeddings at all - return all CUIs
                return cui_list
            
            # Track missing CUIs without logging each one
            missing_cuis = set(cui_list) - set(df['cui'].values)
            if missing_cuis:
                self._missing_embeddings_total += len(missing_cuis)
            
            # Only cluster CUIs with embeddings
            if len(df) < 2:
                # Not enough CUIs with embeddings to cluster
                return cui_list
            
            embeddings = np.vstack(df['embedding'].values)
            cuis = np.array(df['cui'].values)
            
            # Perform clustering
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - similarity_threshold,
                metric='cosine',
                linkage='average'
            )
            
            labels = clustering.fit_predict(embeddings)
            
            # Select diverse CUIs from each cluster
            final_cuis = []
            
            for cluster_id in np.unique(labels):
                idx = np.where(labels == cluster_id)[0]
                cluster_cuis = cuis[idx]
                cluster_embeddings = embeddings[idx]
                
                if len(cluster_cuis) == 1:
                    final_cuis.append(cluster_cuis[0])
                    continue
                
                # Calculate centroid
                centroid = np.mean(cluster_embeddings, axis=0)
                
                # Calculate distances from centroid
                distances = cosine_distances([centroid], cluster_embeddings)[0]
                
                # Keep ALL CUIs that are far enough from centroid
                far_indices = np.where(distances > distance_threshold)[0]
                
                if len(far_indices) > 0:
                    final_cuis.extend(cluster_cuis[far_indices])
                else:
                    # If none are far enough, keep the farthest one
                    farthest_idx = np.argmax(distances)
                    final_cuis.append(cluster_cuis[farthest_idx])
            
            # Add back missing CUIs (those without embeddings)
            final_cuis.extend(missing_cuis)
            
            return list(set(final_cuis))
            
        except Exception as e:
            logger.error(f"Clustering failed: {str(e)}")
            return cui_list

    # Calculate coverage - simplified logging
    def _calculate_coverage(self, original_cuis: List[str], reduced_cuis: List[str]) -> float:
        """Calculate what percentage of original CUIs have a representative in reduced set"""
        
        if not original_cuis or not reduced_cuis:
            return 0.0
        
        try:
            # Fetch embeddings for coverage calculation
            all_cuis = list(set(original_cuis + reduced_cuis))
            
            query = f"""
            SELECT REF_CUI as cui, REF_Embedding as embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
            WHERE REF_CUI IN UNNEST(@cuis)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", all_cuis)
                ]
            )
            
            df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            
            if df.empty:
                return 0.0
            
            # Create embedding lookup
            embedding_dict = {row['cui']: row['embedding'] for _, row in df.iterrows()}
            
            # Check coverage
            covered_count = 0
            coverage_threshold = 0.85
            
            for orig_cui in original_cuis:
                if orig_cui in reduced_cuis:
                    covered_count += 1
                    continue
                
                if orig_cui not in embedding_dict:
                    continue
                
                orig_embedding = embedding_dict[orig_cui]
                
                # Find if any reduced CUI is similar enough
                for red_cui in reduced_cuis:
                    if red_cui not in embedding_dict:
                        continue
                    
                    red_embedding = embedding_dict[red_cui]
                    similarity = 1 - cosine_distances([orig_embedding], [red_embedding])[0, 0]
                    
                    if similarity >= coverage_threshold:
                        covered_count += 1
                        break
            
            return covered_count / len(original_cuis)
            
        except Exception as e:
            logger.error(f"Coverage calculation failed: {str(e)}")
            return 0.0

    # Existing methods remain the same
    def _build_hierarchy_depthwise(self, cuis: List[str]) -> Dict:
        key = tuple(sorted(cuis))
        if key in self._hierarchy_cache:
            return self._hierarchy_cache[key]

        hierarchy = asyncio.run(self._fetch_hierarchy(cuis))
        self._hierarchy_cache[key] = hierarchy
        return hierarchy

    async def _fetch_hierarchy(self, cuis: List[str]) -> Dict:
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(cuis)

        headers = GCPTokenProvider.get_headers()
        batch_size = 50
        timeout = aiohttp.ClientTimeout(total=self.query_timeout)

        async with aiohttp.ClientSession(timeout=timeout) as session:

            async def fetch_batch(batch):
                async with session.post(
                    f"{self.subnet_api_url}/subnet/",
                    json={"cuis": batch, "cross_context": False},
                    headers=headers
                ) as resp:
                    if resp.status != 200:
                        return [], []
                    data = await resp.json()
                    return data.get("output", ([], []))

            tasks = []
            for i in range(0, len(cuis), batch_size):
                batch = cuis[i:i + batch_size]
                tasks.append(fetch_batch(batch))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, Exception):
                    continue
                nodes, edges = result
                for p, c in edges:
                    p, c = self._normalize_cui(p), self._normalize_cui(c)
                    if p and c:
                        parent_to_children[p].append(c)
                        child_to_parents[c].append(p)
                        all_cuis.update([p, c])

        return {
            "child_to_parents": dict(child_to_parents),
            "parent_to_children": dict(parent_to_children),
            "all_cuis": all_cuis
        }

    def _fetch_descriptions(self, cuis: List[str]):
        to_fetch = [c for c in cuis if c not in self._description_cache]
        if not to_fetch:
            return

        query = f"""
            SELECT CUI AS cui, Definition AS description
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_description_table}`
            WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", to_fetch)]
        )

        df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()

        for _, row in df.iterrows():
            self._description_cache[row['cui']] = row['description']

    @staticmethod
    def _normalize_cui(cui: str) -> Optional[str]:
        m = re.match(r"^(C\d{7})(?:-\d+)?$", str(cui))
        return m.group(1) if m else None

    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        return (numerator / denominator * 100) if denominator else 0.0


# Main

if __name__ == "__main__": 
    project_id = project_id
    dataset_id = dataset
    api_url = url
    subnet_api_url = url1
    cui_desc_table = cui_desc_table
    embedding_table = embedding_table
    descendants_table = descendants_table

    texts = ["pharmacotherapy depression type 2 diabetes"] 
    api_client = CUIAPIClient(api_base_url=api_url)
    extracted_cuis = api_client.extract_cuis_batch(texts)

    filtered_cuis = filter_allowed_cuis(extracted_cuis, project_id, dataset_id)

    reducer = EnhancedCUIReducer(
        project_id,
        dataset_id,
        subnet_api_url=subnet_api_url,
        cui_description_table=cui_desc_table,
        cui_embeddings_table=embedding_table,
        cui_narrower_table=descendants_table
    )

    final_cuis, stats = reducer.reduce(
        filtered_cuis,
        ic_percentile=75,
        similarity_threshold=0.88,
        distance_from_centroid_threshold=0.3
    )

    logger.info(f"Reduction Stats: {stats.to_dict()}")
