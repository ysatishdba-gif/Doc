import asyncio
import aiohttp
import subprocess
import re
import logging
from pathlib import Path
import json
import pickle
from typing import List, Dict, Set, Any, Optional, Tuple
from collections import defaultdict, deque
import time
from dataclasses import dataclass, asdict, field
import hashlib
import numpy as np
from google.cloud import bigquery
import nest_asyncio
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
import requests
from requests.adapters import HTTPAdapter, Retry
import threading

# Apply nest_asyncio for Jupyter compatibility
nest_asyncio.apply()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================
# DATA CLASSES
# ============================================

@dataclass
class ReductionStats:
    """Statistics for CUI reduction process"""
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int = 0
    api_call_time: float = 0.0
    
    def to_dict(self):
        return asdict(self)

# ============================================
# CUI API CLIENT (For initial CUI extraction)
# ============================================

class CUIAPIClient:
    """Client for extracting CUIs from text"""
    
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0
    
    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3, max_retries: int = 3):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.top_k = top_k
        self.max_retries = max_retries
        
        # Configure session with connection pooling
        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Get initial token
        self._update_gcp_token()
    
    def _update_gcp_token(self, force: bool = False):
        """Update GCP authentication token"""
        with self._token_lock:
            current_time = time.time()
            
            if not force and self._cached_token and current_time < self._token_expiry:
                return self._cached_token
            
            try:
                result = subprocess.run(
                    ['gcloud', 'auth', 'print-identity-token'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    universal_newlines=True,
                    timeout=10,
                    check=False
                )
                
                if result.returncode != 0:
                    raise Exception(f"gcloud auth failed: {result.stderr}")
                
                identity_token = result.stdout.strip()
                if not identity_token:
                    raise Exception("Empty token received from gcloud")
                
                self._cached_token = {
                    "Authorization": f"Bearer {identity_token}",
                    "Content-Type": "application/json"
                }
                self._token_expiry = current_time + 3300  # 55 minutes
                
                return self._cached_token
                
            except Exception as e:
                raise Exception(f"Failed to get GCP token: {str(e)}")
    
    def extract_cuis_batch(self, texts: List[str], retry_auth: bool = True) -> Set[str]:
        """Extract CUIs from texts"""
        if not texts:
            return set()
        
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = self._update_gcp_token()
        
        try:
            response = self.session.post(
                self.api_base_url,
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            
            if response.status_code == 401 and retry_auth:
                logger.warning("Token expired, refreshing...")
                self._update_gcp_token(force=True)
                return self.extract_cuis_batch(texts, retry_auth=False)
            
            response.raise_for_status()
            data = response.json()
            
            all_cuis = set()
            for text in texts:
                cuis = data.get(text, [])
                if isinstance(cuis, list):
                    all_cuis.update(str(c) for c in cuis if c)
            
            logger.info(f"Extracted {len(all_cuis)} unique CUIs from {len(texts)} texts")
            return all_cuis
            
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return set()

# ============================================
# SUBNET API HIERARCHY BUILDER
# ============================================

class SubnetAPIHierarchyBuilder:
    """Build hierarchy using subnet API instead of MRREL table"""
    
    def __init__(self, api_url: str, timeout: int = 60):
        self.api_url = api_url.rstrip('/')
        self.timeout = timeout
        self.cache = {}
        self._token = None
        self._token_expiry = 0
        self._token_lock = threading.Lock()
    
    def _get_auth_token(self) -> str:
        """Get GCP authentication token"""
        with self._token_lock:
            current_time = time.time()
            if self._token and current_time < self._token_expiry:
                return self._token
            
            try:
                result = subprocess.run(
                    ['gcloud', 'auth', 'print-identity-token'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=10
                )
                
                if result.returncode != 0:
                    raise Exception(f"gcloud auth failed: {result.stderr}")
                
                self._token = result.stdout.strip()
                self._token_expiry = current_time + 3300
                return self._token
                
            except Exception as e:
                raise Exception(f"Failed to get GCP token: {str(e)}")
    
    async def build_hierarchy_from_api(
        self, 
        relevant_cuis: List[str],
        limit_context: List[str] = None,
        max_hierarchy_depth: int = 1
    ) -> Dict:
        """Build hierarchy using subnet API"""
        
        if not limit_context:
            limit_context = ['SNOMEDCT_US.isa', 'ICD10.Empty', 'LOINC.Empty']
        
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(relevant_cuis)
        
        try:
            headers = {
                "Authorization": f"Bearer {self._get_auth_token()}",
                "Content-Type": "application/json"
            }
            
            async with aiohttp.ClientSession() as session:
                visited = set()
                frontier = set(relevant_cuis)
                
                for depth in range(max_hierarchy_depth):
                    if not frontier:
                        break
                    
                    logger.info(f"Fetching hierarchy depth {depth + 1}: {len(frontier)} CUIs")
                    
                    # Process frontier in batches
                    batch_size = 100
                    frontier_list = list(frontier)
                    all_edges = []
                    
                    for i in range(0, len(frontier_list), batch_size):
                        batch = frontier_list[i:i+batch_size]
                        
                        payload = {
                            "cuis": batch,
                            "cross_context": False,
                            "limit_context": limit_context
                        }
                        
                        try:
                            async with session.post(
                                f"{self.api_url}/subnet/",
                                json=payload,
                                headers=headers,
                                timeout=aiohttp.ClientTimeout(total=self.timeout)
                            ) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    nodes, edges = data.get("output", ([], []))
                                    
                                    # Normalize CUIs
                                    normalized_edges = []
                                    for edge in edges:
                                        if isinstance(edge, (list, tuple)) and len(edge) == 2:
                                            parent = self._normalize_cui(edge[0])
                                            child = self._normalize_cui(edge[1])
                                            if parent and child:
                                                normalized_edges.append((parent, child))
                                    
                                    all_edges.extend(normalized_edges)
                                    
                                    # Add normalized nodes
                                    for node in nodes:
                                        normalized = self._normalize_cui(node)
                                        if normalized:
                                            all_cuis.add(normalized)
                                            
                        except Exception as e:
                            logger.warning(f"API call failed for batch at depth {depth + 1}: {e}")
                    
                    # Process edges to build parent-child relationships
                    next_frontier = set()
                    for parent, child in all_edges:
                        parent_to_children[parent].append(child)
                        child_to_parents[child].append(parent)
                        
                        if parent not in visited:
                            next_frontier.add(parent)
                        if child not in visited:
                            next_frontier.add(child)
                    
                    visited.update(frontier)
                    frontier = next_frontier - visited
                    
                    logger.info(f"Depth {depth + 1}: Found {len(all_edges)} relationships")
            
        except Exception as e:
            logger.error(f"Failed to build hierarchy from API: {str(e)}")
        
        hierarchy = {
            'child_to_parents': dict(child_to_parents),
            'parent_to_children': dict(parent_to_children),
            'all_cuis': all_cuis
        }
        
        logger.info(f"Built hierarchy from API: {len(all_cuis)} CUIs total")
        return hierarchy
    
    def _normalize_cui(self, cui: Any) -> Optional[str]:
        """Normalize CUI format"""
        if not cui:
            return None
        cui_str = str(cui).strip()
        match = re.match(r'^(C\d{7})(?:-\d+)?$', cui_str)
        return match.group(1) if match else None

# ============================================
# ENHANCED CUI REDUCER WITH API
# ============================================

class EnhancedCUIReducerWithAPI:
    """CUI Reducer using subnet API for hierarchy"""
    
    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        subnet_api_url: str,
        cui_description_table: str = "cui_descriptions",
        cui_embeddings_table: str = "cui_embeddings",
        cui_narrower_table: str = "cui_narrower_concepts",
        max_hierarchy_depth: int = 1,
        query_timeout: int = 300
    ):
        try:
            self.client = bigquery.Client(project=project_id)
        except Exception as e:
            raise Exception(f"Failed to initialize BigQuery client: {str(e)}")
        
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.cui_description_table = cui_description_table
        self.cui_embeddings_table = cui_embeddings_table
        self.cui_narrower_table = cui_narrower_table
        self.max_hierarchy_depth = max_hierarchy_depth
        self.query_timeout = query_timeout
        
        # Initialize subnet API hierarchy builder
        self.hierarchy_builder = SubnetAPIHierarchyBuilder(
            api_url=subnet_api_url,
            timeout=query_timeout
        )
        
        # Cache
        self._hierarchy_cache = None
        self._ic_scores_cache = None
        self._description_cache = {}
    
    def reduce(
        self,
        input_cuis: List[str],
        target_reduction: float = 0.85,
        ic_threshold: Optional[float] = None,
        ic_percentile: float = 50.0,
        semantic_threshold: float = 0.88,
        use_semantic_clustering: bool = True,
        adaptive_threshold: bool = False,
        limit_context: List[str] = None
    ) -> Tuple[List[str], ReductionStats]:
        """Main reduction method using API-based hierarchy"""
        
        start_time = time.time()
        input_cuis = list(set(input_cuis))
        initial_count = len(input_cuis)
        
        if initial_count == 0:
            return [], ReductionStats(
                initial_count=0,
                after_ic_rollup=0,
                final_count=0,
                ic_rollup_reduction_pct=0.0,
                semantic_clustering_reduction_pct=0.0,
                total_reduction_pct=0.0,
                processing_time=0.0,
                ic_threshold_used=0.0
            )
        
        # Build hierarchy using API
        api_start = time.time()
        hierarchy = asyncio.run(
            self.hierarchy_builder.build_hierarchy_from_api(
                input_cuis, 
                limit_context=limit_context,
                max_hierarchy_depth=self.max_hierarchy_depth
            )
        )
        api_time = time.time() - api_start
        
        # Compute IC scores
        ic_scores = self._compute_ic_scores_safe(hierarchy)
        
        # Determine threshold
        if ic_threshold is None and ic_scores:
            ic_threshold = float(np.median(list(ic_scores.values())))
            logger.info(f"Using MEDIAN IC threshold: {ic_threshold:.3f}")
        elif ic_threshold is None:
            ic_threshold = 5.0
        
        # Semantic rollup
        rolled_up = self._semantic_rollup_with_ic_safe(
            input_cuis, hierarchy, ic_scores, ic_threshold
        )
        after_rollup = len(rolled_up)
        
        # Semantic clustering
        final_cuis = rolled_up
        if use_semantic_clustering and len(rolled_up) > 1:
            final_cuis = self._semantic_clustering_safe(rolled_up, ic_scores)
        
        final_count = len(final_cuis)
        
        # Create stats
        stats = ReductionStats(
            initial_count=initial_count,
            after_ic_rollup=after_rollup,
            final_count=final_count,
            ic_rollup_reduction_pct=self._safe_percentage(
                initial_count - after_rollup, initial_count
            ),
            semantic_clustering_reduction_pct=self._safe_percentage(
                after_rollup - final_count, initial_count
            ),
            total_reduction_pct=self._safe_percentage(
                initial_count - final_count, initial_count
            ),
            processing_time=time.time() - start_time,
            ic_threshold_used=ic_threshold,
            hierarchy_size=len(hierarchy.get("all_cuis", [])),
            api_call_time=api_time
        )
        
        return final_cuis, stats
    
    def _compute_ic_scores_safe(self, hierarchy: Dict) -> Dict[str, float]:
        """Compute Information Content scores"""
        if self._ic_scores_cache is not None:
            return self._ic_scores_cache
        
        parent_to_children = hierarchy.get('parent_to_children', {})
        all_cuis = hierarchy.get('all_cuis', set())
        total = len(all_cuis)
        
        if total == 0:
            return {}
        
        try:
            descendant_counts = {}
            
            def count_descendants(cui: str, visited: Set[str] = None) -> int:
                if visited is None:
                    visited = set()
                if cui in visited or cui in descendant_counts:
                    return descendant_counts.get(cui, 0)
                
                visited.add(cui)
                children = parent_to_children.get(cui, [])
                count = len(children)
                
                for child in children:
                    if child not in visited:
                        count += count_descendants(child, visited)
                
                descendant_counts[cui] = count
                return count
            
            logger.info("Computing IC scores...")
            for cui in all_cuis:
                if cui not in descendant_counts:
                    try:
                        count_descendants(cui)
                    except RecursionError:
                        logger.warning(f"Recursion limit for {cui}")
                        descendant_counts[cui] = 0
            
            # Compute IC scores
            ic_scores = {}
            for cui in all_cuis:
                desc_count = descendant_counts.get(cui, 0)
                ic = -np.log((desc_count + 1) / total) if total > 0 else 0
                ic_scores[cui] = max(0.0, ic)
            
            self._ic_scores_cache = ic_scores
            
            if ic_scores:
                values = list(ic_scores.values())
                logger.info(f"IC scores: {len(ic_scores)} CUIs, range [{min(values):.2f}, {max(values):.2f}]")
            
            return ic_scores
            
        except Exception as e:
            logger.error(f"Failed to compute IC scores: {str(e)}")
            return {cui: 5.0 for cui in all_cuis}
    
    def _semantic_rollup_with_ic_safe(
        self,
        cui_list: List[str],
        hierarchy: Dict,
        ic_scores: Dict[str, float],
        ic_threshold: float
    ) -> List[str]:
        """Perform semantic rollup using IC scores"""
        try:
            child_to_parents = hierarchy.get('child_to_parents', {})
            rolled_up = {}
            
            for cui in cui_list:
                try:
                    # Get ancestors using BFS
                    ancestors = []
                    visited = set()
                    queue = deque([cui])
                    
                    while queue and len(visited) < 100:
                        current = queue.popleft()
                        if current in visited:
                            continue
                        visited.add(current)
                        
                        for parent in child_to_parents.get(current, []):
                            if parent not in visited:
                                ancestors.append(parent)
                                queue.append(parent)
                    
                    # Find best ancestor
                    candidates = [cui] + ancestors
                    valid = [c for c in candidates if ic_scores.get(c, 0) >= ic_threshold]
                    
                    if valid:
                        rolled_up[cui] = min(valid, key=lambda c: ic_scores.get(c, float('inf')))
                    else:
                        rolled_up[cui] = cui
                        
                except Exception as e:
                    logger.debug(f"Rollup failed for {cui}: {str(e)}")
                    rolled_up[cui] = cui
            
            return list(set(rolled_up.values()))
            
        except Exception as e:
            logger.error(f"Semantic rollup failed: {str(e)}")
            return cui_list
    
    def _semantic_clustering_safe(
        self,
        cui_list: List[str],
        ic_scores: Dict[str, float],
        similarity_threshold: float = None,
        min_intra_cluster_distance: float = 0.25
    ) -> List[str]:
        """Cluster CUIs based on semantic similarity"""
        
        if len(cui_list) <= 1:
            return cui_list
        
        try:
            logger.info(f"Fetching embeddings for {len(cui_list)} CUIs...")
            
            query = f"""
            SELECT REF_CUI as cui, REF_Embedding as embedding
            FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
            WHERE REF_CUI IN UNNEST(@cuis)
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis", "STRING", cui_list)
                ]
            )
            
            df = self.client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
            
            if df.empty:
                logger.warning("No embeddings found, skipping clustering")
                return cui_list
            
            # Prepare embeddings
            embeddings = np.vstack(df['embedding'].values)
            cuis = np.array(df['cui'].values)
            
            # Compute similarity threshold
            sim_matrix = 1 - cosine_distances(embeddings)
            upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]
            perc95 = np.percentile(upper_tri, 95) if len(upper_tri) > 0 else 0.9
            
            logger.info(f"Dynamic similarity threshold: {perc95:.4f}")
            
            # Perform clustering
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1 - perc95,
                metric='cosine',
                linkage='average'
            )
            
            labels = clustering.fit_predict(embeddings)
            
            # Select representatives from each cluster
            final_cuis = []
            
            for cluster_id in np.unique(labels):
                idx = np.where(labels == cluster_id)[0]
                cluster_cuis = cuis[idx]
                cluster_embeddings = embeddings[idx]
                
                if len(cluster_cuis) == 1:
                    final_cuis.append(cluster_cuis[0])
                    continue
                
                # Compute intra-cluster distances
                dist_matrix = cosine_distances(cluster_embeddings)
                
                # Sort by IC score
                order = sorted(
                    range(len(cluster_cuis)),
                    key=lambda i: ic_scores.get(cluster_cuis[i], 0),
                    reverse=True
                )
                
                # Keep CUIs that are far enough apart
                kept = []
                for i in order:
                    too_close = any(
                        dist_matrix[i, j] < min_intra_cluster_distance 
                        for j in kept
                    )
                    if not too_close:
                        kept.append(i)
                
                final_cuis.extend(cluster_cuis[kept])
            
            logger.info(
                f"Distance-based clustering reduced {len(cui_list)} → {len(final_cuis)} CUIs"
            )
            
            return list(set(final_cuis))
            
        except Exception as e:
            logger.error(f"Clustering failed: {str(e)}")
            return cui_list
    
    def get_cui_descriptions(self, cui_list: List[str]) -> Dict[str, str]:
        """Get descriptions for CUIs"""
        if not cui_list:
            return {}
        
        uncached = [c for c in cui_list if c not in self._description_cache]
        
        if uncached:
            try:
                query = f"""
                SELECT CUI as cui, Definition as description
                FROM `{self.project_id}.{self.dataset_id}.{self.cui_description_table}`
                WHERE CUI IN UNNEST(@cuis)
                """
                
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ArrayQueryParameter("cuis", "STRING", uncached)
                    ]
                )
                
                df = self.client.query(query, job_config=job_config).result(timeout=30).to_dataframe()
                
                for _, row in df.iterrows():
                    self._description_cache[row['cui']] = row['description']
                
            except Exception as e:
                logger.error(f"Failed to fetch descriptions: {str(e)}")
        
        return {cui: self._description_cache.get(cui, "N/A") for cui in cui_list}
    
    @staticmethod
    def _safe_percentage(numerator: float, denominator: float) -> float:
        """Calculate percentage safely"""
        return (numerator / denominator * 100) if denominator > 0 else 0.0

# ============================================
# UTILITY FUNCTIONS
# ============================================

def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    """Filter CUIs to only ICD, LOINC, or SNOMED"""
    if not cuis:
        return []
    
    try:
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('CCSR_ICD10CM','CCSR_ICD10PCS','DMDICD10','ICD10',
                      'ICD10AE','ICD10AM','ICD10AMAE','ICD10CM','ICD10DUT',
                      'ICD10PCS','ICD9CM','ICPC2ICD10DUT','ICPC2ICD10ENG',
                      'SNOMEDCT_US', 'LOINC')
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
        )
        
        query_job = client.query(query, job_config=job_config)
        df = query_job.result(timeout=60).to_dataframe()
        allowed_cuis = df['CUI'].tolist()
        
        logger.info(f"{len(allowed_cuis)} CUIs after filtering (from {len(cuis)} initial)")
        return allowed_cuis
        
    except Exception as e:
        logger.error(f"Failed to filter CUIs by SAB: {str(e)}")
        return list(cuis)

# ============================================
# MAIN FUNCTION
# ============================================

def run_cui_reduction_with_api(
    texts: List[str],
    project_id: str,
    dataset_id: str,
    api_url: str,
    subnet_api_url: str,
    cui_description_table: str = "cui_descriptions",
    cui_embeddings_table: str = "cui_embeddings",
    cui_narrower_table: str = "cui_narrower_concepts",
    target_reduction: float = 0.85,
    ic_percentile: float = 50.0,
    semantic_threshold: float = 0.88,
    use_semantic_clustering: bool = True,
    adaptive_threshold: bool = False,
    limit_context: List[str] = None
) -> Tuple[List[str], List[str], Dict[str, str], Optional[ReductionStats]]:
    """
    Main pipeline function
    
    Args:
        texts: Input texts to extract CUIs from
        project_id: GCP project ID
        dataset_id: BigQuery dataset ID
        api_url: URL for CUI extraction API
        subnet_api_url: URL for subnet/hierarchy API
        cui_description_table: Table name for CUI descriptions
        cui_embeddings_table: Table name for CUI embeddings
        cui_narrower_table: Table name for narrower concepts
        target_reduction: Target reduction percentage
        ic_percentile: IC score percentile threshold
        semantic_threshold: Semantic similarity threshold
        use_semantic_clustering: Whether to use semantic clustering
        adaptive_threshold: Whether to use adaptive thresholding
        limit_context: Context vocabularies for hierarchy
    
    Returns:
        Tuple of (initial_cuis, reduced_cuis, descriptions, stats)
    """
    
    try:
        # Initialize API client for CUI extraction
        api_client = CUIAPIClient(
            api_base_url=api_url,
            timeout=60,
            top_k=3
        )
        
        # Initialize reducer with subnet API
        cui_reducer = EnhancedCUIReducerWithAPI(
            project_id=project_id,
            dataset_id=dataset_id,
            subnet_api_url=subnet_api_url,
            cui_description_table=cui_description_table,
            cui_embeddings_table=cui_embeddings_table,
            cui_narrower_table=cui_narrower_table,
            max_hierarchy_depth=1,
            query_timeout=300
        )
        
        # Extract CUIs from texts
        logger.info(f"Extracting CUIs from {len(texts)} text(s)...")
        initial_cuis = api_client.extract_cuis_batch(texts)
        
        if not initial_cuis:
            logger.warning("No CUIs extracted from texts")
            return [], [], {}, None
        
        # Filter CUIs
        logger.info(f"Filtering {len(initial_cuis)} CUIs...")
        filtered_cuis = filter_allowed_cuis(initial_cuis, project_id, dataset_id)
        
        if not filtered_cuis:
            logger.warning("No CUIs remain after filtering")
            return list(initial_cuis), [], {}, None
        
        # Set default context if not provided
        if not limit_context:
            limit_context = [
                'LNC.Empty', 'SNOMEDCT_US.Empty', 'SNOMEDCT_US.isa',
                'ICD10.Empty', 'ICD10AM.Empty', 'ICD10CM.Empty',
                'ICD10PCS.Empty', 'CCSR_ICD10PCS.Empty'
            ]
        
        # Reduce CUIs
        logger.info(f"Reducing {len(filtered_cuis)} filtered CUIs...")
        reduced_cuis, stats = cui_reducer.reduce(
            filtered_cuis,
            target_reduction=target_reduction,
            ic_percentile=ic_percentile,
            semantic_threshold=semantic_threshold,
            use_semantic_clustering=use_semantic_clustering,
            adaptive_threshold=adaptive_threshold,
            limit_context=limit_context
        )
        
        # Get descriptions
        logger.info("Fetching descriptions...")
        descriptions = cui_reducer.get_cui_descriptions(reduced_cuis)
        
        logger.info(f"Pipeline complete: {len(initial_cuis)} → {len(reduced_cuis)} CUIs")
        
        return list(initial_cuis), reduced_cuis, descriptions, stats
        
    except Exception as e:
        logger.error(f"Pipeline error: {str(e)}")
        import traceback
        traceback.print_exc()
        return [], [], {}, None

# ============================================
# EXAMPLE USAGE
# ============================================

if __name__ == "__main__":
    # Configuration
    project_id = "your_project"
    dataset_id = "your_dataset"
    api_url = "https://your-cui-extraction-api.com/extract"
    subnet_api_url = "https://your-subnet-api.com"
    
    # Test texts
    texts = ["pharmacotherapy depression type 2 diabetes"]
    
    # Run the pipeline
    initial_cuis, reduced_cuis, descriptions, stats = run_cui_reduction_with_api(
        texts=texts,
        project_id=project_id,
        dataset_id=dataset_id,
        api_url=api_url,
        subnet_api_url=subnet_api_url,
        cui_description_table="cui_descriptions",
        cui_embeddings_table="cui_embeddings",
        cui_narrower_table="cui_narrower_concepts",
        target_reduction=0.85,
        use_semantic_clustering=True
    )
    
    # Print results
    if stats:
        print(f"\n{'='*50}")
        print(f"CUI REDUCTION RESULTS")
        print(f"{'='*50}")
        print(f"Initial CUIs: {stats.initial_count}")
        print(f"After IC Rollup: {stats.after_ic_rollup}")
        print(f"Final CUIs: {stats.final_count}")
        print(f"Total Reduction: {stats.total_reduction_pct:.1f}%")
        print(f"Processing Time: {stats.processing_time:.2f}s")
        print(f"API Call Time: {stats.api_call_time:.2f}s")
        print(f"Hierarchy Size: {stats.hierarchy_size}")
        print(f"IC Threshold Used: {stats.ic_threshold_used:.3f}")
        print(f"{'='*50}")
