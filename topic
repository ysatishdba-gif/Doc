# CUI Reduction Pipeline — Technical Documentation

## Overview

The CUI Reduction Pipeline takes a large set of CUIs (Concept Unique Identifiers) extracted by MetaMap from clinical text and reduces them to a focused, query-relevant subset organized into semantic topics. These reduced CUIs and topics serve as retrieval tags for document search.

**Example run:**
```
Input:  "Patient has severe pain in left knee with swelling"
        19,716 raw CUIs from MetaMap

Output: 777 relevant CUIs in ~80 semantic topics
        Full audit trail for every removal
```

The pipeline is fully adaptive — zero hardcoded thresholds, zero hardcoded anatomy lists. Every decision is data-driven from UMLS ontology, embedding geometry, and query context.

---

## Pipeline Stages

```
19,716 input CUIs
  → Stage 1: SAB Filter         → 8,600  (-56%)
  → Stage 2: Semantic Type Filter → 8,600  (~0%)
  → Stage 3: Term Dedup          → 8,599  (-0.01%)
  → Stage 4: Hierarchy Reduction  → 6,382  (-26%)
  → Stage 5A: ScaNN Topic Clustering
  → Stage 5B: Within-Topic Redundancy Removal  → ~1,892  (-70%)
  → Stage 5C: Cross-Topic Dedup   → ~1,879  (-0.7%)
  → Stage 6: Retention Scoring     (rank only)
  → Stage 7: Relevance Filter     → 777    (-59%)
  → Stage 8: Topic Pruning         → ~80 topics (aligned with 777)
```

---

## Stage 1: SAB Filter

**Purpose:** Remove CUIs from untrusted or low-quality source vocabularies.

**How it works:** Each CUI in UMLS comes from one or more Source ABbreviations (SABs) — the vocabularies that define it. The pipeline keeps only CUIs present in allowed SABs.

**Allowed SABs (configurable):**
- `SNOMEDCT_US` — most descriptive clinical terms, includes laterality and qualifiers
- `ICD10CM` — diagnosis codes, lateralized (e.g., M17.11 = "Primary osteoarthritis, right knee")
- `ICD10PCS` — procedure codes
- `ICD9CM` — legacy diagnosis codes
- `LNC` — LOINC lab and measurement codes

**Example:**
```
C0231749  "Knee pain"              → SABs: [SNOMEDCT_US, ICD10CM]  → KEPT
C2127239  "Left anterior knee pain" → SABs: [MEDCIN]               → REMOVED
C0745571  "Knee pain chronic"       → SABs: [CCPSS, CHV]           → REMOVED
```

**Note:** This stage is a temporary upstream placeholder. In production, SAB filtering happens before CUIs reach this pipeline.

---

## Stage 2: Semantic Type Filter

**Purpose:** Remove CUIs belonging to semantic types that are never useful for document retrieval.

**How it works:** Every CUI in UMLS has one or more Semantic Types (STYs) from the UMLS Semantic Network — a hierarchy of 133 types like "Sign or Symptom" (T184), "Disease or Syndrome" (T047), "Body Part, Organ, or Organ Component" (T023). The pipeline removes CUIs whose types are in an exclusion list.

**Example exclusions:**
- T116 "Amino Acid, Peptide, or Protein" — too granular for document search
- T126 "Enzyme" — molecular level, not clinical

**Example:**
```
C0231749  "Knee pain"         → T184 (Sign or Symptom)    → KEPT
C0002645  "Amyloid protein"   → T116 (Amino Acid)         → REMOVED
```

---

## Stage 3: Term Dedup

**Purpose:** Collapse CUIs that map to the exact same normalized preferred term.

**How it works:**
1. Each CUI's preferred term is normalized: lowercased, punctuation stripped, whitespace collapsed
2. If two CUIs share the same normalized term, keep the one with higher Information Content (IC)

**Example:**
```
C0231749  normalized: "knee pain"    IC=10.2  → KEPT
C9999999  normalized: "knee pain"    IC=8.7   → REMOVED (duplicate term, lower IC)
```

**Impact:** Minimal (~1 CUI typically). Most CUIs have unique preferred terms.

---

## Stage 4: Hierarchy Reduction

**Purpose:** When a parent concept and its child concept both exist, remove the parent — the child is more specific and more useful for retrieval.

**How it works:**
1. Builds a lookup of CUI → ancestors using the UMLS parent→child graph (NetworkX)
2. For each CUI, checks if any of its descendants also survive in the set
3. If child exists, shares at least one semantic type with parent, and has IC >= parent IC → parent removed

**Information Content (IC):**
```
IC(concept) = -log₂((descendants + 1) / total_nodes)
```
Rare concepts (few descendants) have high IC. Common concepts (many descendants) have low IC. "Knee pain" (IC=10.2) is more general than "Left knee pain" (IC=12.4).

**Example:**
```
UMLS graph:
  C0030193  "Pain"                    IC=6.1   (grandparent)
    └── C0231749  "Knee pain"         IC=10.2  (parent)
          └── C4545783  "Left knee pain"  IC=12.4  (child)

Both "Knee pain" and "Left knee pain" in set:
  Child has higher IC (12.4 > 10.2) ✓
  Shares semantic type T184 ✓
  → "Knee pain" REMOVED, "Left knee pain" KEPT
```

**Impact:** 8,599 → 6,382 (-26%). Removes generalized ancestors when specific descendants exist.

---

## Stage 5A: ScaNN Topic Clustering

**Purpose:** Group the surviving CUIs into coherent semantic topics. Topics become document retrieval tags.

**How it works — step by step:**

### Step 1: Group by Semantic Type
CUIs are first split by their UMLS semantic type. Each type is clustered independently.

```
Sign or Symptom (T184):     248 CUIs → cluster separately
Disease or Syndrome (T047):  412 CUIs → cluster separately
Body Part (T023):            189 CUIs → cluster separately
...
```

**Why pre-group?** Smaller, semantically coherent groups produce tighter clusters. "Left knee pain" and "Right knee pain" stand out more in a 248-CUI Sign/Symptom group than in a 6,382-CUI mixed pool. Tighter clusters → more redundancy detected → better reduction.

### Step 2: Fetch Embeddings
Each CUI has a pre-computed 3072-dimensional embedding vector (Gemini embedding-001 model) stored in BigQuery. These vectors encode semantic meaning — similar concepts have similar vectors.

```
C4545783  "Left knee pain"     → [0.012, -0.034, 0.087, ...] (3072 dims)
C0409326  "Anterior knee pain" → [0.011, -0.033, 0.089, ...] (3072 dims)
  cosine similarity: 0.97 → very close in embedding space
```

### Step 3: Build ScaNN Index
ScaNN (Scalable Nearest Neighbors) is Google's approximate nearest neighbor library. For each semantic type group:

1. **Normalize** all embedding vectors to unit length (L2 norm)
2. **Build index** with dot-product similarity (equivalent to cosine on normalized vectors)
3. **Configure search** with parameters tuned for the group size:
   - `num_leaves`: √n (square root of group size) — controls partition granularity
   - `num_leaves_to_search`: 80% of num_leaves — search breadth
   - `reordering_num_neighbors`: 2× candidates — re-ranking precision

```python
# For a group of 248 CUIs:
num_leaves = √248 ≈ 16
num_leaves_to_search = 13  (80% of 16)

searcher = (
    scann.scann_ops_pybind.builder(vectors, num_neighbors=10, distance_measure="dot_product")
    .tree(num_leaves=16, num_leaves_to_search=13)
    .score_ah(dimensions_per_block=2)
    .reorder(50)  # re-rank top 50 for precision
    .build()
)
```

### Step 4: Find Neighbors and Build Clusters
For each CUI in the group, ScaNN finds its K nearest neighbors (K=10 or group_size-1, whichever is smaller).

```
C4545783 "Left knee pain" neighbors:
  1. C0409326 "Anterior knee pain"       dist=0.03
  2. C5551296 "Knee pain severity"       dist=0.08
  3. C4695974 "Knee pain while sitting"  dist=0.11
  4. C0162297 "Joint swelling"           dist=0.22
  ...
```

### Step 5: Adaptive Threshold Detection
Instead of a hardcoded distance threshold, the pipeline detects each CUI's natural neighborhood boundary:

1. Sort neighbor distances: [0.03, 0.08, 0.11, 0.22, ...]
2. Compute gaps between consecutive distances: [0.05, 0.03, 0.11, ...]
3. Find the first gap > 2× median gap → that's the "elbow"
4. All neighbors closer than the elbow distance are in the same cluster

```
Distances: [0.03, 0.08, 0.11, 0.22, 0.25, 0.28]
Gaps:      [0.05, 0.03, 0.11, 0.03, 0.03]
Median gap: 0.03
2× median:  0.06

Gap at position 3 (0.11) > 0.06 → elbow at distance 0.22
Tight radius: 0.11 (last distance before elbow)
Guard radius: 0.22 (elbow distance)
```

### Step 6: Union-Find Clustering
CUI pairs within each other's tight radius are merged into the same cluster using union-find (disjoint set). This creates transitive clusters:

```
A ↔ B (dist 0.03) → same cluster
B ↔ C (dist 0.08) → same cluster
Therefore A, B, C → same cluster (topic)
```

**Output per semantic type:**
```
Sign or Symptom → 12 topics:
  topic_3: [C4545783, C0409326, C5551296, C4695974, ...]  "knee pain variants"
  topic_7: [C0038999, C0162297, C4081756, ...]             "swelling variants"
  ...
```

### Deterministic Results
ScaNN uses random seeds for tree partitioning. The pipeline sets `seed=42` for reproducible results — same input always produces same clusters.

---

## Stage 5B: Within-Topic Redundancy Removal

**Purpose:** Inside each topic, remove near-duplicate CUIs while preserving clinically distinct concepts.

**How it works:**
For each topic, iterate through CUIs sorted by IC (ascending — least specific first). For each CUI, check if it's redundant with any higher-IC CUI in the same topic:

1. **Distance check:** Is embedding distance within the tight threshold? If not → KEEP (too different)
2. **Safety check 1 — `terms_diverge`:** Do the terms share ANY tokens? If zero overlap (e.g., "Gonalgia" vs "Left knee pain") → KEEP both (can't confirm redundancy from terms alone)
3. **Safety check 2 — `qualifier_conflict`:** Are they nearly identical but differ by 1-2 tokens? If yes → KEEP both (e.g., "left knee pain" vs "right knee pain")

### terms_diverge
```python
def terms_diverge(tokens_a, tokens_b):
    """Zero shared tokens = can't confirm same concept."""
    return len(tokens_a & tokens_b) == 0
```

```
"Gonalgia" tokens: {gonalgia}
"Left knee pain" tokens: {left, knee, pain}
Shared: {} → DIVERGENT → both survive
```

### qualifier_conflict
```python
def qualifier_conflict(tokens_a, tokens_b):
    """Nearly identical terms with 1-2 token swaps = qualifier distinction."""
    shared = tokens_a & tokens_b
    only_a = tokens_a - tokens_b
    only_b = tokens_b - tokens_a
    overlap = len(shared) / len(tokens_a | tokens_b)
    # Both sides must differ (not just one adding extra detail)
    if overlap >= 0.5 and 1 <= len(only_a) <= 2 and 1 <= len(only_b) <= 2:
        return True
    return False
```

**Examples of qualifier conflicts (both survive):**
```
"left knee pain" vs "right knee pain"
  shared: {knee, pain}  only_a: {left}  only_b: {right}
  overlap: 66%  both sides differ by 1 → BLOCKED ✓

"upper abdomen" vs "lower abdomen"
  shared: {abdomen}  only_a: {upper}  only_b: {lower}
  overlap: 50%  both sides differ by 1 → BLOCKED ✓

"acute sinusitis" vs "chronic sinusitis"
  shared: {sinusitis}  only_a: {acute}  only_b: {chronic}
  overlap: 50%  both sides differ by 1 → BLOCKED ✓
```

**Example of allowed collapse:**
```
"knee pain" vs "knee pain syndrome"
  shared: {knee, pain}  only_a: {}  only_b: {syndrome}
  only_a = 0 (less than 1) → NOT a qualifier swap → collapses correctly
```

### Token Sources: `all_tokens`
Safety checks use `all_tokens` — built from the preferred term of each allowed SAB (SNOMEDCT_US, ICD10CM, etc.), not just the single UMLS preferred term. This gives 3-5 curated terms per CUI instead of one.

```
CUI: C4545783
  UMLS preferred: "Gonalgia" → term_tokens: {gonalgia}
  SNOMED preferred: "Pain in left knee joint (finding)" → {pain, left, knee, joint, finding}
  ICD10CM preferred: "Pain in left knee" → {pain, left, knee}
  
  all_tokens: {gonalgia, pain, left, knee, joint, finding}
```

Now safety checks see "left" and "knee" even when the preferred term is "Gonalgia".

### Tiebreaker
When two CUIs in a topic are near-duplicates (pass all safety checks), the one with higher IC survives. Higher IC = more specific concept.

---

## Stage 5C: Cross-Topic Dedup

**Purpose:** Catch duplicates that ended up in different semantic type topics.

**How it works:**
1. Flatten all surviving CUIs into one list
2. Embed the query text (one API call)
3. Compute pairwise cosine distances for all CUIs
4. Adaptive threshold: 5th percentile of all pairwise distances, capped at 0.05
5. For each near-duplicate pair across topics:
   - Apply same safety checks (terms_diverge, qualifier_conflict)
   - Tiebreaker: keep the CUI with higher text alignment (cosine with query embedding), not IC

**Why text alignment instead of IC?** Cross-topic pairs come from different semantic types (e.g., "Sign/Symptom" vs "Finding"). IC isn't comparable across types. Text alignment measures which CUI better represents the query.

**Example:**
```
topic_3 (Sign/Symptom): "Knee joint swelling"   align=0.74
topic_7 (Finding):      "Swollen knee"           align=0.72

embedding distance: 0.04 → near-duplicate (below 0.05 cap)
qualifier_conflict? No
→ keep "Knee joint swelling" (higher alignment to query)
```

**Impact:** Minimal (~13 removals). Most cross-type duplicates are already caught by hierarchy reduction.

---

## Stage 6: Retention Scoring

**Purpose:** Rank surviving CUIs by relevance to the input text. No CUIs are removed — this is purely ordering.

**How it works:**
1. Embed the input text using Gemini embedding-001 (reuses the embedding from cross-topic dedup — zero extra API calls)
2. Compute cosine similarity between text embedding and each CUI's embedding
3. Sort descending by alignment score

```
text_vec = embed("Patient has severe pain in left knee with swelling")

C4545783  "Left knee pain"                cosine = 0.755  ← rank 1
C0409326  "Anterior knee pain"            cosine = 0.734  ← rank 2
C4081756  "Swelling of knee"              cosine = 0.728  ← rank 3
...
C2237078  "Swelling of right upper arm"   cosine = 0.646  ← rank 800+
...
C2894873  "Ankylosis, right wrist"        cosine = 0.555  ← rank 1800+
```

### explains_cuis
Each surviving CUI carries a list of CUIs it "explains" — the CUIs that were removed in its favor during reduction. This provides provenance:

```
C4545783 "Left knee pain" explains:
  C0231749 "Knee pain" (removed by hierarchy)
  C4695245 "Knee pain frequency" (removed by embedding redundancy)
```

---

## Stage 7: Relevance Filter (Query-Token Boost + Otsu)

**Purpose:** Remove CUIs that are irrelevant to the query. This is the final precision stage — everything that survives goes to document search.

**The problem:** Pure embedding alignment compresses all musculoskeletal concepts into a 0.55-0.76 band. "Knee pain" scores 0.73 but "Ankle swelling" scores 0.65 — not enough gap for any threshold to separate them cleanly. There's no natural cliff in the score distribution.

**The solution:** Boost CUI scores by how many query tokens they share. This widens the gap between CUIs that mention query anatomy and those that don't.

### Step 1: Extract Query Tokens
```
"Patient has severe pain in left knee with swelling"
→ tokenize → {patient, has, severe, pain, in, left, knee, with, swelling}
```

No stop words removed. Common tokens like "in", "with" appear in almost every CUI and boost everyone equally — zero discriminating effect. The signal comes from content tokens like "knee", "left", "pain", "swelling".

### Step 2: Compute Boosted Scores
For each CUI:
```
boosted_score = alignment + (0.1 × overlap_ratio)

where overlap_ratio = |query_tokens ∩ cui_all_tokens| / |query_tokens|
```

**Example:**
```
CUI: "Anterior knee pain"
  all_tokens: {anterior, knee, pain, ...}
  overlap with query: {knee, pain} = 2 tokens
  overlap_ratio: 2/9 = 0.222
  boost: 0.1 × 0.222 = 0.022
  boosted: 0.734 + 0.022 = 0.756  ← pushed UP significantly

CUI: "Swelling of right upper arm"
  all_tokens: {swelling, right, upper, arm, ...}
  overlap with query: {swelling} = 1 token
  overlap_ratio: 1/9 = 0.111
  boost: 0.1 × 0.111 = 0.011
  boosted: 0.646 + 0.011 = 0.657  ← barely moved

CUI: "Ankylosis, right wrist"
  all_tokens: {ankylosis, right, wrist, ...}
  overlap with query: {} = 0 tokens
  boost: 0
  boosted: 0.555 + 0 = 0.555  ← no change
```

### Step 3: Otsu's Method for Automatic Threshold

Now the boosted scores show two clear groups. Otsu's method finds the optimal threshold to separate them.

**How Otsu works:**
1. Bin the boosted scores into 256 levels (like a histogram)
2. For every possible threshold T, compute "between-class variance":
   - Group A: all scores above T (assumed relevant)
   - Group B: all scores below T (assumed noise)
   - Between-class variance = weight_A × weight_B × (mean_A - mean_B)²
3. The threshold T that maximizes this variance is the optimal split

This is the same algorithm used in image processing to separate foreground from background. It works because it finds where the two groups are **most different** from each other.

**For our example:**
```
Boosted scores distribution:

  0.72-0.86: ████████████  (knee/pain/swelling CUIs, boosted)
  0.67-0.72: ██████        (related musculoskeletal)
  0.55-0.67: ████████████████████  (unrelated, no boost)

Otsu scans all possible thresholds:
  T=0.60: small variance (both groups overlap)
  T=0.65: medium variance
  T=0.70: MAXIMUM variance (cleanest split) ← SELECTED
  T=0.75: variance drops (cutting into relevant group)

Otsu threshold: 0.70
```

**Key property:** Zero parameters. Otsu is fully adaptive — a vague query produces a lower threshold, a specific query produces a higher one.

### Step 4: Apply Cutoff
Everything with boosted score ≥ 0.70 survives. Everything below is removed with an audit entry.

```
C4545783 | boosted=0.756 | "Left knee pain"             → KEPT ✓
C0409326 | boosted=0.756 | "Anterior knee pain"         → KEPT ✓
C4081756 | boosted=0.750 | "Swelling of knee"           → KEPT ✓
...
C2237078 | boosted=0.657 | "Swelling of right upper arm" → REMOVED ✓
C2894873 | boosted=0.555 | "Ankylosis, right wrist"     → REMOVED ✓
```

**~1,879 → 777**

---

## Stage 8: Topic Pruning

**Purpose:** Align topics with the final CUI list. Topics are document retrieval tags — they must only contain CUIs that survived all stages.

**How it works:**
1. Build a set of the final 777 CUIs
2. For each topic, keep only CUIs that are in the final set
3. Drop topics that become empty (all their CUIs were removed)

```
Before pruning:
  topic_3: [C4545783, C0409326, C5551296, C2237078, ...]  (20 CUIs)
  topic_12: [C2894873, C2893824, C2901732, ...]            (8 CUIs)

After pruning:
  topic_3: [C4545783, C0409326, C5551296, ...]  (15 CUIs) — C2237078 removed
  topic_12: DROPPED (all 8 CUIs were noise)
```

---

## Metadata Fetching

Each CUI needs metadata from UMLS tables in BigQuery. Fetched once, cached, shared across all stages.

### What's fetched per CUI:
- **preferred_term:** ISPREF=Y, longest English string
- **normalized_term:** lowercased, punctuation stripped
- **term_tokens:** tokenized preferred term
- **all_tokens:** tokenized preferred terms from each allowed SAB (SNOMEDCT_US, ICD10CM, etc.) — 3-5 curated terms per CUI
- **semantic_types:** list of STY names (e.g., "Sign or Symptom")
- **semantic_type_ids:** list of TUI codes (e.g., "T184")
- **source_vocabs:** list of SABs the CUI belongs to
- **ic_score:** Information Content (computed from graph)

### BQ Query (simplified):
```sql
-- One preferred term per allowed SAB
WITH sab_pref AS (
  SELECT CUI, SAB, STR,
    ROW_NUMBER() OVER (
      PARTITION BY CUI, SAB
      ORDER BY CASE WHEN ISPREF='Y' THEN 0 ELSE 1 END, LENGTH(STR) DESC
    ) AS sab_rn
  FROM MRCONSO
  WHERE CUI IN UNNEST(@cuis) AND LAT='ENG'
    AND SAB IN ('SNOMEDCT_US','ICD10CM','ICD10PCS','ICD9CM','LNC')
)
SELECT CUI, pref_term, tuis, stys, sabs,
       ARRAY_AGG(DISTINCT sp.STR) AS sab_terms  -- one per SAB
FROM ... LEFT JOIN sab_pref sp ON ... AND sp.sab_rn = 1
```

### Batching
CUIs are batched in groups of 5,000 for BigQuery (avoids query parameter limits). Metadata and embeddings are fetched once across all input texts, then shared.

---

## Embedding Architecture

**Model:** Gemini embedding-001 (text-embedding-004 equivalent)
**Dimensions:** 3,072
**Normalization:** L2-normalized to unit length

### Pre-computed CUI Embeddings
Each CUI's preferred term was embedded and stored in BigQuery:
```sql
SELECT CUI, embedding  -- ARRAY<FLOAT64> of 3072 elements
FROM cui_embeddings
WHERE CUI IN UNNEST(@cuis)
```

### Runtime Text Embedding
The input text is embedded at runtime via Vertex AI GenAI SDK:
```python
response = client.models.embed_content(
    model="gemini-embedding-001",
    contents="Patient has severe pain in left knee with swelling"
)
text_vec = normalize(response.embeddings[0].values)  # 3072-dim
```

**Cost:** 1 API call per input text (~100ms, ~$0.0001).

---

## Safety Mechanisms Summary

| Mechanism | What it protects | Where it runs |
|---|---|---|
| `terms_diverge` | Concepts with unrelated names (Gonalgia vs Left knee pain) | Stage 5B, 5C |
| `qualifier_conflict` | Clinically distinct qualifiers (left vs right, acute vs chronic) | Stage 5B, 5C |
| `all_tokens` from SABs | Ensures safety checks see laterality even when preferred term is opaque | Stage 5B, 5C |
| Token boost | Separates query-relevant from query-irrelevant CUIs | Stage 7 |
| Otsu threshold | Adaptive cutoff with no hardcoded values | Stage 7 |
| Audit trail | Every removal is traceable and verifiable | All stages |

---

## Audit Trail

Every CUI removal is logged as an `AuditEntry`:
```python
AuditEntry(
    removed_cui="C0231749",
    removed_term="Knee pain",
    kept_cui="C4545783",
    kept_term="Left knee pain",
    reason="ancestor_subsumed",
    detail="child IC 12.4 >= parent IC 10.2, shared semtype T184"
)
```

**Reason codes:**
- `sab_filtered` — not in allowed SABs
- `semtype_excluded` — excluded semantic type
- `term_duplicate` — identical normalized term
- `ancestor_subsumed` — parent removed in favor of child
- `embedding_redundant` — near-duplicate in embedding space
- `cross_topic_duplicate` — same concept across topics
- `below_relevance_threshold` — below Otsu cutoff after token boost

---

## Validation Cells

The notebook includes inspection cells to verify pipeline behavior:

**Cell 19:** Main pipeline output — top 20 by retention score, topic list with counts

**Cell 20:** Inspect all CUIs under a specific topic

**Cell 21:** Inspect which CUIs a given CUI "explains" (what was removed in its favor)

**Cell 22:** Search any term across both retained and removed CUIs — traces what happened to it

**Cell 23:** Relevance filter validation — shows cutoff, bottom 10 kept (borderline), top 10 removed (check these), bottom 10 removed (clear noise), distribution stats

---

## Usage — Query Side vs Document Side

The same pipeline runs on both sides of retrieval:

**Query processing:**
```
Query text → MetaMap → 19,716 CUIs → reduce() → 777 CUIs in ~80 topics
```

**Document indexing:**
```
Document → chunks → per-chunk MetaMap → CUIs → reduce() per chunk → union of all chunk results
```

Each chunk uses its own text as context, so relevance filter adapts per chunk:
```
Chunk 1: "left knee pain with swelling"
  → boosts knee/pain/swelling CUIs → keeps knee CUIs

Chunk 2: "history of type 2 diabetes on metformin"
  → boosts diabetes/metformin CUIs → keeps diabetes CUIs

Document topics = union(chunk_1_topics, chunk_2_topics)
```

Nothing is lost at the document level — the relevance filter is per-chunk, and the document gets the union of all chunk results.

---

## Performance

| Metric | Value |
|---|---|
| Pipeline latency | ~3-5s for 19,716 CUIs |
| API calls | 1 embedding call per text (~100ms) |
| Cost per text | ~$0.0001 |
| BQ queries | 2 batched (metadata + embeddings) |
| Memory | ~50MB for 20K CUI embeddings |
| Reproducibility | Deterministic (ScaNN seed=42) |
