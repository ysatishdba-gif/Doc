"""
COMPLETE PIPELINE: MODULE 1 (CUI REDUCTION) + MODULE 2 (TOPIC DISCOVERY)
Single cell execution
"""

import time
import threading
from typing import List, Dict, Optional
from dataclasses import dataclass
import numpy as np
import networkx as nx
import pickle
import psutil
import requests
from google.cloud import bigquery
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor, as_completed
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import subprocess
import pandas as pd

# ═══════════════════════════════════════════════════════════════════
# MODULE 1: CUI EXTRACTION & REDUCTION
# ═══════════════════════════════════════════════════════════════════

# ------------------------- THREAD-SAFE PRINT -------------------------
print_lock = threading.Lock()
def thread_safe_print(msg: str):
    with print_lock:
        print(msg, flush=True)

# ------------------------- FULLY ADAPTIVE LRU CACHE -------------------------
class FullyAdaptiveLRUCache:
    """LRU cache that grows dynamically and evicts only on memory pressure"""
    def __init__(self):
        self.cache = {}
        self.order = []
        self.lock = threading.RLock()
    
    def get(self, key):
        with self.lock:
            if key in self.cache:
                self.order.remove(key)
                self.order.append(key)
                return self.cache[key]
            return None
    
    def put(self, key, value):
        with self.lock:
            if key in self.cache:
                self.order.remove(key)
            self.cache[key] = value
            self.order.append(key)
            self._evict_if_needed()
    
    def _evict_if_needed(self):
        mem = psutil.virtual_memory()
        while mem.available < 0.2 * mem.total and self.order:
            oldest = self.order.pop(0)
            del self.cache[oldest]
            mem = psutil.virtual_memory()

# ------------------------- HIERARCHY CLIENT -------------------------
class HierarchyClient:
    def __init__(self, network_obj, ic_scores: Optional[Dict[str, float]] = None):
        self.network = network_obj
        self.ic_scores = ic_scores or {}
        self.ancestors_cache = FullyAdaptiveLRUCache()
        self.children_cache = FullyAdaptiveLRUCache()
        self.ic_cache = FullyAdaptiveLRUCache()
        self.lock = threading.RLock()
    
    def get_children(self, cui: str) -> List[str]:
        cached = self.children_cache.get(cui)
        if cached is not None:
            return cached
        children = list(self.network.successors(cui)) if self.network.has_node(cui) else []
        self.children_cache.put(cui, children)
        return children
    
    def get_parents(self, cui: str) -> List[str]:
        return list(self.network.predecessors(cui)) if self.network.has_node(cui) else []
    
    def get_ancestors(self, cui: str, max_depth: int = 10) -> List[List[str]]:
        key = (cui, max_depth)
        cached = self.ancestors_cache.get(key)
        if cached is not None:
            return cached
        queue = [(0, [cui])]
        visited = set()
        paths = []
        while queue:
            depth, path = queue.pop(0)
            node = path[-1]
            if node in visited:
                continue
            visited.add(node)
            if depth >= max_depth or not self.network.has_node(node):
                paths.append(path)
                continue
            for parent in self.get_parents(node):
                queue.append((depth + 1, path + [parent]))
        self.ancestors_cache.put(key, paths)
        return paths
    
    def get_ic_score(self, cui: str) -> float:
        cached = self.ic_cache.get(cui)
        if cached is not None:
            return cached
        if cui in self.ic_scores:
            ic = self.ic_scores[cui]
        else:
            paths = self.get_ancestors(cui)
            if paths:
                avg_depth = sum(len(p) for p in paths) / len(paths)
                ic = min(10.0, 2.0 + np.log1p(avg_depth) * 2.0)
            else:
                ic = 3.0
        self.ic_cache.put(cui, ic)
        return ic

# ------------------------- EDGE DETECTOR -------------------------
class EdgeDetectorAdaptive:
    def __init__(self, hierarchy: HierarchyClient):
        self.h = hierarchy

    def adjust_ic(self, cui: str) -> str:
        ic = self.h.get_ic_score(cui)
        target_min, target_max, target_opt = 4.0, 7.0, 5.5
        if target_min <= ic <= target_max:
            return cui
        paths = self.h.get_ancestors(cui, max_depth=20)
        best, best_dist = cui, abs(ic - target_opt)
        for path in paths:
            for ancestor in path:
                a_ic = self.h.get_ic_score(ancestor)
                dist = abs(a_ic - target_opt)
                if target_min <= a_ic <= target_max and dist < best_dist:
                    best, best_dist = ancestor, dist
        return best

    def select_representatives(self, group: set) -> List[str]:
        if not group:
            return []
        max_reps = max(1, int(len(group) ** 0.5))
        scored = [(c, abs(self.h.get_ic_score(c) - 5.5)) for c in group]
        scored.sort(key=lambda x: x[1])
        reps = [self.adjust_ic(c[0]) for c in scored[:max_reps]]
        return list(set(reps))

# ------------------------- CLUSTERER -------------------------
class ClustererAdaptive:
    def __init__(self, hierarchy: HierarchyClient):
        self.h = hierarchy

    def cluster(self, cuis: List[str]) -> Dict[str, set]:
        buckets = {}
        for cui in cuis:
            paths = self.h.get_ancestors(cui, max_depth=5)
            ancestor = paths[0][0] if paths and paths[0] else cui
            buckets.setdefault(ancestor, set()).add(cui)
        return buckets

# ------------------------- REDUCTION ENGINE -------------------------
class ReductionEngineAdaptive:
    def __init__(self, hierarchy: HierarchyClient):
        self.h = hierarchy
        self.clusterer = ClustererAdaptive(hierarchy)
        self.edge_detector = EdgeDetectorAdaptive(hierarchy)

    def reduce_cuis(self, cuis: List[str]):
        cuis = list(set(cuis))
        buckets = self.clusterer.cluster(cuis)
        reduced = []
        for ancestor, group in buckets.items():
            reps = self.edge_detector.select_representatives(group)
            reduced.extend(reps)
        return list(set(reduced)), len(buckets)

# ------------------------- CUI EXTRACTOR -------------------------
class CUIExtractor:
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.session = requests.Session()

        tmp = subprocess.run(
            ['gcloud', 'auth', 'print-identity-token'],
            stdout=subprocess.PIPE,
            universal_newlines=True
        )
        token = tmp.stdout.strip()

        self.headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }

        retry = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract_for_text(self, text: str) -> List[str]:
        try:
            payload = {
                "query_texts": [text],
                "top_k": 3
            }

            resp = self.session.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=200
            )

            thread_safe_print(f"[Extractor] Status: {resp.status_code}")

            resp.raise_for_status()

            data = resp.json()
            cuis = []

            for v in data.values():
                if isinstance(v, list):
                    cuis.extend(v)

            return list(set(map(str, cuis)))

        except Exception as e:
            thread_safe_print(f"CUI extraction failed: {str(e)}")
            return []

# ------------------------- SAB FILTER -------------------------
ALLOWED_SABS = ['ICD10CM', 'ICD10PCS', 'ICD9CM', 'SNOMEDCT_US', 'LOINC']

def filter_by_sab(cuis: List[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    try:
        client = bigquery.Client(project=project_id)
        filtered = []
        for i in range(0, len(cuis), 2000):
            batch = cuis[i:i+2000]
            query = f"""
            SELECT DISTINCT CUI
            FROM `{project_id}.{dataset_id}.MRCONSO`
            WHERE CUI IN UNNEST(@cuis)
              AND SAB IN UNNEST(@sabs)
            """
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ArrayQueryParameter("cuis","STRING",batch),
                    bigquery.ArrayQueryParameter("sabs","STRING",ALLOWED_SABS)
                ]
            )
            results = client.query(query, job_config=job_config).result(timeout=200)
            filtered.extend([row.CUI for row in results])
        return filtered
    except Exception as e:
        thread_safe_print(f"SAB filter failed: {e}")
        return []

# ------------------------- EMBEDDING-BASED REDUCTION -------------------------
def fetch_cui_embeddings(cuis: List[str], project_id: str, dataset_id: str, table_name: str) -> Dict[str, np.ndarray]:
    client = bigquery.Client(project=project_id)
    embeddings = {}
    for i in range(0, len(cuis), 2000):
        batch = cuis[i:i+2000]
        query = f"""
        SELECT CUI, embedding
        FROM `{project_id}.{dataset_id}.{table_name}`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis","STRING",batch)]
        )
        results = client.query(query, job_config=job_config).result(timeout=200)
        for row in results:
            embeddings[row.CUI] = np.array(row.embedding)
    return embeddings

def cluster_cuis_by_embedding(cuis: List[str], embeddings: Dict[str, np.ndarray], n_clusters=None) -> Dict[int, List[str]]:
    valid_cuis = [c for c in cuis if c in embeddings]
    if not valid_cuis:
        return {}
    X = np.stack([embeddings[c] for c in valid_cuis])
    n_clusters = n_clusters or max(1, int(len(valid_cuis)**0.5))
    clustering = AgglomerativeClustering(n_clusters=n_clusters,metric='cosine',linkage='average')
    labels = clustering.fit_predict(X)
    clusters = {}
    for cui, label in zip(valid_cuis, labels):
        clusters.setdefault(label, []).append(cui)
    return clusters

def pick_representative_per_embedding_cluster(clusters: Dict[int, List[str]], embeddings: Dict[str, np.ndarray]) -> List[str]:
    reduced = []
    for cluster_cuis in clusters.values():
        if len(cluster_cuis) == 1:
            reduced.append(cluster_cuis[0])
            continue
        X = np.stack([embeddings[c] for c in cluster_cuis])
        centroid = np.mean(X, axis=0)
        distances = np.linalg.norm(X - centroid, axis=1)
        idx = np.argmin(distances)
        reduced.append(cluster_cuis[idx])
    return reduced

# ═══════════════════════════════════════════════════════════════════
# MODULE 2: ADAPTIVE TOPIC DISCOVERY
# ═══════════════════════════════════════════════════════════════════

@dataclass
class Topic:
    """Discovered topic"""
    topic_id: int
    topic_name: str
    representative_cuis: List[str]
    cui_definitions: Dict[str, str]
    cui_count: int
    source_texts: List[str]

@dataclass
class TopicResult:
    """Complete result"""
    topics: List[Topic]
    cui_to_topic: Dict[str, int]
    n_topics: int
    silhouette_score: float
    processing_time: float

def normalize(v: np.ndarray) -> np.ndarray:
    """Normalize vector"""
    return v / (np.linalg.norm(v) + 1e-10)

def fetch_cui_data(cuis: List[str], project_id: str, dataset_id: str, embedding_table: str) -> Dict[str, Dict]:
    """Fetch CUI embeddings and definitions"""
    client = bigquery.Client(project=project_id)
    
    query = f"""
    SELECT CUI, Embedding, Definition
    FROM `{project_id}.{dataset_id}.{embedding_table}`
    WHERE CUI IN UNNEST(@cuis)
    """
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ArrayQueryParameter("cuis", "STRING", list(set(cuis)))
        ]
    )
    
    results = client.query(query, job_config=job_config).result(timeout=200)
    
    cui_data = {}
    for row in results:
        cui_data[row.CUI] = {
            'embedding': np.array(row.Embedding, dtype=np.float32),
            'definition': row.Definition
        }
    
    return cui_data

def find_optimal_clusters(X: np.ndarray, max_clusters: int = None) -> int:
    """Automatically find optimal number of clusters"""
    n_samples = len(X)
    
    if max_clusters is None:
        max_clusters = min(n_samples - 1, max(2, int(np.sqrt(n_samples))))
    
    if n_samples < 2:
        return 1
    
    if n_samples == 2:
        return 2
    
    best_k = 2
    best_score = -1
    
    print(f"Finding optimal number of topics (testing 2 to {max_clusters})...")
    
    for k in range(2, max_clusters + 1):
        clustering = AgglomerativeClustering(
            n_clusters=k,
            metric='cosine',
            linkage='average'
        )
        labels = clustering.fit_predict(X)
        
        score = silhouette_score(X, labels, metric='cosine')
        
        print(f"  k={k}: silhouette={score:.3f}")
        
        if score > best_score:
            best_score = score
            best_k = k
    
    print(f"Optimal number of topics: {best_k} (silhouette={best_score:.3f})")
    return best_k

def discover_topics_adaptive(df_module1: pd.DataFrame, project_id: str, dataset_id: str, 
                            embedding_table: str, max_topics: int = None) -> TopicResult:
    """Adaptively discover topics from Module 1 output"""
    
    start_time = time.time()
    
    print(f"\n{'='*70}")
    print("MODULE 2: ADAPTIVE TOPIC DISCOVERY")
    print(f"{'='*70}\n")
    
    # Collect all CUIs
    print(f"Processing {len(df_module1)} texts from Module 1...")
    
    all_cuis = []
    text_to_cuis = {}
    
    for idx, row in df_module1.iterrows():
        text = row["Text"]
        final_cuis = row["Final CUIs"]
        
        if final_cuis and isinstance(final_cuis, list):
            all_cuis.extend(final_cuis)
            text_to_cuis[text] = final_cuis
    
    unique_cuis = list(set(all_cuis))
    print(f"Found {len(unique_cuis)} unique CUIs across all texts")
    
    if len(unique_cuis) == 0:
        print("No CUIs found")
        return TopicResult([], {}, 0, 0.0, time.time() - start_time)
    
    if len(unique_cuis) == 1:
        print("Only 1 unique CUI - creating single topic")
        cui_data = fetch_cui_data(unique_cuis, project_id, dataset_id, embedding_table)
        cui = unique_cuis[0]
        source_texts = [text for text, text_cuis in text_to_cuis.items() if cui in text_cuis]
        
        topic = Topic(
            topic_id=0,
            topic_name=cui_data.get(cui, {}).get('definition', cui)[:60],
            representative_cuis=[cui],
            cui_definitions={cui: cui_data.get(cui, {}).get('definition', '')},
            cui_count=1,
            source_texts=source_texts
        )
        
        return TopicResult([topic], {cui: 0}, 1, 0.0, time.time() - start_time)
    
    # Fetch embeddings
    print("Fetching CUI embeddings and definitions from BigQuery...")
    cui_data = fetch_cui_data(unique_cuis, project_id, dataset_id, embedding_table)
    
    valid_cuis = [cui for cui in unique_cuis if cui in cui_data]
    print(f"Got embeddings for {len(valid_cuis)} CUIs")
    
    if len(valid_cuis) < 2:
        print("Not enough CUIs with embeddings")
        return TopicResult([], {}, 0, 0.0, time.time() - start_time)
    
    # Prepare embeddings
    X = np.vstack([normalize(cui_data[cui]['embedding']) for cui in valid_cuis])
    
    # Find optimal number of topics
    n_topics = find_optimal_clusters(X, max_clusters=max_topics)
    
    # Cluster
    print(f"\nClustering {len(valid_cuis)} CUIs into {n_topics} topics...")
    
    clustering = AgglomerativeClustering(n_clusters=n_topics, metric='cosine', linkage='average')
    labels = clustering.fit_predict(X)
    
    final_silhouette = float(silhouette_score(X, labels, metric='cosine')) if n_topics > 1 else 0.0
    
    # Build topics
    print("Building topics...")
    
    topics = []
    cui_to_topic = {}
    
    for topic_id in range(n_topics):
        cluster_mask = labels == topic_id
        topic_cuis = [valid_cuis[i] for i in range(len(valid_cuis)) if cluster_mask[i]]
        
        if not topic_cuis:
            continue
        
        source_texts = []
        for text, text_cuis in text_to_cuis.items():
            if any(cui in topic_cuis for cui in text_cuis):
                source_texts.append(text)
        
        cui_definitions = {cui: cui_data[cui]['definition'] for cui in topic_cuis}
        
        topic_name = cui_data[topic_cuis[0]]['definition'][:60]
        if len(cui_data[topic_cuis[0]]['definition']) > 60:
            topic_name += "..."
        
        for cui in topic_cuis:
            cui_to_topic[cui] = topic_id
        
        topic = Topic(
            topic_id=topic_id,
            topic_name=topic_name,
            representative_cuis=topic_cuis,
            cui_definitions=cui_definitions,
            cui_count=len(topic_cuis),
            source_texts=source_texts
        )
        
        topics.append(topic)
    
    elapsed = time.time() - start_time
    
    result = TopicResult(topics, cui_to_topic, len(topics), final_silhouette, elapsed)
    
    # Print summary
    print(f"\n{'='*70}")
    print("DISCOVERED TOPICS")
    print(f"{'='*70}")
    print(f"Topics discovered: {result.n_topics}")
    print(f"Silhouette score: {result.silhouette_score:.3f}")
    print(f"Processing time: {result.processing_time:.2f}s")
    print(f"{'='*70}\n")
    
    for topic in result.topics:
        print(f"Topic {topic.topic_id}: {topic.topic_name}")
        print(f"  CUIs: {topic.cui_count}")
        print(f"  Representative CUIs: {', '.join(topic.representative_cuis)}")
        print(f"  Source texts: {', '.join(topic.source_texts)}")
        print()
    
    return result

# ═══════════════════════════════════════════════════════════════════
# MAIN EXECUTION
# ═══════════════════════════════════════════════════════════════════

if __name__ == "__main__":
    
    # ------------------------- CONFIGURATION -------------------------
    PROJECT_ID = project_id
    DATASET_ID = dataset
    API_URL = url
    EMBEDDING_TABLE = embedding_table
    NETWORK_PKL_PATH = "/home/jupyter/CUIreduction/networkx_cui_context_v1_1_0.pkl"
    
    # Your texts to process
    texts = [
        "ankle pain",
        "bp 120/90"
    ]
    
    # ------------------------- LOAD NETWORK -------------------------
    print("Loading UMLS network...")
    with open(NETWORK_PKL_PATH, "rb") as f:
        UMLS_NETWORK_OBJ = pickle.load(f)
    print(f"Network loaded: {UMLS_NETWORK_OBJ.number_of_nodes()} nodes")
    
    # ------------------------- MODULE 1: CUI REDUCTION -------------------------
    print(f"\n{'='*70}")
    print("MODULE 1: CUI EXTRACTION & REDUCTION")
    print(f"{'='*70}\n")
    
    hierarchy = HierarchyClient(UMLS_NETWORK_OBJ)
    engine = ReductionEngineAdaptive(hierarchy)
    extractor = CUIExtractor(API_URL)
    
    all_rows = []
    
    for text in texts:
        print(f"Processing: {text}")
        
        # Extract
        extracted = extractor.extract_for_text(text)
        
        # Filter by SAB
        filtered = filter_by_sab(extracted, PROJECT_ID, DATASET_ID)
        
        # Hierarchy reduction
        reduced, num_clusters = engine.reduce_cuis(filtered)
        
        # Embedding-based reduction
        if reduced:
            embeddings = fetch_cui_embeddings(reduced, PROJECT_ID, DATASET_ID, EMBEDDING_TABLE)
            clusters = cluster_cuis_by_embedding(reduced, embeddings)
            final_reduced = pick_representative_per_embedding_cluster(clusters, embeddings)
        else:
            final_reduced = []
        
        all_rows.append({
            "Text": text,
            "Extracted CUIs": extracted,
            "Filtered CUIs": filtered,
            "Hierarchy Reduced CUIs": reduced,
            "Final CUIs": final_reduced,
            "Hierarchy Clusters": num_clusters,
            "Embedding Clusters": len(clusters) if reduced else 0
        })
    
    df_module1 = pd.DataFrame(all_rows)
    
    print(f"\n{'='*70}")
    print("MODULE 1 SUMMARY")
    print(f"{'='*70}")
    print(df_module1[["Text", "Final CUIs"]])
    
    # ------------------------- MODULE 2: TOPIC DISCOVERY -------------------------
    
    topic_result = discover_topics_adaptive(
        df_module1=df_module1,
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        embedding_table=EMBEDDING_TABLE,
        max_topics=None  # Auto-detect
    )
    
    # ------------------------- FINAL OUTPUT -------------------------
    print(f"\n{'='*70}")
    print("COMPLETE PIPELINE RESULTS")
    print(f"{'='*70}\n")
    
    print(f"Processed {len(texts)} texts")
    print(f"Discovered {topic_result.n_topics} topics automatically\n")
    
    for topic in topic_result.topics:
        print(f"Topic {topic.topic_id}: {topic.topic_name}")
        print(f"  CUIs: {topic.representative_cuis}")
        print(f"  Source texts: {topic.source_texts}\n")
