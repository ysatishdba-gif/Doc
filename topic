"""
CUI Reduction System

Reduces extracted CUIs through filtering, hierarchy pruning,
embedding-based redundancy removal, and retention scoring.
Groups results into topics sorted by narrative alignment.

pip install google-cloud-bigquery networkx numpy scipy requests scann google-genai
"""

# %% [1] Imports
import time
import logging
import threading
import subprocess
import pickle
import functools
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque, OrderedDict
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import networkx as nx
import requests
import scann
from google.cloud import bigquery
from google import genai
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform


# --- Configurable thresholds (override for domain-specific tuning) ---
DEFAULT_SIBLING_IC = 8.0          # fallback when < 10 parents for Otsu
RARE_CONCEPT_IC_FLOOR = 10.0      # min IC to qualify for rare-concept protection
RARE_CONCEPT_MAX_NEIGHBORS = 1    # max close neighbors to be "isolated"
CROSS_TOPIC_MAX_DIST = 0.05       # cosine distance ceiling for cross-topic dedup
SUBGRAPH_MAX_NODES = 200_000      # safety cap for subgraph construction
WALK_MAX_VISITED = 10_000         # safety cap per-CUI graph walk
PARENT_IC_CACHE_SIZE = 100_000    # bounded cache for on-the-fly IC computation
BQ_BATCH_SIZE = 5000              # BigQuery IN clause batch size


# %% [2] Logging and Timing
logger = logging.getLogger("cui_reduction")
logger.setLevel(logging.INFO)
if not logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s", datefmt="%H:%M:%S"))
    logger.addHandler(_handler)


def log(msg, level="INFO"):
    getattr(logger, level.lower(), logger.info)(msg)


_timings = defaultdict(list)
_timing_lock = threading.Lock()
_MAX_TIMING_ENTRIES = 1000  # per function, prevents memory leak


def timed(name):
    def dec(fn):
        @functools.wraps(fn)
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                elapsed = (time.perf_counter() - t0) * 1000
                with _timing_lock:
                    entries = _timings[name]
                    if len(entries) >= _MAX_TIMING_ENTRIES:
                        entries.pop(0)
                    entries.append(elapsed)
        return wrapper
    return dec


# %% [3] Data Models
@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]
    semantic_type_ids: List[str]
    ic_score: float
    source_vocabs: List[str]


@dataclass
class TopicInfo:
    topic_id: str
    semantic_types: Set[str]
    cuis: List[str]
    cui_terms: Dict[str, str]


@dataclass
class AuditEntry:
    removed_cui: str
    removed_term: str
    kept_cui: str
    kept_term: str
    reason: str
    detail: str


@dataclass
class RetentionEntry:
    cui: str
    term: str
    alignment: float
    semantic_types: List[str]
    explains_cuis: List[str]


@dataclass
class ReductionResult:
    context_string: str
    input_count: int
    after_filter_count: int
    after_hierarchy_count: int
    after_redundancy_count: int
    topics: Dict[str, TopicInfo]
    all_reduced_cuis: List[str]
    retention: List[RetentionEntry]
    audit_trail: List[AuditEntry]
    processing_time_ms: float
    metadata_coverage: float
    embedding_coverage: float
    # quality metrics
    silhouette_score: float = 0.0        # cluster quality (-1 to 1, higher = better)
    confidence_score: float = 0.0        # reduction confidence (0 to 1)
    semantic_loss: float = 0.0           # estimated info loss (0 to 1, lower = better)


# %% [4] Graph Safety Checks


_parent_ic_cache = {}
_parent_ic_cache_lock = threading.Lock()


def _get_parent_ic(graph, parent, ic_map):
    """Get IC for a parent CUI. Checks ic_map → cache → compute."""
    pic = ic_map.get(parent)
    if pic is not None:
        return pic
    cached = _parent_ic_cache.get(parent)
    if cached is not None:
        return cached
    desc = _count_desc(graph, parent)
    total = max(graph.number_of_nodes(), 1)
    pic = -np.log((desc + 1) / total)
    with _parent_ic_cache_lock:
        if len(_parent_ic_cache) >= PARENT_IC_CACHE_SIZE:
            # evict oldest 10% to avoid repeated eviction
            evict_n = PARENT_IC_CACHE_SIZE // 10
            keys = list(_parent_ic_cache.keys())[:evict_n]
            for k in keys:
                del _parent_ic_cache[k]
        _parent_ic_cache[parent] = pic
    return pic


def _count_desc(graph, cui, max_count=5000):
    """Count descendants via BFS, capped for performance.
    Broad parents hit cap fast → low IC. Specific parents finish → exact IC."""
    if not graph.has_node(cui):
        return 0
    visited = set()
    frontier = deque([cui])
    while frontier and len(visited) < max_count:
        n = frontier.popleft()
        for child in graph.successors(n):
            if child not in visited:
                visited.add(child)
                frontier.append(child)
    return len(visited)


def _otsu(values):
    """Otsu's method — threshold maximizing between-class variance."""
    lo, hi = float(values.min()), float(values.max())
    if hi - lo < 1e-9:
        return lo

    n_bins = 256
    bins = np.linspace(lo, hi, n_bins + 1)
    hist, _ = np.histogram(values, bins=bins)
    hist = hist.astype(np.float64)
    total = hist.sum()
    if total == 0:
        return lo

    centers = (bins[:-1] + bins[1:]) / 2.0
    best_t, best_var = lo, -1.0
    sum_total = np.sum(hist * centers)
    sum_bg, weight_bg = 0.0, 0.0

    for i in range(n_bins):
        weight_bg += hist[i]
        if weight_bg == 0:
            continue
        weight_fg = total - weight_bg
        if weight_fg == 0:
            break
        sum_bg += hist[i] * centers[i]
        mean_bg = sum_bg / weight_bg
        mean_fg = (sum_total - sum_bg) / weight_fg
        var = weight_bg * weight_fg * (mean_bg - mean_fg) ** 2
        if var > best_var:
            best_var = var
            best_t = centers[i]

    return float(best_t)


# --- Precomputed batch graph info ---

class BatchGraphInfo:
    """Precompute parents and IC-bounded ancestors for a CUI batch.
    Walks up from each CUI collecting ancestors while IC >= threshold."""

    def __init__(self, graph, cuis, ic_map, sibling_threshold=DEFAULT_SIBLING_IC):
        self.graph = graph
        self.ic_map = ic_map
        self.parents = {}
        self.ancestors = {}
        self._distant_cache = {}

        for cui in cuis:
            if graph is not None and graph.has_node(cui):
                p = set(graph.predecessors(cui))
                self.parents[cui] = p
                anc = set()
                frontier = set(p)
                while frontier:
                    next_frontier = set()
                    for node in frontier:
                        pic = _get_parent_ic(graph, node, ic_map)
                        if pic >= sibling_threshold:
                            anc.add(node)
                            if graph.has_node(node):
                                for gp in graph.predecessors(node):
                                    if gp not in anc:
                                        next_frontier.add(gp)
                    frontier = next_frontier
                    if len(anc) > WALK_MAX_VISITED:  # safety cap
                        break
                self.ancestors[cui] = anc
            else:
                self.parents[cui] = set()
                self.ancestors[cui] = set()

        log(f"    BatchGraphInfo: {len(self.ancestors)} CUIs, "
            f"IC-bounded ancestors (threshold={sibling_threshold:.2f})")

    def are_siblings(self, cui_a, cui_b, sibling_threshold=DEFAULT_SIBLING_IC):
        """Check if two CUIs share a specific parent (IC >= threshold)."""
        pa = self.parents.get(cui_a, set())
        pb = self.parents.get(cui_b, set())
        shared = pa & pb
        if not shared:
            return False
        for p in shared:
            pic = _get_parent_ic(self.graph, p, self.ic_map)
            if pic >= sibling_threshold:
                return True
        return False

    def graph_distant(self, cui_a, cui_b):
        """Cached: check if two CUIs share no IC-bounded ancestors.
        Returns False for CUIs with empty ancestor sets (unknown → don't protect)."""
        key = (cui_a, cui_b) if cui_a < cui_b else (cui_b, cui_a)
        if key in self._distant_cache:
            return self._distant_cache[key]

        anc_a = self.ancestors.get(cui_a, set())
        anc_b = self.ancestors.get(cui_b, set())

        if cui_a in anc_b or cui_b in anc_a:
            self._distant_cache[key] = False
            return False

        distant = (len(anc_a & anc_b) == 0) and bool(anc_a) and bool(anc_b)
        self._distant_cache[key] = distant
        return distant

    def ontology_penalty(self, cui_a, cui_b, sibling_threshold=DEFAULT_SIBLING_IC):
        """Continuous penalty [0,1]: 0=related, 0.3=broad sibs, 0.6=specific sibs, 1=distant."""
        anc_a = self.ancestors.get(cui_a, set())
        anc_b = self.ancestors.get(cui_b, set())

        if cui_a in anc_b or cui_b in anc_a:
            return 0.0

        if (len(anc_a & anc_b) == 0) and anc_a and anc_b:
            return 1.0

        shared_parents = self.parents.get(cui_a, set()) & self.parents.get(cui_b, set())
        if shared_parents:
            max_pic = max(_get_parent_ic(self.graph, p, self.ic_map)
                         for p in shared_parents)
            if max_pic >= sibling_threshold:
                return 0.6
            else:
                return 0.3

        return 0.2

    def ontology_branch(self, cui, max_depth=3):
        """Get highest ancestor within max_depth hops (branch preclustering)."""
        if self.graph is None or not self.graph.has_node(cui):
            return cui
        current = cui
        for _ in range(max_depth):
            pars = list(self.graph.predecessors(current))
            if not pars:
                break
            current = max(pars, key=lambda p: self.graph.out_degree(p))
        return current


@timed("compute_sibling_threshold")
def compute_sibling_threshold(graph, cuis, ic_map):
    """Data-driven threshold for sibling protection using Otsu's method."""
    parent_ics = []
    seen_parents = set()
    for cui in cuis:
        if not graph.has_node(cui):
            continue
        for p in graph.predecessors(cui):
            if p not in seen_parents:
                seen_parents.add(p)
                pic = _get_parent_ic(graph, p, ic_map)
                parent_ics.append(pic)

    if len(parent_ics) < 10:
        return DEFAULT_SIBLING_IC

    arr = np.array(parent_ics)
    threshold = _otsu(arr)

    log(f"    Sibling IC threshold (Otsu): {threshold:.2f} "
        f"(from {len(parent_ics)} parents, "
        f"range {arr.min():.2f}-{arr.max():.2f}, "
        f"median {np.median(arr):.2f})")

    return threshold


@timed("build_subgraph")
def build_subgraph(full_graph, cuis, ic_map, ic_floor):
    """Build a small subgraph: input CUIs + IC-bounded ancestors.
    Walks up from each CUI, collecting ancestors while IC >= ic_floor.
    Same IC logic as the downstream walks — no arbitrary depth cap."""
    relevant = set()
    for cui in cuis:
        if full_graph.has_node(cui):
            relevant.add(cui)

    frontier = set(relevant)
    while frontier:
        next_frontier = set()
        for node in frontier:
            for parent in full_graph.predecessors(node):
                if parent not in relevant:
                    pic = _get_parent_ic(full_graph, parent, ic_map)
                    if pic >= ic_floor:
                        relevant.add(parent)
                        next_frontier.add(parent)
        frontier = next_frontier
        if len(relevant) > SUBGRAPH_MAX_NODES:  # safety cap
            log(f"    Subgraph safety cap hit at {len(relevant)} nodes", "WARNING")
            break

    sub = full_graph.subgraph(relevant)
    log(f"    Subgraph: {sub.number_of_nodes()} nodes, "
        f"{sub.number_of_edges()} edges "
        f"(from {full_graph.number_of_nodes()} full)")
    return sub


# %% [5] LRU Cache (thread-safe)
class Cache:
    def __init__(self, max_size):
        self._max = max_size
        self._d = OrderedDict()
        self._lock = threading.Lock()

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
        return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# %% [6] Metadata from BigQuery


class MetadataFetcher:
    """Pulls CUI metadata from MRCONSO/MRSTY.
    Picks preferred term: ISPREF=Y first, then longest STR."""

    def __init__(self, bq, pid, did):
        self.bq = bq
        self.pid = pid
        self.did = did
        self._cache = Cache(50_000)

    @timed("metadata_fetch")
    def fetch(self, cuis):
        result = {}
        missing = []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), BQ_BATCH_SIZE):
            batch = missing[i:i + BQ_BATCH_SIZE]
            query = f"""
            WITH ranked AS (
              SELECT CUI, STR, SAB, ISPREF,
                ROW_NUMBER() OVER (
                  PARTITION BY CUI
                  ORDER BY
                    CASE SAB
                      WHEN 'SNOMEDCT_US' THEN 0
                      WHEN 'ICD10CM' THEN 1
                      WHEN 'ICD10PCS' THEN 2
                      WHEN 'ICD9CM' THEN 3
                      WHEN 'LNC' THEN 4
                      ELSE 5
                    END,
                    CASE WHEN ISPREF='Y' THEN 0 ELSE 1 END,
                    LENGTH(STR) DESC
                ) AS rn
              FROM `{self.pid}.{self.did}.MRCONSO`
              WHERE CUI IN UNNEST(@cuis) AND LAT = 'ENG'
            )
            SELECT
              r.CUI AS cui,
              MAX(CASE WHEN r.rn = 1 THEN r.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT s.STY IGNORE NULLS) AS stys,
              ARRAY_AGG(DISTINCT c2.SAB IGNORE NULLS) AS sabs
            FROM ranked r
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s
              ON r.CUI = s.CUI
            LEFT JOIN `{self.pid}.{self.did}.MRCONSO` c2
              ON r.CUI = c2.CUI AND c2.LAT = 'ENG'
            WHERE r.rn = 1
            GROUP BY r.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            for row in self.bq.query(
                    query, job_config=jc, timeout=300).result():
                term = row.pref_term or row.cui
                meta = CUIMetadata(
                    cui=row.cui,
                    preferred_term=term,
                    semantic_types=list(row.stys or []),
                    semantic_type_ids=list(row.tuis or []),
                    ic_score=0.0,
                    source_vocabs=list(row.sabs or []),
                )
                self._cache.put(row.cui, meta)
                result[row.cui] = meta

        return result


# %% [7] IC Scores
class ICComputer:
    """IC = -log((descendants + 1) / total).
    Use precomputed dict in production to avoid BFS."""

    def __init__(self, graph, precomputed=None):
        self.graph = graph
        self.precomputed = precomputed or {}
        self._cache = Cache(100_000)
        self._total = max(graph.number_of_nodes(), 1)

    @timed("ic_compute")
    def compute(self, cuis, metadata):
        result = {}
        for cui in cuis:
            if cui in self.precomputed:
                ic = self.precomputed[cui]
            else:
                cached = self._cache.get(cui)
                if cached is not None:
                    ic = cached
                else:
                    desc = self._count_descendants(cui)
                    ic = -np.log((desc + 1) / self._total)
                    self._cache.put(cui, ic)
            result[cui] = ic
            m = metadata.get(cui)
            if m:
                m.ic_score = ic
        return result

    def _count_descendants(self, cui):
        if not self.graph.has_node(cui):
            return 0
        visited = set()
        q = deque([cui])
        while q:
            n = q.popleft()
            for child in self.graph.successors(n):
                if child not in visited:
                    visited.add(child)
                    q.append(child)
        return len(visited)


# %% [8] Filters
class SABFilter:
    def __init__(self, allowed):
        self.allowed = set(allowed)

    @timed("sab_filter")
    def run(self, cuis, metadata):
        kept, audit = [], []
        for cui in cuis:
            m = metadata.get(cui)
            if not m:
                audit.append(AuditEntry(
                    cui, "", "", "", "no_metadata",
                    "CUI not found in BQ"))
                continue
            if not any(s in self.allowed for s in m.source_vocabs):
                audit.append(AuditEntry(
                    cui, m.preferred_term, "", "",
                    "sab_filtered",
                    f"vocabs {m.source_vocabs} not in allowed"))
                continue
            kept.append(cui)
        return kept, audit


# %% [9] Hierarchy Reduction
class HierarchyReducer:
    """Remove ancestor CUIs when a more specific descendant exists.
    Uses IC-bounded ancestor walk to catch multi-hop subsumption."""

    def __init__(self, graph):
        self.graph = graph

    @timed("hierarchy_reduce")
    def run(self, cuis, ic_map, metadata, sibling_threshold=None, graph=None):
        g = graph if graph is not None else self.graph
        cui_set = set(cuis)
        edges = set()

        if sibling_threshold is not None:
            ic_floor = sibling_threshold
        else:
            ic_vals = [ic_map.get(c, 0) for c in cuis if ic_map.get(c, 0) > 0]
            ic_floor = float(np.median(ic_vals)) / 2 if ic_vals else 4.0

        to_remove = {}

        for cui in cuis:
            if not g.has_node(cui):
                continue

            c_ic = ic_map.get(cui, 0)
            c_meta = metadata.get(cui)
            c_types = set(c_meta.semantic_types) if c_meta and c_meta.semantic_types else set()

            visited = set()
            frontier = set(g.predecessors(cui))
            while frontier:
                next_frontier = set()
                for anc in frontier:
                    if anc in visited or anc == cui:
                        continue
                    visited.add(anc)

                    pic = _get_parent_ic(g, anc, ic_map)

                    if anc in cui_set:
                        a_ic = ic_map.get(anc, 0)
                        if c_ic >= a_ic:
                            a_meta = metadata.get(anc)
                            a_types = set(a_meta.semantic_types) if a_meta and a_meta.semantic_types else set()
                            if not a_types or not c_types or (a_types & c_types):
                                edges.add((anc, cui))
                                if anc not in to_remove or c_ic > to_remove[anc][1]:
                                    to_remove[anc] = (cui, c_ic, a_ic)

                    if pic >= ic_floor and g.has_node(anc):
                        for gp in g.predecessors(anc):
                            if gp not in visited:
                                next_frontier.add(gp)

                frontier = next_frontier
                if len(visited) > WALK_MAX_VISITED:
                    break

        remove_set = set(to_remove.keys())
        surviving = [c for c in cuis if c not in remove_set]

        audit = []
        for parent, (child, c_ic, p_ic) in to_remove.items():
            pm = metadata.get(parent)
            cm = metadata.get(child)
            audit.append(AuditEntry(
                parent, pm.preferred_term if pm else "",
                child, cm.preferred_term if cm else "",
                "ancestor_subsumed",
                f"child IC {c_ic:.2f} >= ancestor IC {p_ic:.2f} (multi-hop)"))

        return surviving, audit, edges


# %% [10] Embedding Fetch (parallel BQ batches + disk cache)
class EmbeddingFetcher:
    def __init__(self, bq, pid, did, embedding_table, cache_path=None):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.table = embedding_table
        self._cache = Cache(50_000)
        self._pool = ThreadPoolExecutor(max_workers=4)
        self._cache_path = cache_path
        self._disk_cache = {}
        self._disk_dirty = False

        # load disk cache if exists
        if cache_path:
            import os
            if os.path.exists(cache_path):
                try:
                    data = np.load(cache_path, allow_pickle=False)
                    cuis_arr = data['cuis']
                    vecs_arr = data['vecs']
                    for i, cui in enumerate(cuis_arr):
                        self._disk_cache[str(cui)] = vecs_arr[i]
                    log(f"    Embedding disk cache: loaded {len(self._disk_cache)} from {cache_path}")
                except Exception as e:
                    log(f"    Embedding disk cache load failed: {e}", "WARNING")

    def shutdown(self):
        """Save dirty cache to disk, then clean up thread pool."""
        if self._cache_path and self._disk_dirty:
            try:
                cuis = list(self._disk_cache.keys())
                vecs = np.array([self._disk_cache[c] for c in cuis], dtype=np.float32)
                np.savez_compressed(self._cache_path, cuis=np.array(cuis), vecs=vecs)
                log(f"    Embedding disk cache: saved {len(cuis)} to {self._cache_path}")
            except Exception as e:
                log(f"    Embedding disk cache save failed: {e}", "WARNING")
        self._pool.shutdown(wait=False)

    def _fetch_batch(self, batch):
        """Fetch one batch from BQ. Called in parallel."""
        rows = []
        query = f"""
        SELECT cui, embedding
        FROM `{self.pid}.{self.did}.{self.table}`
        WHERE cui IN UNNEST(@cuis)
        """
        jc = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ArrayQueryParameter("cuis", "STRING", batch)
        ])
        for row in self.bq.query(
                query, job_config=jc, timeout=300).result():
            raw = row.embedding
            if raw is None or len(raw) == 0:
                continue
            vec = np.array(raw, dtype=np.float32)
            if not np.all(np.isfinite(vec)):
                continue
            norm = np.linalg.norm(vec)
            if norm == 0:
                continue
            vec = vec / norm
            rows.append((row.cui, vec))
        return rows

    @timed("embedding_fetch")
    def fetch(self, cuis):
        result, missing = {}, []
        for c in cuis:
            # check in-memory cache first
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
                continue
            # then disk cache
            if c in self._disk_cache:
                vec = self._disk_cache[c]
                self._cache.put(c, vec)
                result[c] = vec
                continue
            missing.append(c)

        if not missing:
            log(f"    Embeddings: {len(result)} from cache, 0 from BQ")
            return result

        log(f"    Embeddings: {len(result)} cached, {len(missing)} fetching from BQ")
        batches = [missing[i:i + BQ_BATCH_SIZE]
                    for i in range(0, len(missing), BQ_BATCH_SIZE)]

        futures = [self._pool.submit(self._fetch_batch, b) for b in batches]
        for f in futures:
            for cui, vec in f.result():
                self._cache.put(cui, vec)
                self._disk_cache[cui] = vec
                self._disk_dirty = True
                result[cui] = vec

        return result


# %% [11] ScaNN Topic Builder
class TopicBuilder:
    """Clusters CUIs into topics via ScaNN KNN graph + connected components."""

    @staticmethod
    @timed("scann_topics")
    def build(cuis, embeddings, k_neighbors=20):
        # deterministic: sort input + reset numpy RNG
        cuis = sorted(cuis)
        np.random.seed(42)

        matrix = np.array([embeddings[c] for c in cuis], dtype=np.float32)
        n = len(cuis)
        dim = matrix.shape[1] if matrix.ndim == 2 else 0

        if n < 2 or dim == 0:
            return {i: [c] for i, c in enumerate(cuis)}

        k = min(k_neighbors, n - 1)
        k = max(k, 1)

        num_leaves = min(max(2, int(np.sqrt(n))), n)
        builder = scann.scann_ops_pybind.builder(matrix, k, "dot_product")

        if n >= 16 and dim >= 2:
            dims_per_block = 2 if dim % 2 == 0 else 1
            builder = (
                builder
                .tree(
                    num_leaves=num_leaves,
                    num_leaves_to_search=max(1, num_leaves // 5),
                    training_sample_size=min(n, 250_000))
                .score_ah(dims_per_block,
                          anisotropic_quantization_threshold=0.2)
                .reorder(min(k * 4, n))
            )
        else:
            builder = builder.score_brute_force()

        searcher = builder.build()
        neighbors, scores = searcher.search_batched(matrix)

        kth_dists = []
        for i in range(n):
            valid = scores[i][scores[i] > -1e30]
            if len(valid) > 0:
                kth_dists.append(1.0 - float(valid[-1]))

        if kth_dists:
            threshold = float(np.mean(kth_dists))
        else:
            all_sims = matrix @ matrix.T
            np.fill_diagonal(all_sims, -np.inf)
            threshold = float(1.0 - np.mean(all_sims[all_sims > -np.inf]))

        adj = defaultdict(set)
        for i in range(n):
            for j_pos in range(len(neighbors[i])):
                j = int(neighbors[i][j_pos])
                if j < 0 or j >= n or j == i:
                    continue
                if (1.0 - float(scores[i][j_pos])) < threshold:
                    adj[i].add(j)
                    adj[j].add(i)

        visited = set()
        groups = {}
        label = 0
        for i in range(n):
            if i in visited:
                continue
            component = []
            q = deque([i])
            visited.add(i)
            while q:
                node = q.popleft()
                component.append(cuis[node])
                for nb in adj.get(node, set()):
                    if nb not in visited:
                        visited.add(nb)
                        q.append(nb)
            groups[label] = component
            label += 1

        return groups


# %% [12] Embedding Reduction
class EmbeddingReducer:
    """Branch+type grouping → ScaNN topics → coherence split → pairwise reduction."""

    @staticmethod
    @timed("embedding_reduce")
    def run(cuis, metadata, embeddings, ic_map,
            graph=None, sibling_threshold=DEFAULT_SIBLING_IC, batch_info=None):

        # validate embedding dimensions
        if embeddings:
            dims = {v.shape[0] for v in embeddings.values() if hasattr(v, 'shape')}
            if len(dims) > 1:
                target_dim = max(dims, key=lambda d: sum(
                    1 for v in embeddings.values() if hasattr(v, 'shape') and v.shape[0] == d))
                log(f"    Mixed embedding dims {dims}, keeping dim={target_dim}", "WARNING")
                embeddings = {k: v for k, v in embeddings.items()
                             if hasattr(v, 'shape') and v.shape[0] == target_dim}

        with_emb = [c for c in cuis if c in embeddings]
        without_emb = [c for c in cuis if c not in embeddings]

        # CUIs without embeddings survive automatically
        globally_surviving = set(without_emb)

        if len(with_emb) <= 1:
            globally_surviving.update(with_emb)
            topic_id = "topic_0"
            return {topic_id: list(globally_surviving)}, globally_surviving, [], 0.0

        # cluster ALL CUIs with embeddings via ScaNN
        scann_topics = TopicBuilder.build(with_emb, embeddings)

        # validate topic coherence — split mixed topics
        validated_topics = _validate_and_split_topics(scann_topics, metadata)

        all_topics = {}
        removal_candidates = {}
        sil_scores = []
        tid = 0

        for t_cuis in validated_topics.values():
            topic_id = f"topic_{tid}"
            tid += 1

            if len(t_cuis) <= 1:
                all_topics[topic_id] = t_cuis
                for c in t_cuis:
                    globally_surviving.add(c)
                continue

            surviving, removed, sil = _reduce_topic(
                t_cuis, embeddings, metadata, ic_map,
                sibling_threshold, batch_info)
            if sil != 0.0:
                sil_scores.append(sil)
            all_topics[topic_id] = surviving
            for c in surviving:
                globally_surviving.add(c)
            for entry in removed:
                removal_candidates[entry.removed_cui] = entry

        # CUIs without embeddings get individual topics
        for cui in without_emb:
            all_topics[f"topic_{tid}"] = [cui]
            tid += 1

        final_audit = []
        for cui, entry in removal_candidates.items():
            if cui not in globally_surviving:
                final_audit.append(entry)

        seen = set()
        surviving = []
        for t_cuis in all_topics.values():
            for c in t_cuis:
                if c in globally_surviving and c not in seen:
                    seen.add(c)
                    surviving.append(c)

        cleaned = {}
        for topic_id, t_cuis in all_topics.items():
            members = []
            seen_t = set()
            for c in t_cuis:
                if c in globally_surviving and c not in seen_t:
                    seen_t.add(c)
                    members.append(c)
            if members:
                cleaned[topic_id] = members

        avg_sil = float(np.mean(sil_scores)) if sil_scores else 0.0
        return cleaned, surviving, final_audit, avg_sil


def _validate_and_split_topics(scann_topics, metadata):
    """Improvement #6: Topic coherence validation.

    If a topic has high semantic type entropy (many unrelated types mixed),
    split it by dominant semantic type. Prevents incoherent topics like
    mixing "knee pain" with "diabetes medication".
    """
    validated = {}
    sub_id = 0

    for orig_id, t_cuis in scann_topics.items():
        if len(t_cuis) <= 3:
            validated[sub_id] = t_cuis
            sub_id += 1
            continue

        # count semantic types
        type_counts = defaultdict(int)
        cui_types = {}
        for c in t_cuis:
            m = metadata.get(c)
            stys = tuple(sorted(m.semantic_types)) if m and m.semantic_types else ("Unknown",)
            cui_types[c] = stys
            for s in stys:
                type_counts[s] += 1

        n_types = len(type_counts)
        n_cuis = len(t_cuis)

        # entropy check: if >4 types and no single type covers >40%, split
        max_type_frac = max(type_counts.values()) / n_cuis if n_cuis else 1
        if n_types > 4 and max_type_frac < 0.4:
            # split by dominant semantic type per CUI
            type_groups = defaultdict(list)
            for c in t_cuis:
                primary = cui_types[c][0] if cui_types[c] else "Unknown"
                type_groups[primary].append(c)
            for sub_cuis in type_groups.values():
                validated[sub_id] = sub_cuis
                sub_id += 1
        else:
            validated[sub_id] = t_cuis
            sub_id += 1

    return validated


def _reduce_topic(t_cuis, embeddings, metadata, ic_map,
                  sibling_threshold=DEFAULT_SIBLING_IC, batch_info=None):
    if len(t_cuis) > 2000:
        log(f"    Topic too large ({len(t_cuis)} CUIs), skipping pairwise reduction", "WARNING")
        return t_cuis, [], 0.0

    matrix = np.array([embeddings[c] for c in t_cuis])
    sim = matrix @ matrix.T
    np.clip(sim, -1, 1, out=sim)
    dist = 1.0 - sim
    np.fill_diagonal(dist, 0)
    dist = np.maximum(dist, 0)

    if np.any(~np.isfinite(dist)):
        dist = np.nan_to_num(dist, nan=1.0, posinf=1.0, neginf=0.0)

    n = len(t_cuis)
    idx = {c: i for i, c in enumerate(t_cuis)}
    upper = dist[np.triu_indices(n, k=1)]

    if len(upper) == 0 or np.std(upper) < 1e-9:
        return t_cuis, [], 0.0

    guardrail = float(np.median(upper))

    tight_t = float(_otsu(upper))
    nonzero = upper[upper > 0]
    floor = float(np.min(nonzero)) if len(nonzero) > 0 else float(np.finfo(np.float32).eps)
    tight_t = max(tight_t, floor)
    tight_t = min(tight_t, guardrail)

    # Continuous ontology penalty — scales distances by graph relationship.
    # Skip for large topics (>500) to avoid O(N²) Python loop;
    # numpy matmul is fast but the per-pair penalty check is not.
    if batch_info is not None and n > 1 and n <= 500:
        for i in range(n):
            ci = t_cuis[i]
            for j in range(i + 1, n):
                if dist[i, j] > guardrail:
                    continue
                cj = t_cuis[j]
                penalty = batch_info.ontology_penalty(ci, cj, sibling_threshold)
                if penalty > 0:
                    # blend: dist → dist + penalty * (guardrail - dist)
                    # at penalty=1.0: dist becomes guardrail
                    # at penalty=0.6: dist moves 60% toward guardrail
                    # at penalty=0.3: dist moves 30% toward guardrail
                    gap = guardrail - dist[i, j]
                    adjustment = penalty * gap
                    dist[i, j] += adjustment
                    dist[j, i] += adjustment

    condensed = squareform(dist, checks=False)
    Z = linkage(condensed, method="average")
    labels = fcluster(Z, t=tight_t, criterion="distance")

    # silhouette score
    n_clusters = len(set(labels))
    sil_score = 0.0
    if 2 <= n_clusters < n:
        sil_vals = []
        for i in range(n):
            my_label = labels[i]
            intra = [dist[i, j] for j in range(n)
                     if j != i and labels[j] == my_label]
            a_i = np.mean(intra) if intra else 0.0
            best_other = float('inf')
            for lbl in set(labels):
                if lbl == my_label:
                    continue
                other = [dist[i, j] for j in range(n) if labels[j] == lbl]
                if other:
                    best_other = min(best_other, np.mean(other))
            b_i = best_other if best_other < float('inf') else 0.0
            s_i = (b_i - a_i) / max(a_i, b_i) if max(a_i, b_i) > 0 else 0.0
            sil_vals.append(s_i)
        sil_score = float(np.mean(sil_vals))

    r_groups = defaultdict(list)
    for cui, lbl in zip(t_cuis, labels):
        r_groups[lbl].append(cui)

    all_kept, all_removed = [], []
    for r_cuis in r_groups.values():
        kept, removed = _resolve_group(
            r_cuis, metadata, ic_map, dist, idx, tight_t)
        all_kept.extend(kept)
        all_removed.extend(removed)

    return all_kept, all_removed, sil_score


def _resolve_group(r_cuis, metadata, ic_map, dist, index_map, tight_t):
    """Pick representative via IC+centrality. Remove CUIs that are close
    to ANY already-kept CUI (not just best). Safety checks protect
    rare/isolated concepts only."""
    if len(r_cuis) <= 1:
        return r_cuis, []

    # score all CUIs: IC (specificity) + centrality (cluster fit)
    ic_vals = {c: ic_map.get(c, 0) for c in r_cuis}
    cent_vals = {}
    for c in r_cuis:
        ci = index_map.get(c)
        if ci is not None and len(r_cuis) > 1:
            dists = [dist[ci, index_map[o]] for o in r_cuis
                     if o != c and index_map.get(o) is not None]
            cent_vals[c] = 1.0 - (sum(dists) / len(dists)) if dists else 0.0
        else:
            cent_vals[c] = 0.0

    def _norm(vals):
        lo = min(vals.values())
        hi = max(vals.values())
        rng = hi - lo
        if rng < 1e-9:
            return {k: 0.5 for k in vals}
        return {k: (v - lo) / rng for k, v in vals.items()}

    ic_n = _norm(ic_vals)
    cent_n = _norm(cent_vals)

    # sort by composite score, pick best first
    ranked = sorted(r_cuis, key=lambda c: ic_n[c] + cent_n[c], reverse=True)
    best = ranked[0]

    kept = [best]
    audit = []

    for c in ranked[1:]:
        c_meta = metadata.get(c)
        ci = index_map.get(c)

        # check if c is close to ANY already-kept CUI
        is_close_to_kept = False
        closest_kept = best
        closest_dist = float('inf')
        if ci is not None:
            for k in kept:
                ki = index_map.get(k)
                if ki is not None:
                    d = dist[ci, ki]
                    if d < closest_dist:
                        closest_dist = d
                        closest_kept = k
                    if d < tight_t:
                        is_close_to_kept = True

        # not close to any kept CUI → keep (genuinely different)
        if not is_close_to_kept:
            kept.append(c)
            continue

        # rare concept protection: high IC + isolated in the full topic
        c_ic = ic_map.get(c, 0)
        if c_ic > RARE_CONCEPT_IC_FLOOR and ci is not None:
            close_count = sum(1 for o in r_cuis if o != c and
                             index_map.get(o) is not None and
                             dist[ci, index_map[o]] < tight_t)
            if close_count <= RARE_CONCEPT_MAX_NEIGHBORS:
                kept.append(c)
                continue

        # close to a kept CUI and not rare → remove
        k_meta = metadata.get(closest_kept)
        audit.append(AuditEntry(
            removed_cui=c,
            removed_term=c_meta.preferred_term if c_meta else "",
            kept_cui=closest_kept,
            kept_term=k_meta.preferred_term if k_meta else "",
            reason="embedding_redundant",
            detail=f"dist {closest_dist:.3f} < tight {tight_t:.3f} | "
                   f"IC {ic_map.get(closest_kept,0):.2f} vs {ic_map.get(c,0):.2f} | "
                   f"centrality {cent_vals.get(closest_kept,0):.3f} vs {cent_vals.get(c,0):.3f}"))

    return kept, audit


# %% [13] Cross-Topic Dedup
class CrossTopicDedup:
    """Remove near-duplicate CUIs across topics. Alignment as tiebreaker."""

    @staticmethod
    @timed("cross_topic_dedup")
    def run(surviving, embeddings, text_vec, metadata, ic_map,
            sibling_threshold=DEFAULT_SIBLING_IC, batch_info=None):
        with_emb = [c for c in surviving if c in embeddings]
        without_emb = [c for c in surviving if c not in embeddings]

        if len(with_emb) <= 1:
            return surviving, []

        matrix = np.array([embeddings[c] for c in with_emb], dtype=np.float32)
        idx_map = {c: i for i, c in enumerate(with_emb)}

        # text alignment for tiebreaking
        alignments = {}
        for c in with_emb:
            alignments[c] = float(np.dot(text_vec, embeddings[c]))

        # pairwise cosine distances
        sim = matrix @ matrix.T
        np.clip(sim, -1, 1, out=sim)
        dist = 1.0 - sim
        np.fill_diagonal(dist, 999.0)

        # adaptive threshold: very tight — only true near-duplicates
        upper = dist[np.triu_indices(len(with_emb), k=1)]
        upper = upper[upper < 999.0]
        if len(upper) == 0:
            return surviving, []

        # 5th percentile of distances = tightest pairs
        threshold = float(np.percentile(upper, 5))
        # floor: don't go above 0.05 cosine distance
        threshold = min(threshold, CROSS_TOPIC_MAX_DIST)

        # find pairs below threshold, remove the one with lower alignment
        removed = set()
        audit = []

        for i in range(len(with_emb)):
            if with_emb[i] in removed:
                continue
            for j in range(i + 1, len(with_emb)):
                if with_emb[j] in removed:
                    continue
                if dist[i, j] > threshold:
                    continue

                ci, cj = with_emb[i], with_emb[j]

                # safety guards via batch_info (precomputed + cached)
                if batch_info.are_siblings(ci, cj, sibling_threshold):
                    continue
                if batch_info.graph_distant(ci, cj):
                    continue

                # keep higher alignment, break ties by IC
                if (alignments.get(ci, 0), ic_map.get(ci, 0)) >= \
                   (alignments.get(cj, 0), ic_map.get(cj, 0)):
                    keep, drop = ci, cj
                else:
                    keep, drop = cj, ci

                removed.add(drop)
                km = metadata.get(keep)
                dm = metadata.get(drop)
                audit.append(AuditEntry(
                    removed_cui=drop,
                    removed_term=dm.preferred_term if dm else "",
                    kept_cui=keep,
                    kept_term=km.preferred_term if km else "",
                    reason="cross_topic_dedup",
                    detail=f"dist {dist[idx_map[keep], idx_map[drop]]:.4f} "
                           f"< threshold {threshold:.4f} | "
                           f"align {alignments.get(keep,0):.3f} vs {alignments.get(drop,0):.3f}"))

        final = [c for c in surviving if c not in removed] 
        return final, audit


# %% [14] Coverage Rescue
@timed("coverage_rescue")
def coverage_rescue(surviving, all_input_cuis, metadata, ic_map, embeddings,
                    text_vec=None):
    """Rescue CUIs from semantic axes lost during reduction."""
    input_types = defaultdict(set)
    for c in all_input_cuis:
        m = metadata.get(c)
        if m and m.semantic_types:
            for t in m.semantic_types:
                input_types[t].add(c)

    output_types = set()
    for c in surviving:
        m = metadata.get(c)
        if m and m.semantic_types:
            output_types.update(m.semantic_types)

    lost_types = set(input_types.keys()) - output_types
    if not lost_types:
        return surviving, []

    surviving_set = set(surviving)
    rescued = []
    audit = []

    for lost_type in sorted(lost_types):
        candidates = input_types[lost_type] - surviving_set
        if not candidates:
            continue
        with_emb = [c for c in candidates if c in embeddings]
        pool = with_emb if with_emb else list(candidates)

        # prefer alignment to input text, fall back to IC
        if text_vec is not None and with_emb:
            best = max(pool, key=lambda c: float(np.dot(text_vec, embeddings[c])))
        else:
            best = max(pool, key=lambda c: ic_map.get(c, 0))

        rescued.append(best)
        surviving_set.add(best)
        m = metadata.get(best)
        audit.append(AuditEntry(
            removed_cui="", removed_term="",
            kept_cui=best,
            kept_term=m.preferred_term if m else "",
            reason="coverage_rescued",
            detail=f"rescued for lost type '{lost_type}' | IC {ic_map.get(best,0):.2f}"))

    if rescued:
        surviving = list(surviving) + rescued
        log(f"    Coverage Rescue: +{len(rescued)} CUIs for "
            f"{len(lost_types)} lost semantic types")

    return surviving, audit


# %% [15] Retention Scorer
class RetentionScorer:
    """Scores each CUI by narrative alignment via cosine(text_emb, cui_emb).
    Cost: 1 embedding API call (~100ms) per input text."""

    def __init__(self, project_id, location="us-central1"):
        self.client = genai.Client(
            vertexai=True, project=project_id, location=location)

    @timed("text_embed")
    def embed_text(self, text):
        """Embed input text. Returns normalized 3072-dim vector."""
        response = self.client.models.embed_content(
            model="gemini-embedding-001",
            contents=text,
        )
        text_vec = np.array(response.embeddings[0].values, dtype=np.float32)
        norm = np.linalg.norm(text_vec)
        if norm > 0:
            text_vec = text_vec / norm
        return text_vec

    @timed("retention_score")
    def score(self, text_vec, surviving, embeddings, metadata, audit_trail):

        alignments = {}
        for cui in surviving:
            if cui in embeddings:
                alignments[cui] = float(np.dot(text_vec, embeddings[cui]))
            else:
                alignments[cui] = 0.0

        explains = defaultdict(list)
        for entry in audit_trail:
            if entry.kept_cui and entry.removed_cui:
                explains[entry.kept_cui].append(entry.removed_cui)

        entries = []
        for cui in surviving:
            m = metadata.get(cui)
            entries.append(RetentionEntry(
                cui=cui,
                term=m.preferred_term if m else cui,
                alignment=round(alignments.get(cui, 0.0), 4),
                semantic_types=list(m.semantic_types) if m else [],
                explains_cuis=explains.get(cui, []),
            ))

        entries.sort(key=lambda e: e.alignment, reverse=True)
        sorted_cuis = [e.cui for e in entries]

        return sorted_cuis, entries


# %% [16] Pipeline
class CUIReductionSystem:

    def __init__(self, project_id, dataset_id, full_network,
                 allowed_sabs, embedding_table,
                 ic_scores=None, genai_location="us-central1",
                 embedding_cache_path="embedding_cache.npz"):

        self.bq = bigquery.Client(project=project_id)
        self.fetcher = MetadataFetcher(self.bq, project_id, dataset_id)
        self.ic_computer = ICComputer(full_network, ic_scores)
        self.sab_filter = SABFilter(allowed_sabs)
        self.hierarchy_reducer = HierarchyReducer(full_network)
        self.emb_fetcher = EmbeddingFetcher(
            self.bq, project_id, dataset_id, embedding_table,
            cache_path=embedding_cache_path)
        self.retention_scorer = RetentionScorer(project_id, genai_location)

        log(f"[Init] graph={full_network.number_of_nodes()} nodes, sabs={allowed_sabs}")

    @timed("full_pipeline")
    def reduce(self, cuis, context_string):
        """Reduce a single text. For multiple texts, use reduce_batch."""
        results = self.reduce_batch([(cuis, context_string)])
        return results[0]

    @timed("batch_pipeline")
    def reduce_batch(self, inputs):
        """Reduce multiple texts efficiently.

        Args:
            inputs: list of (cuis, context_string) tuples

        Batches expensive BQ calls (metadata, embeddings) across
        all texts, then runs per-text filtering and reduction.
        """

        t0 = time.perf_counter()

        # collect all unique CUIs across all texts
        all_cuis = set()
        for cuis, _ in inputs:
            all_cuis.update(cuis)
        log(f"[Batch] {len(inputs)} texts, {len(all_cuis)} unique CUIs")

        # ONE metadata fetch for all CUIs
        metadata = self.fetcher.fetch(list(all_cuis))
        log(f"[Batch] Metadata: {len(metadata)}/{len(all_cuis)} matched")

        # ONE IC computation for all CUIs
        ic_map = self.ic_computer.compute(list(all_cuis), metadata)
        log(f"[Batch] IC Scores: {len(ic_map)} computed")

        # per-text: cheap in-memory filtering + hierarchy
        per_text = []
        all_post_hier = set()

        for cuis, context_string in inputs:
            if not cuis:
                per_text.append((context_string, [], [], set(), [], 8.0, None))
                continue

            label = context_string[:50]
            log(f"\n  [{label}]")

            after_sab, sab_audit = self.sab_filter.run(cuis, metadata)
            log(f"    SAB Filter: {len(cuis)} -> {len(after_sab)} (removed {len(cuis) - len(after_sab)})")

            # compute sibling IC threshold (1-hop parents only, fast on full graph)
            sib_threshold = compute_sibling_threshold(
                self.hierarchy_reducer.graph, after_sab, ic_map)

            # build IC-bounded subgraph — same threshold controls the boundary
            subgraph = build_subgraph(
                self.hierarchy_reducer.graph, after_sab, ic_map, ic_floor=sib_threshold)

            after_hier, hier_audit, hier_edges = self.hierarchy_reducer.run(
                after_sab, ic_map, metadata, sibling_threshold=sib_threshold,
                graph=subgraph)
            log(f"    Hierarchy: {len(after_sab)} -> {len(after_hier)} (removed {len(after_sab) - len(after_hier)})")

            audit = sab_audit + hier_audit
            all_post_hier.update(after_hier)
            per_text.append((context_string, cuis, after_hier, hier_edges, audit, sib_threshold, subgraph))

        # ONE embedding fetch for all post-hierarchy CUIs
        log(f"\n[Batch] Fetching embeddings for {len(all_post_hier)} unique post-hierarchy CUIs")
        all_embeddings = self.emb_fetcher.fetch(list(all_post_hier))
        log(f"[Batch] Embeddings: {len(all_embeddings)}/{len(all_post_hier)} matched")

        # per-text: embedding reduction + topics + retention
        results = []
        for context_string, cuis, after_hier, hier_edges, all_audit, sib_threshold, subgraph in per_text:
            t_start = time.perf_counter()
            label = context_string[:50]

            if not after_hier:
                results.append(self._empty(context_string, t_start, all_audit))
                continue

            try:
                embeddings = {c: all_embeddings[c] for c in after_hier if c in all_embeddings}
                emb_cov = len(embeddings) / len(after_hier) if after_hier else 0

                # precompute graph info on subgraph (not full graph)
                batch_info = BatchGraphInfo(
                    subgraph, after_hier, ic_map,
                    sibling_threshold=sib_threshold)

                topics, surviving, emb_audit, avg_sil = EmbeddingReducer.run(
                    after_hier, metadata, embeddings, ic_map,
                    graph=subgraph,
                    sibling_threshold=sib_threshold,
                    batch_info=batch_info)
                all_audit.extend(emb_audit)
                log(f"\n  [{label}]")
                log(f"    Embedding Reduction: {len(after_hier)} -> {len(surviving)} (removed {len(after_hier) - len(surviving)})")
                if avg_sil != 0.0:
                    log(f"    Avg Silhouette: {avg_sil:.3f}")

                topics_out = {}
                for topic_id, t_cuis in topics.items():
                    if not t_cuis:
                        continue
                    topic_types = set()
                    cui_terms = {}
                    for c in t_cuis:
                        m = metadata.get(c)
                        if m and m.semantic_types:
                            topic_types.update(m.semantic_types)
                        cui_terms[c] = m.preferred_term if m else c
                    topics_out[topic_id] = TopicInfo(
                        topic_id=topic_id,
                        semantic_types=topic_types or {"Unknown"},
                        cuis=t_cuis,
                        cui_terms=cui_terms)

                log(f"    Topics: {len(surviving)} CUIs -> {len(topics_out)} topics")

                # embed input text once — used for cross-topic dedup AND retention
                text_vec = self.retention_scorer.embed_text(context_string)

                # cross-topic dedup (topics stay intact, only output list trimmed)
                before_xtd = len(surviving)
                surviving, xtd_audit = CrossTopicDedup.run(
                    surviving, embeddings, text_vec, metadata, ic_map,
                    sibling_threshold=sib_threshold, batch_info=batch_info)
                all_audit.extend(xtd_audit)
                if xtd_audit:
                    log(f"    Cross-Topic Dedup: {before_xtd} -> {len(surviving)} (removed {before_xtd - len(surviving)})")

                # coverage rescue: restore CUIs from lost semantic axes
                surviving, rescue_audit = coverage_rescue(
                    surviving, cuis, metadata, ic_map, embeddings,
                    text_vec=text_vec)
                all_audit.extend(rescue_audit)

                # retention scoring (alignment for downstream use)
                sorted_cuis, retention = self.retention_scorer.score(
                    text_vec, surviving, embeddings, metadata, all_audit)
                if retention:
                    log(f"    Retention: scored {len(retention)} (top={retention[0].alignment:.3f}, bottom={retention[-1].alignment:.3f})")

                # prune topics to match surviving CUIs (after cross-topic dedup)
                final_set = set(sorted_cuis)
                before_prune = len(topics_out)
                pruned_topics = {}
                for topic_id, topic in topics_out.items():
                    kept_cuis = [c for c in topic.cuis if c in final_set]
                    if not kept_cuis:
                        continue
                    kept_terms = {c: topic.cui_terms[c] for c in kept_cuis if c in topic.cui_terms}
                    pruned_topics[topic_id] = TopicInfo(
                        topic_id=topic_id,
                        semantic_types=topic.semantic_types,
                        cuis=kept_cuis,
                        cui_terms=kept_terms)
                topics_out = pruned_topics
                if before_prune != len(topics_out):
                    log(f"    Topics after pruning: {len(topics_out)} (dropped {before_prune - len(topics_out)} empty)")

                audited = {e.removed_cui for e in all_audit}
                survived = set(surviving)
                unaccounted = set(cuis) - survived - audited
                if unaccounted:
                    for cui in unaccounted:
                        m = metadata.get(cui)
                        all_audit.append(AuditEntry(
                            cui, m.preferred_term if m else "",
                            "", "", "unaccounted",
                            "CUI not in surviving or audit trail"))

                # --- Quality metrics ---
                # Semantic loss: what fraction of input semantic types are NOT
                # covered by any surviving CUI?
                input_stys = set()
                for cui in cuis:
                    m = metadata.get(cui)
                    if m and m.semantic_types:
                        input_stys.update(m.semantic_types)
                output_stys = set()
                for cui in surviving:
                    m = metadata.get(cui)
                    if m and m.semantic_types:
                        output_stys.update(m.semantic_types)
                sem_loss = 1.0 - (len(output_stys) / len(input_stys)) if input_stys else 0.0

                # Confidence score (0-1): composite of cluster quality,
                # reduction ratio sanity, and embedding coverage
                # - silhouette: cluster quality (0 = random, 1 = perfect)
                # - ratio penalty: if we removed >95% or <30%, something may be off
                # - embedding coverage: how many CUIs had embeddings
                ratio = len(surviving) / len(cuis) if cuis else 0
                ratio_score = 1.0 - abs(ratio - 0.15) / 0.85  # peaks at ~15% retention
                ratio_score = max(0.0, min(1.0, ratio_score))
                sil_component = (avg_sil + 1.0) / 2.0  # normalize -1..1 → 0..1
                confidence = (
                    0.40 * sil_component +
                    0.30 * emb_cov +
                    0.20 * (1.0 - sem_loss) +
                    0.10 * ratio_score
                )

                log(f"    Quality: silhouette={avg_sil:.3f}, confidence={confidence:.3f}, "
                    f"semantic_loss={sem_loss:.3f}")

                elapsed = (time.perf_counter() - t_start) * 1000
                log(f"    DONE: {len(cuis)} -> {len(surviving)} CUIs, {len(topics_out)} topics ({elapsed:.0f}ms)")

                results.append(ReductionResult(
                    context_string=context_string,
                    input_count=len(cuis),
                    after_filter_count=len(after_hier),
                    after_hierarchy_count=len(after_hier),
                    after_redundancy_count=len(surviving),
                    topics=topics_out,
                    all_reduced_cuis=sorted_cuis,
                    retention=retention,
                    audit_trail=all_audit,
                    processing_time_ms=elapsed,
                    metadata_coverage=emb_cov,
                    embedding_coverage=emb_cov,
                    silhouette_score=avg_sil,
                    confidence_score=confidence,
                    semantic_loss=sem_loss))

            except Exception as e:
                log(f"    FAILED [{label}]: {e}", "ERROR")
                results.append(self._empty(context_string, t_start, all_audit))

        self.emb_fetcher.shutdown()
        total = (time.perf_counter() - t0) * 1000
        log(f"\n[Batch] ALL DONE: {len(inputs)} texts in {total:.0f}ms")

        return results

    def _empty(self, ctx, t0, audit=None, meta_cov=0.0):
        return ReductionResult(
            ctx, 0, 0, 0, 0, {}, [], [],
            audit or [], (time.perf_counter() - t0) * 1000, meta_cov, 0.0)

    def get_stats(self):
        with _timing_lock:
            out = {}
            for k, v in _timings.items():
                if v:
                    vals = list(v)
                    out[k] = {
                        "count": len(vals),
                        "mean_ms": float(np.mean(vals)),
                        "p99_ms": float(np.percentile(vals, 99)),
                    }
            return out


# %% [17] CUI Extraction API
class CUIExtractor:
    """Calls CUI extraction endpoint. Filters by confidence."""

    def __init__(self, api_url, min_confidence=0.0, top_k=3):
        self.url = api_url
        self.min_confidence = min_confidence
        self.top_k = top_k
        self.session = requests.Session()

        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text):
        resp = self.session.post(
            self.url, headers=self.headers,
            json={"query_texts": [text], "top_k": self.top_k},
            timeout=200)
        resp.raise_for_status()
        data = resp.json()

        cuis = []
        if isinstance(data, dict):
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        raw_score = item.get("score", item.get("confidence", 1.0))
                        try:
                            score = float(raw_score)
                        except (ValueError, TypeError):
                            score = 0.0
                        if score >= self.min_confidence:
                            cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
        return list(set(cuis))


# %% [18] Load Pickles -- RUN ONCE
HIERARCHY_PATH = "umls_hierarchy.pkl"
IC_PATH = "ic_precomputed.pkl"

log("Loading hierarchy...")
with open(HIERARCHY_PATH, "rb") as f:
    hierarchy = pickle.load(f)
log(f"Hierarchy: {hierarchy.number_of_nodes()} nodes, {hierarchy.number_of_edges()} edges")

log("Loading IC scores...")
with open(IC_PATH, "rb") as f:
    ic_precomputed = pickle.load(f)
log(f"IC scores: {len(ic_precomputed)} entries")


# %% [19] Init System
PROJECT_ID = "your-project-id"
DATASET_ID = "your-dataset"
API_URL = "https://your-api/extract"
EMBEDDING_TABLE = "cui_embeddings"
ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM", "SNOMEDCT_US", "LNC"]

system = CUIReductionSystem(
    PROJECT_ID, DATASET_ID,
    hierarchy,
    ALLOWED_SABS,
    EMBEDDING_TABLE,
    ic_scores=ic_precomputed,
)
extractor = CUIExtractor(API_URL)
log("System ready.")


# %% [20] Run Pipeline
texts = [
    "Patient has severe pain in left knee with swelling",
]

# extract CUIs in parallel
def _extract(text):
    cuis = extractor.extract(text)
    return (cuis, text) if cuis else None

with ThreadPoolExecutor(max_workers=min(len(texts), 8)) as pool:
    raw = pool.map(_extract, texts)
    inputs = [r for r in raw if r is not None]

# single batch call
results = system.reduce_batch(inputs)

# display results
for result in results:
    log(f"\n{'=' * 60}")
    log(f"TEXT: {result.context_string}")
    log(f"SUMMARY: {result.input_count} -> {result.after_redundancy_count} CUIs, "
        f"{len(result.topics)} topics ({result.processing_time_ms:.0f}ms)")
    log(f"  Quality: silhouette={result.silhouette_score:.3f}, "
        f"confidence={result.confidence_score:.3f}, "
        f"semantic_loss={result.semantic_loss:.3f}")

    log(f"\n-- Top 20 CUIs by retention (of {len(result.retention)}) --")
    for entry in result.retention[:20]:
        stys = ", ".join(entry.semantic_types[:2]) if entry.semantic_types else "?"
        line = f"  {entry.cui} | align={entry.alignment} | [{stys}] {entry.term}"
        if entry.explains_cuis:
            line += f" | explains {len(entry.explains_cuis)}"
        log(line)

    log(f"\n-- Topics: {len(result.topics)} (showing first 20) --")
    for tid, topic in list(result.topics.items())[:20]:
        log(f"  {tid} ({len(topic.cuis)} CUIs)")
    if len(result.topics) > 20:
        log(f"  ... {len(result.topics) - 20} more topics")
