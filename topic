"""
CUI Reduction System

Reduces extracted CUIs through filtering, hierarchy pruning,
embedding-based redundancy removal, and retention scoring.
Groups results into topics sorted by narrative alignment.

pip install google-cloud-bigquery networkx numpy requests scann google-genai
"""

# %% [1] Imports
import time
import logging
import threading
import subprocess
import pickle
import functools
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque, OrderedDict
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import networkx as nx
import requests
import scann
from google.cloud import bigquery
from google import genai


# --- Configurable thresholds (override for domain-specific tuning) ---
DEFAULT_SIBLING_IC = 8.0          # fallback when < 10 parents for Otsu
RARE_CONCEPT_IC_FLOOR = 10.0      # min IC to qualify for rare-concept protection
RARE_CONCEPT_MAX_NEIGHBORS = 1    # max close neighbors to be "isolated"
CROSS_TOPIC_MAX_DIST = 0.05       # cosine distance ceiling for cross-topic dedup
SUBGRAPH_MAX_NODES = 200_000      # safety cap for subgraph construction
WALK_MAX_VISITED = 10_000         # safety cap per-CUI graph walk
PARENT_IC_CACHE_SIZE = 100_000    # bounded cache for on-the-fly IC computation
BQ_BATCH_SIZE = 5000              # BigQuery IN clause batch size


# %% [2] Logging and Timing
logger = logging.getLogger("cui_reduction")
logger.setLevel(logging.INFO)
if not logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s", datefmt="%H:%M:%S"))
    logger.addHandler(_handler)


def log(msg, level="INFO"):
    getattr(logger, level.lower(), logger.info)(msg)


_timings = defaultdict(list)
_timing_lock = threading.Lock()
_MAX_TIMING_ENTRIES = 1000  # per function, prevents memory leak


def timed(name):
    def dec(fn):
        @functools.wraps(fn)
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                elapsed = (time.perf_counter() - t0) * 1000
                with _timing_lock:
                    entries = _timings[name]
                    if len(entries) >= _MAX_TIMING_ENTRIES:
                        entries.pop(0)
                    entries.append(elapsed)
        return wrapper
    return dec


# %% [3] Data Models
@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    semantic_types: List[str]
    semantic_type_ids: List[str]
    ic_score: float
    source_vocabs: List[str]


@dataclass
class TopicInfo:
    topic_id: str
    semantic_types: Set[str]
    cuis: List[str]
    cui_terms: Dict[str, str]


@dataclass
class AuditEntry:
    removed_cui: str
    removed_term: str
    kept_cui: str
    kept_term: str
    reason: str
    detail: str


@dataclass
class RetentionEntry:
    cui: str
    term: str
    alignment: float
    semantic_types: List[str]
    explains_cuis: List[str]


@dataclass
class ReductionResult:
    context_string: str
    input_count: int
    after_filter_count: int
    after_hierarchy_count: int
    after_redundancy_count: int
    topics: Dict[str, TopicInfo]
    all_reduced_cuis: List[str]
    retention: List[RetentionEntry]
    audit_trail: List[AuditEntry]
    processing_time_ms: float
    metadata_coverage: float
    embedding_coverage: float
    # quality metrics
    silhouette_score: float = 0.0        # cluster quality (-1 to 1, higher = better)
    confidence_score: float = 0.0        # reduction confidence (0 to 1)
    semantic_loss: float = 0.0           # estimated info loss (0 to 1, lower = better)


# %% [4] Graph Safety Checks


_parent_ic_cache = {}
_parent_ic_cache_lock = threading.Lock()


def _get_parent_ic(graph, parent, ic_map):
    """Get IC for a parent CUI. Checks ic_map → cache → compute."""
    pic = ic_map.get(parent)
    if pic is not None:
        return pic
    cached = _parent_ic_cache.get(parent)
    if cached is not None:
        return cached
    desc = _count_desc(graph, parent)
    total = max(graph.number_of_nodes(), 1)
    pic = -np.log((desc + 1) / total)
    with _parent_ic_cache_lock:
        if len(_parent_ic_cache) >= PARENT_IC_CACHE_SIZE:
            # evict oldest 10% to avoid repeated eviction
            evict_n = PARENT_IC_CACHE_SIZE // 10
            keys = list(_parent_ic_cache.keys())[:evict_n]
            for k in keys:
                del _parent_ic_cache[k]
        _parent_ic_cache[parent] = pic
    return pic


def _count_desc(graph, cui, max_count=5000):
    """Count descendants via BFS, capped for performance.
    Broad parents hit cap fast → low IC. Specific parents finish → exact IC."""
    if not graph.has_node(cui):
        return 0
    visited = set()
    frontier = deque([cui])
    while frontier and len(visited) < max_count:
        n = frontier.popleft()
        for child in graph.successors(n):
            if child not in visited:
                visited.add(child)
                frontier.append(child)
    return len(visited)


def _otsu(values):
    """Otsu's method — threshold maximizing between-class variance."""
    lo, hi = float(values.min()), float(values.max())
    if hi - lo < 1e-9:
        return lo

    n_bins = 256
    bins = np.linspace(lo, hi, n_bins + 1)
    hist, _ = np.histogram(values, bins=bins)
    hist = hist.astype(np.float64)
    total = hist.sum()
    if total == 0:
        return lo

    centers = (bins[:-1] + bins[1:]) / 2.0
    best_t, best_var = lo, -1.0
    sum_total = np.sum(hist * centers)
    sum_bg, weight_bg = 0.0, 0.0

    for i in range(n_bins):
        weight_bg += hist[i]
        if weight_bg == 0:
            continue
        weight_fg = total - weight_bg
        if weight_fg == 0:
            break
        sum_bg += hist[i] * centers[i]
        mean_bg = sum_bg / weight_bg
        mean_fg = (sum_total - sum_bg) / weight_fg
        var = weight_bg * weight_fg * (mean_bg - mean_fg) ** 2
        if var > best_var:
            best_var = var
            best_t = centers[i]

    return float(best_t)


# --- Precomputed batch graph info ---

class BatchGraphInfo:
    """Precompute parents and IC-bounded ancestors for a CUI batch.
    Walks up from each CUI collecting ancestors while IC >= threshold."""

    def __init__(self, graph, cuis, ic_map, sibling_threshold=DEFAULT_SIBLING_IC):
        self.graph = graph
        self.ic_map = ic_map
        self.parents = {}
        self.ancestors = {}
        self._distant_cache = {}

        for cui in cuis:
            if graph is not None and graph.has_node(cui):
                p = set(graph.predecessors(cui))
                self.parents[cui] = p
                anc = set()
                frontier = set(p)
                while frontier:
                    next_frontier = set()
                    for node in frontier:
                        pic = _get_parent_ic(graph, node, ic_map)
                        if pic >= sibling_threshold:
                            anc.add(node)
                            if graph.has_node(node):
                                for gp in graph.predecessors(node):
                                    if gp not in anc:
                                        next_frontier.add(gp)
                    frontier = next_frontier
                    if len(anc) > WALK_MAX_VISITED:  # safety cap
                        break
                self.ancestors[cui] = anc
            else:
                self.parents[cui] = set()
                self.ancestors[cui] = set()

        log(f"    BatchGraphInfo: {len(self.ancestors)} CUIs, "
            f"IC-bounded ancestors (threshold={sibling_threshold:.2f})")

    def are_siblings(self, cui_a, cui_b, sibling_threshold=DEFAULT_SIBLING_IC):
        """Check if two CUIs share a specific parent (IC >= threshold)."""
        pa = self.parents.get(cui_a, set())
        pb = self.parents.get(cui_b, set())
        shared = pa & pb
        if not shared:
            return False
        for p in shared:
            pic = _get_parent_ic(self.graph, p, self.ic_map)
            if pic >= sibling_threshold:
                return True
        return False

    def graph_distant(self, cui_a, cui_b):
        """Cached: check if two CUIs share no IC-bounded ancestors.
        Returns False for CUIs with empty ancestor sets (unknown → don't protect)."""
        key = (cui_a, cui_b) if cui_a < cui_b else (cui_b, cui_a)
        if key in self._distant_cache:
            return self._distant_cache[key]

        anc_a = self.ancestors.get(cui_a, set())
        anc_b = self.ancestors.get(cui_b, set())

        if cui_a in anc_b or cui_b in anc_a:
            self._distant_cache[key] = False
            return False

        distant = (len(anc_a & anc_b) == 0) and bool(anc_a) and bool(anc_b)
        self._distant_cache[key] = distant
        return distant

    def ontology_penalty(self, cui_a, cui_b, sibling_threshold=DEFAULT_SIBLING_IC):
        """Continuous penalty [0,1]: 0=related, 0.3=broad sibs, 0.6=specific sibs, 1=distant."""
        anc_a = self.ancestors.get(cui_a, set())
        anc_b = self.ancestors.get(cui_b, set())

        if cui_a in anc_b or cui_b in anc_a:
            return 0.0

        if (len(anc_a & anc_b) == 0) and anc_a and anc_b:
            return 1.0

        shared_parents = self.parents.get(cui_a, set()) & self.parents.get(cui_b, set())
        if shared_parents:
            max_pic = max(_get_parent_ic(self.graph, p, self.ic_map)
                         for p in shared_parents)
            if max_pic >= sibling_threshold:
                return 0.6
            else:
                return 0.3

        return 0.2

    def ontology_branch(self, cui, max_depth=3):
        """Get highest ancestor within max_depth hops (branch preclustering)."""
        if self.graph is None or not self.graph.has_node(cui):
            return cui
        current = cui
        for _ in range(max_depth):
            pars = list(self.graph.predecessors(current))
            if not pars:
                break
            current = max(pars, key=lambda p: self.graph.out_degree(p))
        return current


@timed("compute_sibling_threshold")
def compute_sibling_threshold(graph, cuis, ic_map):
    """Data-driven threshold for sibling protection using Otsu's method."""
    parent_ics = []
    seen_parents = set()
    for cui in cuis:
        if not graph.has_node(cui):
            continue
        for p in graph.predecessors(cui):
            if p not in seen_parents:
                seen_parents.add(p)
                pic = _get_parent_ic(graph, p, ic_map)
                parent_ics.append(pic)

    if len(parent_ics) < 10:
        return DEFAULT_SIBLING_IC

    arr = np.array(parent_ics)
    threshold = _otsu(arr)

    log(f"    Sibling IC threshold (Otsu): {threshold:.2f} "
        f"(from {len(parent_ics)} parents, "
        f"range {arr.min():.2f}-{arr.max():.2f}, "
        f"median {np.median(arr):.2f})")

    return threshold


@timed("build_subgraph")
def build_subgraph(full_graph, cuis, ic_map, ic_floor):
    """Build a small subgraph: input CUIs + IC-bounded ancestors.
    Walks up from each CUI, collecting ancestors while IC >= ic_floor.
    Same IC logic as the downstream walks — no arbitrary depth cap."""
    relevant = set()
    for cui in cuis:
        if full_graph.has_node(cui):
            relevant.add(cui)

    frontier = set(relevant)
    while frontier:
        next_frontier = set()
        for node in frontier:
            for parent in full_graph.predecessors(node):
                if parent not in relevant:
                    pic = _get_parent_ic(full_graph, parent, ic_map)
                    if pic >= ic_floor:
                        relevant.add(parent)
                        next_frontier.add(parent)
        frontier = next_frontier
        if len(relevant) > SUBGRAPH_MAX_NODES:  # safety cap
            log(f"    Subgraph safety cap hit at {len(relevant)} nodes", "WARNING")
            break

    sub = full_graph.subgraph(relevant)
    log(f"    Subgraph: {sub.number_of_nodes()} nodes, "
        f"{sub.number_of_edges()} edges "
        f"(from {full_graph.number_of_nodes()} full)")
    return sub


# %% [5] LRU Cache (thread-safe)
class Cache:
    def __init__(self, max_size):
        self._max = max_size
        self._d = OrderedDict()
        self._lock = threading.Lock()

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
        return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# %% [6] Metadata from BigQuery


class MetadataFetcher:
    """Pulls CUI metadata from MRCONSO/MRSTY.
    Picks preferred term: ISPREF=Y first, then longest STR."""

    def __init__(self, bq, pid, did):
        self.bq = bq
        self.pid = pid
        self.did = did
        self._cache = Cache(50_000)

    @timed("metadata_fetch")
    def fetch(self, cuis):
        result = {}
        missing = []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), BQ_BATCH_SIZE):
            batch = missing[i:i + BQ_BATCH_SIZE]
            query = f"""
            WITH ranked AS (
              SELECT CUI, STR, SAB, ISPREF,
                ROW_NUMBER() OVER (
                  PARTITION BY CUI
                  ORDER BY
                    CASE SAB
                      WHEN 'SNOMEDCT_US' THEN 0
                      WHEN 'ICD10CM' THEN 1
                      WHEN 'ICD10PCS' THEN 2
                      WHEN 'ICD9CM' THEN 3
                      WHEN 'LNC' THEN 4
                      ELSE 5
                    END,
                    CASE WHEN ISPREF='Y' THEN 0 ELSE 1 END,
                    LENGTH(STR) DESC
                ) AS rn
              FROM `{self.pid}.{self.did}.MRCONSO`
              WHERE CUI IN UNNEST(@cuis) AND LAT = 'ENG'
            )
            SELECT
              r.CUI AS cui,
              MAX(CASE WHEN r.rn = 1 THEN r.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT s.STY IGNORE NULLS) AS stys,
              ARRAY_AGG(DISTINCT c2.SAB IGNORE NULLS) AS sabs
            FROM ranked r
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s
              ON r.CUI = s.CUI
            LEFT JOIN `{self.pid}.{self.did}.MRCONSO` c2
              ON r.CUI = c2.CUI AND c2.LAT = 'ENG'
            WHERE r.rn = 1
            GROUP BY r.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            for row in self.bq.query(
                    query, job_config=jc, timeout=300).result():
                term = row.pref_term or row.cui
                meta = CUIMetadata(
                    cui=row.cui,
                    preferred_term=term,
                    semantic_types=list(row.stys or []),
                    semantic_type_ids=list(row.tuis or []),
                    ic_score=0.0,
                    source_vocabs=list(row.sabs or []),
                )
                self._cache.put(row.cui, meta)
                result[row.cui] = meta

        return result


# %% [7] IC Scores
class ICComputer:
    """IC = -log((descendants + 1) / total).
    Use precomputed dict in production to avoid BFS."""

    def __init__(self, graph, precomputed=None):
        self.graph = graph
        self.precomputed = precomputed or {}
        self._cache = Cache(100_000)
        self._total = max(graph.number_of_nodes(), 1)

    @timed("ic_compute")
    def compute(self, cuis, metadata):
        result = {}
        for cui in cuis:
            if cui in self.precomputed:
                ic = self.precomputed[cui]
            else:
                cached = self._cache.get(cui)
                if cached is not None:
                    ic = cached
                else:
                    desc = self._count_descendants(cui)
                    ic = -np.log((desc + 1) / self._total)
                    self._cache.put(cui, ic)
            result[cui] = ic
            m = metadata.get(cui)
            if m:
                m.ic_score = ic
        return result

    def _count_descendants(self, cui):
        if not self.graph.has_node(cui):
            return 0
        visited = set()
        q = deque([cui])
        while q:
            n = q.popleft()
            for child in self.graph.successors(n):
                if child not in visited:
                    visited.add(child)
                    q.append(child)
        return len(visited)


# %% [8] Filters
class SABFilter:
    def __init__(self, allowed):
        self.allowed = set(allowed)

    @timed("sab_filter")
    def run(self, cuis, metadata):
        kept, audit = [], []
        for cui in cuis:
            m = metadata.get(cui)
            if not m:
                audit.append(AuditEntry(
                    cui, "", "", "", "no_metadata",
                    "CUI not found in BQ"))
                continue
            if not any(s in self.allowed for s in m.source_vocabs):
                audit.append(AuditEntry(
                    cui, m.preferred_term, "", "",
                    "sab_filtered",
                    f"vocabs {m.source_vocabs} not in allowed"))
                continue
            kept.append(cui)
        return kept, audit


# %% [9] Hierarchy Reduction
class HierarchyReducer:
    """Remove ancestor CUIs when a more specific descendant exists.
    Uses IC-bounded ancestor walk to catch multi-hop subsumption."""

    def __init__(self, graph):
        self.graph = graph

    @timed("hierarchy_reduce")
    def run(self, cuis, ic_map, metadata, sibling_threshold=None, graph=None):
        g = graph if graph is not None else self.graph
        cui_set = set(cuis)
        edges = set()

        if sibling_threshold is not None:
            ic_floor = sibling_threshold
        else:
            ic_vals = [ic_map.get(c, 0) for c in cuis if ic_map.get(c, 0) > 0]
            ic_floor = float(np.median(ic_vals)) / 2 if ic_vals else 4.0

        to_remove = {}

        for cui in cuis:
            if not g.has_node(cui):
                continue

            c_ic = ic_map.get(cui, 0)
            c_meta = metadata.get(cui)
            c_types = set(c_meta.semantic_types) if c_meta and c_meta.semantic_types else set()

            visited = set()
            frontier = set(g.predecessors(cui))
            while frontier:
                next_frontier = set()
                for anc in frontier:
                    if anc in visited or anc == cui:
                        continue
                    visited.add(anc)

                    pic = _get_parent_ic(g, anc, ic_map)

                    if anc in cui_set:
                        a_ic = ic_map.get(anc, 0)
                        if c_ic >= a_ic:
                            a_meta = metadata.get(anc)
                            a_types = set(a_meta.semantic_types) if a_meta and a_meta.semantic_types else set()
                            if not a_types or not c_types or (a_types & c_types):
                                edges.add((anc, cui))
                                if anc not in to_remove or c_ic > to_remove[anc][1]:
                                    to_remove[anc] = (cui, c_ic, a_ic)

                    if pic >= ic_floor and g.has_node(anc):
                        for gp in g.predecessors(anc):
                            if gp not in visited:
                                next_frontier.add(gp)

                frontier = next_frontier
                if len(visited) > WALK_MAX_VISITED:
                    break

        remove_set = set(to_remove.keys())
        surviving = [c for c in cuis if c not in remove_set]

        audit = []
        for parent, (child, c_ic, p_ic) in to_remove.items():
            pm = metadata.get(parent)
            cm = metadata.get(child)
            audit.append(AuditEntry(
                parent, pm.preferred_term if pm else "",
                child, cm.preferred_term if cm else "",
                "ancestor_subsumed",
                f"child IC {c_ic:.2f} >= ancestor IC {p_ic:.2f} (multi-hop)"))

        return surviving, audit, edges


# %% [10] Embedding Fetch (parallel BQ batches + disk cache)
class EmbeddingFetcher:
    def __init__(self, bq, pid, did, embedding_table, cache_path=None):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.table = embedding_table
        self._cache = Cache(50_000)
        self._pool = ThreadPoolExecutor(max_workers=4)
        self._cache_path = cache_path
        self._disk_cache = {}
        self._disk_dirty = False

        # load disk cache if exists
        if cache_path:
            import os
            if os.path.exists(cache_path):
                try:
                    data = np.load(cache_path, allow_pickle=False)
                    cuis_arr = data['cuis']
                    vecs_arr = data['vecs']
                    for i, cui in enumerate(cuis_arr):
                        self._disk_cache[str(cui)] = vecs_arr[i]
                    log(f"    Embedding disk cache: loaded {len(self._disk_cache)} from {cache_path}")
                except Exception as e:
                    log(f"    Embedding disk cache load failed: {e}", "WARNING")

    def shutdown(self):
        """Save dirty cache to disk, then clean up thread pool."""
        if self._cache_path and self._disk_dirty:
            try:
                cuis = list(self._disk_cache.keys())
                vecs = np.array([self._disk_cache[c] for c in cuis], dtype=np.float32)
                np.savez_compressed(self._cache_path, cuis=np.array(cuis), vecs=vecs)
                log(f"    Embedding disk cache: saved {len(cuis)} to {self._cache_path}")
            except Exception as e:
                log(f"    Embedding disk cache save failed: {e}", "WARNING")
        self._pool.shutdown(wait=False)

    def _fetch_batch(self, batch):
        """Fetch one batch from BQ. Called in parallel."""
        rows = []
        query = f"""
        SELECT cui, embedding
        FROM `{self.pid}.{self.did}.{self.table}`
        WHERE cui IN UNNEST(@cuis)
        """
        jc = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ArrayQueryParameter("cuis", "STRING", batch)
        ])
        for row in self.bq.query(
                query, job_config=jc, timeout=300).result():
            raw = row.embedding
            if raw is None or len(raw) == 0:
                continue
            vec = np.array(raw, dtype=np.float32)
            if not np.all(np.isfinite(vec)):
                continue
            norm = np.linalg.norm(vec)
            if norm == 0:
                continue
            vec = vec / norm
            rows.append((row.cui, vec))
        return rows

    @timed("embedding_fetch")
    def fetch(self, cuis):
        result, missing = {}, []
        for c in cuis:
            # check in-memory cache first
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
                continue
            # then disk cache
            if c in self._disk_cache:
                vec = self._disk_cache[c]
                self._cache.put(c, vec)
                result[c] = vec
                continue
            missing.append(c)

        if not missing:
            log(f"    Embeddings: {len(result)} from cache, 0 from BQ")
            return result

        log(f"    Embeddings: {len(result)} cached, {len(missing)} fetching from BQ")
        batches = [missing[i:i + BQ_BATCH_SIZE]
                    for i in range(0, len(missing), BQ_BATCH_SIZE)]

        futures = [self._pool.submit(self._fetch_batch, b) for b in batches]
        for f in futures:
            for cui, vec in f.result():
                self._cache.put(cui, vec)
                self._disk_cache[cui] = vec
                self._disk_dirty = True
                result[cui] = vec

        return result


# %% [11] Greedy Embedding Reduction via ScaNN

def _find_elbow(values):
    """Find elbow point in a descending-sorted curve using max perpendicular
    distance from the line connecting first and last points (Kneedle method).
    Returns the index of the elbow point."""
    n = len(values)
    if n < 3:
        return n - 1

    # normalize to [0,1] on both axes
    x = np.linspace(0, 1, n)
    y_min, y_max = values[-1], values[0]
    y_range = y_max - y_min
    if y_range < 1e-12:
        return n // 2
    y = (np.array(values) - y_min) / y_range

    # line from first to last point
    dx = x[-1] - x[0]
    dy = y[-1] - y[0]
    line_len = np.sqrt(dx * dx + dy * dy)
    if line_len < 1e-12:
        return n // 2

    # perpendicular distance from each point to the line
    dist = np.abs(dy * x - dx * y + x[-1] * y[0] - y[-1] * x[0]) / line_len

    return int(np.argmax(dist))


class EmbeddingReducer:
    """Greedy reduction: rank by IC, keep CUI if far from all already-kept.
    Elbow detection on sorted uniqueness scores — fully data-driven cutoff.
    ONE batch ScaNN search, then greedy from pre-computed neighbors."""

    @staticmethod
    @timed("embedding_reduce")
    def run(cuis, metadata, embeddings, ic_map,
            graph=None, sibling_threshold=DEFAULT_SIBLING_IC, batch_info=None):

        # validate embedding dimensions
        if embeddings:
            dims = {v.shape[0] for v in embeddings.values() if hasattr(v, 'shape')}
            if len(dims) > 1:
                target_dim = max(dims, key=lambda d: sum(
                    1 for v in embeddings.values() if hasattr(v, 'shape') and v.shape[0] == d))
                log(f"    Mixed embedding dims {dims}, keeping dim={target_dim}", "WARNING")
                embeddings = {k: v for k, v in embeddings.items()
                             if hasattr(v, 'shape') and v.shape[0] == target_dim}

        with_emb = [c for c in cuis if c in embeddings]
        without_emb = [c for c in cuis if c not in embeddings]

        if len(with_emb) <= 1:
            surviving = list(cuis)
            return {"topic_0": surviving}, surviving, [], 0.0

        n = len(with_emb)

        # rank by IC descending (most specific first)
        ranked = sorted(with_emb, key=lambda c: ic_map.get(c, 0), reverse=True)
        matrix = np.array([embeddings[c] for c in ranked], dtype=np.float32)
        dim = matrix.shape[1]

        # --- ONE batch ScaNN search ---
        k_search = min(100, n - 1)
        num_leaves = min(max(2, int(np.sqrt(n))), n)
        builder = scann.scann_ops_pybind.builder(matrix, k_search, "dot_product")
        if n >= 16 and dim >= 2:
            dims_per_block = 2 if dim % 2 == 0 else 1
            builder = (
                builder
                .tree(num_leaves=num_leaves,
                      num_leaves_to_search=max(1, num_leaves // 3),
                      training_sample_size=min(n, 250_000))
                .score_ah(dims_per_block, anisotropic_quantization_threshold=0.2)
                .reorder(min(k_search * 4, n))
            )
        else:
            builder = builder.score_brute_force()
        searcher = builder.build()
        all_neighbors, all_scores = searcher.search_batched(matrix)

        # --- Greedy pass: score every CUI's uniqueness ---
        # Walk IC-ranked list, for each CUI record its min distance to
        # any previously-seen CUI. This is its "uniqueness score":
        #   high = adds new information, low = redundant with something better
        kept_set = set()
        uniqueness = np.zeros(n, dtype=np.float64)
        closest_kept = np.full(n, -1, dtype=np.int64)

        for i in range(n):
            if not kept_set:
                uniqueness[i] = float('inf')
                kept_set.add(i)
                continue

            best_dist = float('inf')
            best_idx = -1
            for j_pos in range(len(all_neighbors[i])):
                j = int(all_neighbors[i][j_pos])
                if j < 0 or j >= n or j == i:
                    continue
                if j not in kept_set:
                    continue
                d = 1.0 - float(all_scores[i][j_pos])
                if d < best_dist:
                    best_dist = d
                    best_idx = j

            # if no kept neighbor found in ScaNN results, treat as unique
            if best_idx < 0:
                best_dist = 1.0

            # ontology rescue: boost score for ontologically distinct pairs
            if batch_info is not None and best_idx >= 0:
                penalty = batch_info.ontology_penalty(
                    ranked[i], ranked[best_idx], sibling_threshold)
                if penalty > 0.5:
                    best_dist = max(best_dist, 0.99)

            uniqueness[i] = best_dist
            closest_kept[i] = best_idx
            kept_set.add(i)

        # --- Elbow detection on sorted uniqueness scores ---
        # Sort descending: most unique first
        sorted_indices = np.argsort(-uniqueness)
        sorted_scores = uniqueness[sorted_indices]

        # replace inf with next highest for clean curve
        finite_mask = np.isfinite(sorted_scores)
        if not np.all(finite_mask):
            max_finite = np.max(sorted_scores[finite_mask]) if np.any(finite_mask) else 1.0
            sorted_scores[~finite_mask] = max_finite * 1.1

        # find elbow: where adding more CUIs gives diminishing returns
        elbow_idx = _find_elbow(sorted_scores.tolist())

        # safety bounds: keep at least 10, at most 90% of input
        elbow_idx = max(elbow_idx, min(10, n))
        elbow_idx = min(elbow_idx, int(n * 0.9))

        # the elbow CUIs are the ones to keep
        final_kept = set(sorted_indices[:elbow_idx].tolist())
        threshold = float(sorted_scores[elbow_idx - 1]) if elbow_idx > 0 else 0.0

        log(f"    Elbow detection: keep {elbow_idx}/{n} "
            f"(threshold={threshold:.4f}, "
            f"top={float(sorted_scores[0]):.4f}, "
            f"bottom={float(sorted_scores[-1]):.4f})")

        # --- Build audit trail ---
        audit = []
        for i in range(n):
            if i in final_kept:
                continue
            cui = ranked[i]
            k_idx = int(closest_kept[i])
            c_meta = metadata.get(cui)
            k_cui = ranked[k_idx] if k_idx >= 0 else ranked[0]
            k_meta = metadata.get(k_cui)
            audit.append(AuditEntry(
                removed_cui=cui,
                removed_term=c_meta.preferred_term if c_meta else "",
                kept_cui=k_cui,
                kept_term=k_meta.preferred_term if k_meta else "",
                reason="embedding_redundant",
                detail=f"uniqueness {uniqueness[i]:.3f} < elbow {threshold:.3f}"))

        surviving = [ranked[i] for i in sorted(final_kept)] + list(without_emb)
        log(f"    Greedy reduction: {n} -> {len(final_kept)} "
            f"(+{len(without_emb)} no-emb) = {len(surviving)} total")

        # --- Assign topics on surviving CUIs ---
        surv_with_emb = [c for c in surviving if c in embeddings]
        if len(surv_with_emb) < 2:
            return {"topic_0": surviving}, surviving, audit, 0.0

        topics = _assign_topics(surv_with_emb, embeddings)

        tid = len(topics)
        for c in without_emb:
            topics[f"topic_{tid}"] = [c]
            tid += 1

        return topics, surviving, audit, 0.0


def _assign_topics(cuis, embeddings, target_topic_size=15):
    """Assign topics via ScaNN k-means partitioning (not connected components).
    Produces balanced topics of ~target_topic_size CUIs."""
    n = len(cuis)
    matrix = np.array([embeddings[c] for c in cuis], dtype=np.float32)

    # number of topics ≈ n / target_topic_size
    n_topics = max(2, n // target_topic_size)
    n_topics = min(n_topics, n)

    # use ScaNN's tree partitioner as k-means
    k = min(5, n - 1)
    builder = scann.scann_ops_pybind.builder(matrix, k, "dot_product")
    builder = builder.tree(
        num_leaves=n_topics,
        num_leaves_to_search=1,
        training_sample_size=min(n, 250_000))
    if matrix.shape[1] % 2 == 0:
        builder = builder.score_ah(2, anisotropic_quantization_threshold=0.2)
    else:
        builder = builder.score_brute_force()
    searcher = builder.build()

    # each CUI's partition = its topic
    # search with leaves_to_search=1 returns the CUI's assigned partition
    _, _ = searcher.search_batched(matrix)

    # use the partitioner directly to get assignments
    # Simpler approach: just cluster by nearest centroid via batch search
    # Since ScaNN doesn't expose partition labels, use a quick k-means
    np.random.seed(42)
    # pick n_topics centroids via k-means++ style
    centroids_idx = [0]
    for _ in range(n_topics - 1):
        dists = 1.0 - matrix @ matrix[centroids_idx].T  # n x len(centroids)
        min_dists = dists.min(axis=1)
        # pick farthest point
        next_idx = int(np.argmax(min_dists))
        centroids_idx.append(next_idx)

    centroids = matrix[centroids_idx]  # n_topics x dim
    # assign each CUI to nearest centroid
    sims = matrix @ centroids.T  # n x n_topics
    labels = np.argmax(sims, axis=1)

    topics = defaultdict(list)
    for i, lbl in enumerate(labels):
        topics[f"topic_{lbl}"].append(cuis[i])

    return dict(topics)
    """Pick representative via IC+centrality. Remove CUIs that are close
    to ANY already-kept CUI (not just best). Safety checks protect
    rare/isolated concepts only."""
    if len(r_cuis) <= 1:
        return r_cuis, []

    # score all CUIs: IC (specificity) + centrality (cluster fit)
    ic_vals = {c: ic_map.get(c, 0) for c in r_cuis}
    cent_vals = {}
    for c in r_cuis:
        ci = index_map.get(c)
        if ci is not None and len(r_cuis) > 1:
            dists = [dist[ci, index_map[o]] for o in r_cuis
                     if o != c and index_map.get(o) is not None]
            cent_vals[c] = 1.0 - (sum(dists) / len(dists)) if dists else 0.0
        else:
            cent_vals[c] = 0.0

    def _norm(vals):
        lo = min(vals.values())
        hi = max(vals.values())
        rng = hi - lo
        if rng < 1e-9:
            return {k: 0.5 for k in vals}
        return {k: (v - lo) / rng for k, v in vals.items()}

    ic_n = _norm(ic_vals)
    cent_n = _norm(cent_vals)

    # sort by composite score, pick best first
    ranked = sorted(r_cuis, key=lambda c: ic_n[c] + cent_n[c], reverse=True)
    best = ranked[0]

    kept = [best]
    audit = []

    for c in ranked[1:]:
        c_meta = metadata.get(c)
        ci = index_map.get(c)

        # check if c is close to ANY already-kept CUI
        is_close_to_kept = False
        closest_kept = best
        closest_dist = float('inf')
        if ci is not None:
            for k in kept:
                ki = index_map.get(k)
                if ki is not None:
                    d = dist[ci, ki]
                    if d < closest_dist:
                        closest_dist = d
                        closest_kept = k
                    if d < tight_t:
                        is_close_to_kept = True

        # not close to any kept CUI → keep (genuinely different)
        if not is_close_to_kept:
            kept.append(c)
            continue

        # rare concept protection: high IC + isolated in the full topic
        c_ic = ic_map.get(c, 0)
        if c_ic > RARE_CONCEPT_IC_FLOOR and ci is not None:
            close_count = sum(1 for o in r_cuis if o != c and
                             index_map.get(o) is not None and
                             dist[ci, index_map[o]] < tight_t)
            if close_count <= RARE_CONCEPT_MAX_NEIGHBORS:
                kept.append(c)
                continue

        # close to a kept CUI and not rare → remove
        k_meta = metadata.get(closest_kept)
        audit.append(AuditEntry(
            removed_cui=c,
            removed_term=c_meta.preferred_term if c_meta else "",
            kept_cui=closest_kept,
            kept_term=k_meta.preferred_term if k_meta else "",
            reason="embedding_redundant",
            detail=f"dist {closest_dist:.3f} < tight {tight_t:.3f} | "
                   f"IC {ic_map.get(closest_kept,0):.2f} vs {ic_map.get(c,0):.2f} | "
                   f"centrality {cent_vals.get(closest_kept,0):.3f} vs {cent_vals.get(c,0):.3f}"))

    return kept, audit


# %% [13] Cross-Topic Dedup
class CrossTopicDedup:
    """Remove near-duplicate CUIs across topics. Alignment as tiebreaker."""

    @staticmethod
    @timed("cross_topic_dedup")
    def run(surviving, embeddings, text_vec, metadata, ic_map,
            sibling_threshold=DEFAULT_SIBLING_IC, batch_info=None):
        with_emb = [c for c in surviving if c in embeddings]
        without_emb = [c for c in surviving if c not in embeddings]

        if len(with_emb) <= 1:
            return surviving, []

        matrix = np.array([embeddings[c] for c in with_emb], dtype=np.float32)
        idx_map = {c: i for i, c in enumerate(with_emb)}

        # text alignment for tiebreaking
        alignments = {}
        for c in with_emb:
            alignments[c] = float(np.dot(text_vec, embeddings[c]))

        # pairwise cosine distances
        sim = matrix @ matrix.T
        np.clip(sim, -1, 1, out=sim)
        dist = 1.0 - sim
        np.fill_diagonal(dist, 999.0)

        # adaptive threshold: very tight — only true near-duplicates
        upper = dist[np.triu_indices(len(with_emb), k=1)]
        upper = upper[upper < 999.0]
        if len(upper) == 0:
            return surviving, []

        # 5th percentile of distances = tightest pairs
        threshold = float(np.percentile(upper, 5))
        # floor: don't go above 0.05 cosine distance
        threshold = min(threshold, CROSS_TOPIC_MAX_DIST)

        # find pairs below threshold, remove the one with lower alignment
        removed = set()
        audit = []

        for i in range(len(with_emb)):
            if with_emb[i] in removed:
                continue
            for j in range(i + 1, len(with_emb)):
                if with_emb[j] in removed:
                    continue
                if dist[i, j] > threshold:
                    continue

                ci, cj = with_emb[i], with_emb[j]

                # safety guards via batch_info (precomputed + cached)
                if batch_info.are_siblings(ci, cj, sibling_threshold):
                    continue
                if batch_info.graph_distant(ci, cj):
                    continue

                # keep higher alignment, break ties by IC
                if (alignments.get(ci, 0), ic_map.get(ci, 0)) >= \
                   (alignments.get(cj, 0), ic_map.get(cj, 0)):
                    keep, drop = ci, cj
                else:
                    keep, drop = cj, ci

                removed.add(drop)
                km = metadata.get(keep)
                dm = metadata.get(drop)
                audit.append(AuditEntry(
                    removed_cui=drop,
                    removed_term=dm.preferred_term if dm else "",
                    kept_cui=keep,
                    kept_term=km.preferred_term if km else "",
                    reason="cross_topic_dedup",
                    detail=f"dist {dist[idx_map[keep], idx_map[drop]]:.4f} "
                           f"< threshold {threshold:.4f} | "
                           f"align {alignments.get(keep,0):.3f} vs {alignments.get(drop,0):.3f}"))

        final = [c for c in surviving if c not in removed] 
        return final, audit


# %% [14] Coverage Rescue
@timed("coverage_rescue")
def coverage_rescue(surviving, all_input_cuis, metadata, ic_map, embeddings,
                    text_vec=None):
    """Rescue CUIs from semantic axes lost during reduction."""
    input_types = defaultdict(set)
    for c in all_input_cuis:
        m = metadata.get(c)
        if m and m.semantic_types:
            for t in m.semantic_types:
                input_types[t].add(c)

    output_types = set()
    for c in surviving:
        m = metadata.get(c)
        if m and m.semantic_types:
            output_types.update(m.semantic_types)

    lost_types = set(input_types.keys()) - output_types
    if not lost_types:
        return surviving, []

    surviving_set = set(surviving)
    rescued = []
    audit = []

    for lost_type in sorted(lost_types):
        candidates = input_types[lost_type] - surviving_set
        if not candidates:
            continue
        with_emb = [c for c in candidates if c in embeddings]
        pool = with_emb if with_emb else list(candidates)

        # prefer alignment to input text, fall back to IC
        if text_vec is not None and with_emb:
            best = max(pool, key=lambda c: float(np.dot(text_vec, embeddings[c])))
        else:
            best = max(pool, key=lambda c: ic_map.get(c, 0))

        rescued.append(best)
        surviving_set.add(best)
        m = metadata.get(best)
        audit.append(AuditEntry(
            removed_cui="", removed_term="",
            kept_cui=best,
            kept_term=m.preferred_term if m else "",
            reason="coverage_rescued",
            detail=f"rescued for lost type '{lost_type}' | IC {ic_map.get(best,0):.2f}"))

    if rescued:
        surviving = list(surviving) + rescued
        log(f"    Coverage Rescue: +{len(rescued)} CUIs for "
            f"{len(lost_types)} lost semantic types")

    return surviving, audit


# %% [15] Retention Scorer
class RetentionScorer:
    """Scores each CUI by narrative alignment via cosine(text_emb, cui_emb).
    Cost: 1 embedding API call (~100ms) per input text."""

    def __init__(self, project_id, location="us-central1"):
        self.client = genai.Client(
            vertexai=True, project=project_id, location=location)

    @timed("text_embed")
    def embed_text(self, text):
        """Embed input text. Returns normalized 3072-dim vector."""
        response = self.client.models.embed_content(
            model="gemini-embedding-001",
            contents=text,
        )
        text_vec = np.array(response.embeddings[0].values, dtype=np.float32)
        norm = np.linalg.norm(text_vec)
        if norm > 0:
            text_vec = text_vec / norm
        return text_vec

    @timed("retention_score")
    def score(self, text_vec, surviving, embeddings, metadata, audit_trail):

        alignments = {}
        for cui in surviving:
            if cui in embeddings:
                alignments[cui] = float(np.dot(text_vec, embeddings[cui]))
            else:
                alignments[cui] = 0.0

        explains = defaultdict(list)
        for entry in audit_trail:
            if entry.kept_cui and entry.removed_cui:
                explains[entry.kept_cui].append(entry.removed_cui)

        entries = []
        for cui in surviving:
            m = metadata.get(cui)
            entries.append(RetentionEntry(
                cui=cui,
                term=m.preferred_term if m else cui,
                alignment=round(alignments.get(cui, 0.0), 4),
                semantic_types=list(m.semantic_types) if m else [],
                explains_cuis=explains.get(cui, []),
            ))

        entries.sort(key=lambda e: e.alignment, reverse=True)
        sorted_cuis = [e.cui for e in entries]

        return sorted_cuis, entries


# %% [16] Pipeline
class CUIReductionSystem:

    def __init__(self, project_id, dataset_id, full_network,
                 allowed_sabs, embedding_table,
                 ic_scores=None, genai_location="us-central1",
                 embedding_cache_path="embedding_cache.npz"):

        self.bq = bigquery.Client(project=project_id)
        self.fetcher = MetadataFetcher(self.bq, project_id, dataset_id)
        self.ic_computer = ICComputer(full_network, ic_scores)
        self.sab_filter = SABFilter(allowed_sabs)
        self.hierarchy_reducer = HierarchyReducer(full_network)
        self.emb_fetcher = EmbeddingFetcher(
            self.bq, project_id, dataset_id, embedding_table,
            cache_path=embedding_cache_path)
        self.retention_scorer = RetentionScorer(project_id, genai_location)

        log(f"[Init] graph={full_network.number_of_nodes()} nodes, sabs={allowed_sabs}")

    @timed("full_pipeline")
    def reduce(self, cuis, context_string):
        """Reduce a single text. For multiple texts, use reduce_batch."""
        results = self.reduce_batch([(cuis, context_string)])
        return results[0]

    @timed("batch_pipeline")
    def reduce_batch(self, inputs):
        """Reduce multiple texts efficiently.

        Args:
            inputs: list of (cuis, context_string) tuples

        Batches expensive BQ calls (metadata, embeddings) across
        all texts, then runs per-text filtering and reduction.
        """

        t0 = time.perf_counter()

        # collect all unique CUIs across all texts
        all_cuis = set()
        for cuis, _ in inputs:
            all_cuis.update(cuis)
        log(f"[Batch] {len(inputs)} texts, {len(all_cuis)} unique CUIs")

        # ONE metadata fetch for all CUIs
        metadata = self.fetcher.fetch(list(all_cuis))
        log(f"[Batch] Metadata: {len(metadata)}/{len(all_cuis)} matched")

        # ONE IC computation for all CUIs
        ic_map = self.ic_computer.compute(list(all_cuis), metadata)
        log(f"[Batch] IC Scores: {len(ic_map)} computed")

        # per-text: cheap in-memory filtering + hierarchy
        per_text = []
        all_post_hier = set()

        for cuis, context_string in inputs:
            if not cuis:
                per_text.append((context_string, [], [], set(), [], 8.0, None))
                continue

            label = context_string[:50]
            log(f"\n  [{label}]")

            after_sab, sab_audit = self.sab_filter.run(cuis, metadata)
            log(f"    SAB Filter: {len(cuis)} -> {len(after_sab)} (removed {len(cuis) - len(after_sab)})")

            # compute sibling IC threshold (1-hop parents only, fast on full graph)
            sib_threshold = compute_sibling_threshold(
                self.hierarchy_reducer.graph, after_sab, ic_map)

            # build IC-bounded subgraph — same threshold controls the boundary
            subgraph = build_subgraph(
                self.hierarchy_reducer.graph, after_sab, ic_map, ic_floor=sib_threshold)

            after_hier, hier_audit, hier_edges = self.hierarchy_reducer.run(
                after_sab, ic_map, metadata, sibling_threshold=sib_threshold,
                graph=subgraph)
            log(f"    Hierarchy: {len(after_sab)} -> {len(after_hier)} (removed {len(after_sab) - len(after_hier)})")

            audit = sab_audit + hier_audit
            all_post_hier.update(after_hier)
            per_text.append((context_string, cuis, after_hier, hier_edges, audit, sib_threshold, subgraph))

        # ONE embedding fetch for all post-hierarchy CUIs
        log(f"\n[Batch] Fetching embeddings for {len(all_post_hier)} unique post-hierarchy CUIs")
        all_embeddings = self.emb_fetcher.fetch(list(all_post_hier))
        log(f"[Batch] Embeddings: {len(all_embeddings)}/{len(all_post_hier)} matched")

        # per-text: embedding reduction + topics + retention
        results = []
        for context_string, cuis, after_hier, hier_edges, all_audit, sib_threshold, subgraph in per_text:
            t_start = time.perf_counter()
            label = context_string[:50]

            if not after_hier:
                results.append(self._empty(context_string, t_start, all_audit))
                continue

            try:
                embeddings = {c: all_embeddings[c] for c in after_hier if c in all_embeddings}
                emb_cov = len(embeddings) / len(after_hier) if after_hier else 0

                # precompute graph info on subgraph (not full graph)
                batch_info = BatchGraphInfo(
                    subgraph, after_hier, ic_map,
                    sibling_threshold=sib_threshold)

                topics, surviving, emb_audit, avg_sil = EmbeddingReducer.run(
                    after_hier, metadata, embeddings, ic_map,
                    graph=subgraph,
                    sibling_threshold=sib_threshold,
                    batch_info=batch_info)
                all_audit.extend(emb_audit)
                log(f"\n  [{label}]")
                log(f"    Embedding Reduction: {len(after_hier)} -> {len(surviving)} (removed {len(after_hier) - len(surviving)})")
                if avg_sil != 0.0:
                    log(f"    Avg Silhouette: {avg_sil:.3f}")

                topics_out = {}
                for topic_id, t_cuis in topics.items():
                    if not t_cuis:
                        continue
                    topic_types = set()
                    cui_terms = {}
                    for c in t_cuis:
                        m = metadata.get(c)
                        if m and m.semantic_types:
                            topic_types.update(m.semantic_types)
                        cui_terms[c] = m.preferred_term if m else c
                    topics_out[topic_id] = TopicInfo(
                        topic_id=topic_id,
                        semantic_types=topic_types or {"Unknown"},
                        cuis=t_cuis,
                        cui_terms=cui_terms)

                log(f"    Topics: {len(surviving)} CUIs -> {len(topics_out)} topics")

                # embed input text once — used for cross-topic dedup AND retention
                text_vec = self.retention_scorer.embed_text(context_string)

                # cross-topic dedup (topics stay intact, only output list trimmed)
                before_xtd = len(surviving)
                surviving, xtd_audit = CrossTopicDedup.run(
                    surviving, embeddings, text_vec, metadata, ic_map,
                    sibling_threshold=sib_threshold, batch_info=batch_info)
                all_audit.extend(xtd_audit)
                if xtd_audit:
                    log(f"    Cross-Topic Dedup: {before_xtd} -> {len(surviving)} (removed {before_xtd - len(surviving)})")

                # coverage rescue: restore CUIs from lost semantic axes
                surviving, rescue_audit = coverage_rescue(
                    surviving, cuis, metadata, ic_map, embeddings,
                    text_vec=text_vec)
                all_audit.extend(rescue_audit)

                # retention scoring (alignment for downstream use)
                sorted_cuis, retention = self.retention_scorer.score(
                    text_vec, surviving, embeddings, metadata, all_audit)
                if retention:
                    log(f"    Retention: scored {len(retention)} (top={retention[0].alignment:.3f}, bottom={retention[-1].alignment:.3f})")

                # prune topics to match surviving CUIs (after cross-topic dedup)
                final_set = set(sorted_cuis)
                before_prune = len(topics_out)
                pruned_topics = {}
                for topic_id, topic in topics_out.items():
                    kept_cuis = [c for c in topic.cuis if c in final_set]
                    if not kept_cuis:
                        continue
                    kept_terms = {c: topic.cui_terms[c] for c in kept_cuis if c in topic.cui_terms}
                    pruned_topics[topic_id] = TopicInfo(
                        topic_id=topic_id,
                        semantic_types=topic.semantic_types,
                        cuis=kept_cuis,
                        cui_terms=kept_terms)
                topics_out = pruned_topics
                if before_prune != len(topics_out):
                    log(f"    Topics after pruning: {len(topics_out)} (dropped {before_prune - len(topics_out)} empty)")

                audited = {e.removed_cui for e in all_audit}
                survived = set(surviving)
                unaccounted = set(cuis) - survived - audited
                if unaccounted:
                    for cui in unaccounted:
                        m = metadata.get(cui)
                        all_audit.append(AuditEntry(
                            cui, m.preferred_term if m else "",
                            "", "", "unaccounted",
                            "CUI not in surviving or audit trail"))

                # --- Quality metrics ---
                # Semantic loss: what fraction of input semantic types are NOT
                # covered by any surviving CUI?
                input_stys = set()
                for cui in cuis:
                    m = metadata.get(cui)
                    if m and m.semantic_types:
                        input_stys.update(m.semantic_types)
                output_stys = set()
                for cui in surviving:
                    m = metadata.get(cui)
                    if m and m.semantic_types:
                        output_stys.update(m.semantic_types)
                sem_loss = 1.0 - (len(output_stys) / len(input_stys)) if input_stys else 0.0

                # Confidence score (0-1): composite of cluster quality,
                # reduction ratio sanity, and embedding coverage
                # - silhouette: cluster quality (0 = random, 1 = perfect)
                # - ratio penalty: if we removed >95% or <30%, something may be off
                # - embedding coverage: how many CUIs had embeddings
                ratio = len(surviving) / len(cuis) if cuis else 0
                ratio_score = 1.0 - abs(ratio - 0.15) / 0.85  # peaks at ~15% retention
                ratio_score = max(0.0, min(1.0, ratio_score))
                sil_component = (avg_sil + 1.0) / 2.0  # normalize -1..1 → 0..1
                confidence = (
                    0.40 * sil_component +
                    0.30 * emb_cov +
                    0.20 * (1.0 - sem_loss) +
                    0.10 * ratio_score
                )

                log(f"    Quality: silhouette={avg_sil:.3f}, confidence={confidence:.3f}, "
                    f"semantic_loss={sem_loss:.3f}")

                elapsed = (time.perf_counter() - t_start) * 1000
                log(f"    DONE: {len(cuis)} -> {len(surviving)} CUIs, {len(topics_out)} topics ({elapsed:.0f}ms)")

                results.append(ReductionResult(
                    context_string=context_string,
                    input_count=len(cuis),
                    after_filter_count=len(after_hier),
                    after_hierarchy_count=len(after_hier),
                    after_redundancy_count=len(surviving),
                    topics=topics_out,
                    all_reduced_cuis=sorted_cuis,
                    retention=retention,
                    audit_trail=all_audit,
                    processing_time_ms=elapsed,
                    metadata_coverage=emb_cov,
                    embedding_coverage=emb_cov,
                    silhouette_score=avg_sil,
                    confidence_score=confidence,
                    semantic_loss=sem_loss))

            except Exception as e:
                log(f"    FAILED [{label}]: {e}", "ERROR")
                results.append(self._empty(context_string, t_start, all_audit))

        self.emb_fetcher.shutdown()
        total = (time.perf_counter() - t0) * 1000
        log(f"\n[Batch] ALL DONE: {len(inputs)} texts in {total:.0f}ms")

        return results

    def _empty(self, ctx, t0, audit=None, meta_cov=0.0):
        return ReductionResult(
            ctx, 0, 0, 0, 0, {}, [], [],
            audit or [], (time.perf_counter() - t0) * 1000, meta_cov, 0.0)

    def get_stats(self):
        with _timing_lock:
            out = {}
            for k, v in _timings.items():
                if v:
                    vals = list(v)
                    out[k] = {
                        "count": len(vals),
                        "mean_ms": float(np.mean(vals)),
                        "p99_ms": float(np.percentile(vals, 99)),
                    }
            return out


# %% [17] CUI Extraction API
class CUIExtractor:
    """Calls CUI extraction endpoint. Filters by confidence."""

    def __init__(self, api_url, min_confidence=0.0, top_k=3):
        self.url = api_url
        self.min_confidence = min_confidence
        self.top_k = top_k
        self.session = requests.Session()

        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text):
        resp = self.session.post(
            self.url, headers=self.headers,
            json={"query_texts": [text], "top_k": self.top_k},
            timeout=200)
        resp.raise_for_status()
        data = resp.json()

        cuis = []
        if isinstance(data, dict):
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        raw_score = item.get("score", item.get("confidence", 1.0))
                        try:
                            score = float(raw_score)
                        except (ValueError, TypeError):
                            score = 0.0
                        if score >= self.min_confidence:
                            cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
        return list(set(cuis))


# %% [18] Load Pickles -- RUN ONCE
HIERARCHY_PATH = "umls_hierarchy.pkl"
IC_PATH = "ic_precomputed.pkl"

log("Loading hierarchy...")
with open(HIERARCHY_PATH, "rb") as f:
    hierarchy = pickle.load(f)
log(f"Hierarchy: {hierarchy.number_of_nodes()} nodes, {hierarchy.number_of_edges()} edges")

log("Loading IC scores...")
with open(IC_PATH, "rb") as f:
    ic_precomputed = pickle.load(f)
log(f"IC scores: {len(ic_precomputed)} entries")


# %% [19] Init System
PROJECT_ID = "your-project-id"
DATASET_ID = "your-dataset"
API_URL = "https://your-api/extract"
EMBEDDING_TABLE = "cui_embeddings"
ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM", "SNOMEDCT_US", "LNC"]

system = CUIReductionSystem(
    PROJECT_ID, DATASET_ID,
    hierarchy,
    ALLOWED_SABS,
    EMBEDDING_TABLE,
    ic_scores=ic_precomputed,
)
extractor = CUIExtractor(API_URL)
log("System ready.")


# %% [20] Run Pipeline
texts = [
    "Patient has severe pain in left knee with swelling",
]

# extract CUIs in parallel
def _extract(text):
    cuis = extractor.extract(text)
    return (cuis, text) if cuis else None

with ThreadPoolExecutor(max_workers=min(len(texts), 8)) as pool:
    raw = pool.map(_extract, texts)
    inputs = [r for r in raw if r is not None]

# single batch call
results = system.reduce_batch(inputs)

# display results
for result in results:
    log(f"\n{'=' * 60}")
    log(f"TEXT: {result.context_string}")
    log(f"SUMMARY: {result.input_count} -> {result.after_redundancy_count} CUIs, "
        f"{len(result.topics)} topics ({result.processing_time_ms:.0f}ms)")
    log(f"  Quality: silhouette={result.silhouette_score:.3f}, "
        f"confidence={result.confidence_score:.3f}, "
        f"semantic_loss={result.semantic_loss:.3f}")

    log(f"\n-- Top 20 CUIs by retention (of {len(result.retention)}) --")
    for entry in result.retention[:20]:
        stys = ", ".join(entry.semantic_types[:2]) if entry.semantic_types else "?"
        line = f"  {entry.cui} | align={entry.alignment} | [{stys}] {entry.term}"
        if entry.explains_cuis:
            line += f" | explains {len(entry.explains_cuis)}"
        log(line)

    log(f"\n-- Topics: {len(result.topics)} (showing first 20) --")
    for tid, topic in list(result.topics.items())[:20]:
        log(f"  {tid} ({len(topic.cuis)} CUIs)")
    if len(result.topics) > 20:
        log(f"  ... {len(result.topics) - 20} more topics")
