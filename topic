"""
CUI Reduction System

Reduces extracted CUIs through filtering, hierarchy pruning,
embedding-based redundancy removal, and retention scoring.
Groups results into topics sorted by narrative alignment.

pip install google-cloud-bigquery networkx numpy scipy requests scann google-genai
"""

# %% [1] Imports
import re
import time
import logging
import threading
import unicodedata
import subprocess
import pickle
from typing import List, Dict, Optional, Set, Tuple, FrozenSet
from dataclasses import dataclass, field
from collections import defaultdict, deque, OrderedDict
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import networkx as nx
import requests
import scann
from google.cloud import bigquery
from google import genai
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform


# %% [2] Logging and Timing
logger = logging.getLogger("cui_reduction")
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter(
        "%(asctime)s %(message)s", datefmt="%H:%M:%S"))
    logger.addHandler(_handler)


def log(msg, level="INFO"):
    getattr(logger, level.lower(), logger.info)(msg)


_timings = defaultdict(list)
_timing_lock = threading.Lock()


def timed(name):
    def dec(fn):
        def wrapper(*a, **kw):
            t0 = time.perf_counter()
            try:
                return fn(*a, **kw)
            finally:
                with _timing_lock:
                    _timings[name].append(
                        (time.perf_counter() - t0) * 1000)
        return wrapper
    return dec


# %% [3] Data Models
@dataclass
class CUIMetadata:
    cui: str
    preferred_term: str
    normalized_term: str
    all_tokens: FrozenSet[str]  # tokens from preferred terms per allowed SAB
    semantic_types: List[str]
    semantic_type_ids: List[str]
    ic_score: float
    source_vocabs: List[str]


@dataclass
class TopicInfo:
    topic_id: str
    semantic_types: Set[str]
    cuis: List[str]
    cui_terms: Dict[str, str]


@dataclass
class AuditEntry:
    removed_cui: str
    removed_term: str
    kept_cui: str
    kept_term: str
    reason: str
    detail: str


@dataclass
class RetentionEntry:
    cui: str
    term: str
    alignment: float
    semantic_types: List[str]
    explains_cuis: List[str]


@dataclass
class ReductionResult:
    context_string: str
    input_count: int
    after_filter_count: int
    after_hierarchy_count: int
    after_redundancy_count: int
    after_relevance_count: int
    topics: Dict[str, TopicInfo]
    all_reduced_cuis: List[str]
    retention: List[RetentionEntry]
    audit_trail: List[AuditEntry]
    processing_time_ms: float
    metadata_coverage: float
    embedding_coverage: float


# %% [4] Text Utilities
def normalize_term(term):
    t = unicodedata.normalize("NFKD", term).lower()
    t = re.sub(r"[^a-z0-9\s]", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def tokenize_term(normalized):
    if not normalized:
        return frozenset()
    return frozenset(normalized.split())


def terms_diverge(tokens_a, tokens_b):
    """Block collapse only when terms share NO tokens at all."""
    if not tokens_a or not tokens_b:
        return False
    return len(tokens_a & tokens_b) == 0


def qualifier_conflict(tokens_a, tokens_b):
    """Block collapse when terms are nearly identical but differ by 1-2 tokens.

    If 80%+ of tokens overlap but a small set differs, those differing
    tokens are likely clinical qualifiers (left/right, upper/lower,
    front/back, acute/chronic, mild/severe, etc).

    No hardcoded list — works for any qualifier pattern:
      "left knee pain" vs "right knee pain" → 2/3 overlap, differ by left/right → block
      "upper abdomen pain" vs "lower abdomen pain" → same logic
      "acute sinusitis" vs "chronic sinusitis" → same logic
      "knee pain" vs "elbow pain" → same logic (different body part)
    """
    if not tokens_a or not tokens_b:
        return False

    shared = tokens_a & tokens_b
    only_a = tokens_a - tokens_b
    only_b = tokens_b - tokens_a

    # no difference — identical terms, not a conflict
    if not only_a and not only_b:
        return False

    # high overlap with small difference = qualifier distinction
    total = len(tokens_a | tokens_b)
    overlap_ratio = len(shared) / total if total > 0 else 0

    # 50%+ overlap and BOTH sides differ by 1-2 tokens = qualifier distinction
    # requires both sides to have unique tokens — "knee pain" vs "knee pain syndrome"
    # has only_a=0, so it's not a qualifier swap, just extra detail
    if overlap_ratio >= 0.5 and 1 <= len(only_a) <= 2 and 1 <= len(only_b) <= 2:
        return True

    return False


# %% [5] LRU Cache (thread-safe)
class Cache:
    def __init__(self, max_size):
        self._max = max_size
        self._d = OrderedDict()
        self._lock = threading.Lock()

    def get(self, key):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
                return self._d[key]
        return None

    def put(self, key, val):
        with self._lock:
            if key in self._d:
                self._d.move_to_end(key)
            self._d[key] = val
            while len(self._d) > self._max:
                self._d.popitem(last=False)


# %% [6] Metadata from BigQuery
_BQ_BATCH = 5000


class MetadataFetcher:
    """Pulls CUI metadata from MRCONSO/MRSTY.
    Picks preferred term: ISPREF=Y first, then longest STR."""

    def __init__(self, bq, pid, did):
        self.bq = bq
        self.pid = pid
        self.did = did
        self._cache = Cache(50_000)

    @timed("metadata_fetch")
    def fetch(self, cuis):
        result = {}
        missing = []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        for i in range(0, len(missing), _BQ_BATCH):
            batch = missing[i:i + _BQ_BATCH]
            query = f"""
            WITH ranked AS (
              SELECT CUI, STR, SAB, ISPREF,
                ROW_NUMBER() OVER (
                  PARTITION BY CUI
                  ORDER BY
                    CASE SAB
                      WHEN 'SNOMEDCT_US' THEN 0
                      WHEN 'ICD10CM' THEN 1
                      WHEN 'ICD10PCS' THEN 2
                      WHEN 'ICD9CM' THEN 3
                      WHEN 'LNC' THEN 4
                      ELSE 5
                    END,
                    CASE WHEN ISPREF='Y' THEN 0 ELSE 1 END,
                    LENGTH(STR) DESC
                ) AS rn
              FROM `{self.pid}.{self.did}.MRCONSO`
              WHERE CUI IN UNNEST(@cuis) AND LAT = 'ENG'
            ),
            sab_pref AS (
              SELECT CUI, SAB, STR,
                ROW_NUMBER() OVER (
                  PARTITION BY CUI, SAB
                  ORDER BY
                    CASE WHEN ISPREF='Y' THEN 0 ELSE 1 END,
                    LENGTH(STR) DESC
                ) AS sab_rn
              FROM `{self.pid}.{self.did}.MRCONSO`
              WHERE CUI IN UNNEST(@cuis) AND LAT = 'ENG'
                AND SAB IN ('SNOMEDCT_US','ICD10CM','ICD10PCS','ICD9CM','LNC')
            )
            SELECT
              r.CUI AS cui,
              MAX(CASE WHEN r.rn = 1 THEN r.STR END) AS pref_term,
              ARRAY_AGG(DISTINCT s.TUI IGNORE NULLS) AS tuis,
              ARRAY_AGG(DISTINCT s.STY IGNORE NULLS) AS stys,
              ARRAY_AGG(DISTINCT c2.SAB IGNORE NULLS) AS sabs,
              ARRAY_AGG(DISTINCT sp.STR IGNORE NULLS) AS sab_terms
            FROM ranked r
            LEFT JOIN `{self.pid}.{self.did}.MRSTY` s
              ON r.CUI = s.CUI
            LEFT JOIN `{self.pid}.{self.did}.MRCONSO` c2
              ON r.CUI = c2.CUI AND c2.LAT = 'ENG'
            LEFT JOIN sab_pref sp
              ON r.CUI = sp.CUI AND sp.sab_rn = 1
            WHERE r.rn = 1
            GROUP BY r.CUI
            """
            jc = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", batch)
            ])
            for row in self.bq.query(
                    query, job_config=jc, timeout=300).result():
                term = row.pref_term or row.cui
                norm = normalize_term(term)
                # tokens from preferred term per allowed SAB
                sab_tok = set()
                for t in (row.sab_terms or []):
                    sab_tok.update(tokenize_term(normalize_term(t)))
                # include the preferred term tokens too
                pref_tok = tokenize_term(norm)
                combined = frozenset(sab_tok | pref_tok) if sab_tok else pref_tok
                meta = CUIMetadata(
                    cui=row.cui,
                    preferred_term=term,
                    normalized_term=norm,
                    all_tokens=combined,
                    semantic_types=list(row.stys or []),
                    semantic_type_ids=list(row.tuis or []),
                    ic_score=0.0,
                    source_vocabs=list(row.sabs or []),
                )
                self._cache.put(row.cui, meta)
                result[row.cui] = meta

        return result


# %% [7] IC Scores
class ICComputer:
    """IC = -log((descendants + 1) / total).
    Use precomputed dict in production to avoid BFS."""

    def __init__(self, graph, precomputed=None):
        self.graph = graph
        self.precomputed = precomputed or {}
        self._cache = Cache(100_000)
        self._total = max(graph.number_of_nodes(), 1)

    @timed("ic_compute")
    def compute(self, cuis, metadata):
        result = {}
        for cui in cuis:
            if cui in self.precomputed:
                ic = self.precomputed[cui]
            else:
                cached = self._cache.get(cui)
                if cached is not None:
                    ic = cached
                else:
                    desc = self._count_descendants(cui)
                    ic = -np.log((desc + 1) / self._total)
                    self._cache.put(cui, ic)
            result[cui] = ic
            m = metadata.get(cui)
            if m:
                m.ic_score = ic
        return result

    def _count_descendants(self, cui):
        if not self.graph.has_node(cui):
            return 0
        visited = set()
        q = deque([cui])
        while q:
            n = q.popleft()
            for child in self.graph.successors(n):
                if child not in visited:
                    visited.add(child)
                    q.append(child)
        return len(visited)


# %% [8] Filters
class SABFilter:
    def __init__(self, allowed):
        self.allowed = set(allowed)

    @timed("sab_filter")
    def run(self, cuis, metadata):
        kept, audit = [], []
        for cui in cuis:
            m = metadata.get(cui)
            if not m:
                audit.append(AuditEntry(
                    cui, "", "", "", "no_metadata",
                    "CUI not found in BQ"))
                continue
            if not any(s in self.allowed for s in m.source_vocabs):
                audit.append(AuditEntry(
                    cui, m.preferred_term, "", "",
                    "sab_filtered",
                    f"vocabs {m.source_vocabs} not in allowed"))
                continue
            kept.append(cui)
        return kept, audit


class SemanticTypeFilter:
    def __init__(self, excluded):
        self.excluded = set(excluded) if excluded else set()

    @timed("semtype_filter")
    def run(self, cuis, metadata):
        if not self.excluded:
            return cuis, []
        kept, audit = [], []
        for cui in cuis:
            m = metadata.get(cui)
            if not m or not m.semantic_types:
                kept.append(cui)
                continue
            if all(st in self.excluded for st in m.semantic_types):
                audit.append(AuditEntry(
                    cui, m.preferred_term, "", "",
                    "semtype_excluded",
                    f"all types {m.semantic_types} excluded"))
                continue
            kept.append(cui)
        return kept, audit


class TermDeduplicator:
    """Collapse CUIs with identical normalized terms. Keep highest IC."""

    @staticmethod
    @timed("term_dedup")
    def run(cuis, metadata, ic_map):
        groups = defaultdict(list)
        for cui in cuis:
            m = metadata.get(cui)
            key = m.normalized_term if m else cui
            groups[key].append(cui)

        kept, audit = [], []
        for term, members in groups.items():
            best = max(members, key=lambda c: ic_map.get(c, 0))
            kept.append(best)
            for c in members:
                if c != best:
                    bm = metadata.get(best)
                    cm = metadata.get(c)
                    audit.append(AuditEntry(
                        c, cm.preferred_term if cm else "",
                        best, bm.preferred_term if bm else "",
                        "term_dedup",
                        f"identical norm term '{term}'"))
        return kept, audit


# %% [9] Hierarchy Reduction
class HierarchyReducer:
    """Remove ancestor CUIs when a more specific child exists."""

    def __init__(self, graph):
        self.graph = graph

    @timed("hierarchy_reduce")
    def run(self, cuis, ic_map, metadata):
        cui_set = set(cuis)
        edges = set()
        parent_children = defaultdict(set)

        for cui in cuis:
            if not self.graph.has_node(cui):
                continue
            for parent in self.graph.predecessors(cui):
                if parent in cui_set and parent != cui:
                    parent_children[parent].add(cui)
                    edges.add((parent, cui))

        to_remove = {}
        for parent, children in parent_children.items():
            p_ic = ic_map.get(parent, 0)
            p_meta = metadata.get(parent)
            p_types = set(p_meta.semantic_types) if p_meta and p_meta.semantic_types else set()

            for child in children:
                c_ic = ic_map.get(child, 0)
                if c_ic < p_ic:
                    continue
                c_meta = metadata.get(child)
                c_types = set(c_meta.semantic_types) if c_meta and c_meta.semantic_types else set()
                if p_types and c_types and not (p_types & c_types):
                    continue
                if parent not in to_remove or c_ic > to_remove[parent][1]:
                    to_remove[parent] = (child, c_ic, p_ic)

        remove_set = set(to_remove.keys())
        surviving = [c for c in cuis if c not in remove_set]

        audit = []
        for parent, (child, c_ic, p_ic) in to_remove.items():
            pm = metadata.get(parent)
            cm = metadata.get(child)
            audit.append(AuditEntry(
                parent, pm.preferred_term if pm else "",
                child, cm.preferred_term if cm else "",
                "ancestor_subsumed",
                f"child IC {c_ic:.2f} >= parent IC {p_ic:.2f}"))

        return surviving, audit, edges


# %% [10] Embedding Fetch (parallel BQ batches)
class EmbeddingFetcher:
    def __init__(self, bq, pid, did, embedding_table):
        self.bq = bq
        self.pid = pid
        self.did = did
        self.table = embedding_table
        self._cache = Cache(50_000)
        self._pool = ThreadPoolExecutor(max_workers=4)

    def _fetch_batch(self, batch):
        """Fetch one batch from BQ. Called in parallel."""
        rows = []
        query = f"""
        SELECT cui, embedding
        FROM `{self.pid}.{self.did}.{self.table}`
        WHERE cui IN UNNEST(@cuis)
        """
        jc = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ArrayQueryParameter("cuis", "STRING", batch)
        ])
        for row in self.bq.query(
                query, job_config=jc, timeout=300).result():
            raw = row.embedding
            if raw is None or len(raw) == 0:
                continue
            vec = np.array(raw, dtype=np.float32)
            if not np.all(np.isfinite(vec)):
                continue
            norm = np.linalg.norm(vec)
            if norm == 0:
                continue
            vec = vec / norm
            rows.append((row.cui, vec))
        return rows

    @timed("embedding_fetch")
    def fetch(self, cuis):
        result, missing = {}, []
        for c in cuis:
            cached = self._cache.get(c)
            if cached is not None:
                result[c] = cached
            else:
                missing.append(c)

        if not missing:
            return result

        batches = [missing[i:i + _BQ_BATCH]
                    for i in range(0, len(missing), _BQ_BATCH)]

        futures = [self._pool.submit(self._fetch_batch, b) for b in batches]
        for f in futures:
            for cui, vec in f.result():
                self._cache.put(cui, vec)
                result[cui] = vec

        return result


# %% [11] ScaNN Topic Builder
class TopicBuilder:
    """Clusters CUIs into topics via ScaNN KNN graph + connected components."""

    @staticmethod
    @timed("scann_topics")
    def build(cuis, embeddings, k_neighbors=20):
        # deterministic: sort input + reset numpy RNG
        cuis = sorted(cuis)
        np.random.seed(42)

        matrix = np.array([embeddings[c] for c in cuis], dtype=np.float32)
        n = len(cuis)
        dim = matrix.shape[1] if matrix.ndim == 2 else 0

        if n < 2 or dim == 0:
            return {i: [c] for i, c in enumerate(cuis)}

        k = min(k_neighbors, n - 1)
        k = max(k, 1)

        num_leaves = min(max(2, int(np.sqrt(n))), n)
        builder = scann.scann_ops_pybind.builder(matrix, k, "dot_product")

        if n >= 16 and dim >= 2:
            dims_per_block = 2 if dim % 2 == 0 else 1
            builder = (
                builder
                .tree(
                    num_leaves=num_leaves,
                    num_leaves_to_search=max(1, num_leaves // 5),
                    training_sample_size=min(n, 250_000))
                .score_ah(dims_per_block,
                          anisotropic_quantization_threshold=0.2)
                .reorder(min(k * 4, n))
            )
        else:
            builder = builder.score_brute_force()

        searcher = builder.build()
        neighbors, scores = searcher.search_batched(matrix)

        kth_dists = []
        for i in range(n):
            valid = scores[i][scores[i] > -1e30]
            if len(valid) > 0:
                kth_dists.append(1.0 - float(valid[-1]))

        if kth_dists:
            threshold = float(np.mean(kth_dists))
        else:
            all_sims = matrix @ matrix.T
            np.fill_diagonal(all_sims, -np.inf)
            threshold = float(1.0 - np.mean(all_sims[all_sims > -np.inf]))

        adj = defaultdict(set)
        for i in range(n):
            for j_pos in range(len(neighbors[i])):
                j = int(neighbors[i][j_pos])
                if j < 0 or j >= n or j == i:
                    continue
                if (1.0 - float(scores[i][j_pos])) < threshold:
                    adj[i].add(j)
                    adj[j].add(i)

        visited = set()
        groups = {}
        label = 0
        for i in range(n):
            if i in visited:
                continue
            component = []
            q = deque([i])
            visited.add(i)
            while q:
                node = q.popleft()
                component.append(cuis[node])
                for nb in adj.get(node, set()):
                    if nb not in visited:
                        visited.add(nb)
                        q.append(nb)
            groups[label] = component
            label += 1

        return groups


# %% [12] Embedding Reduction
class EmbeddingReducer:
    """Groups by semantic type, runs ScaNN topics per group,
    then dense pairwise redundancy removal within each topic."""

    @staticmethod
    @timed("embedding_reduce")
    def run(cuis, metadata, embeddings, ic_map, hierarchy_edges):

        type_groups = defaultdict(list)
        for cui in cuis:
            meta = metadata.get(cui)
            if meta and meta.semantic_types:
                for sty in meta.semantic_types:
                    type_groups[sty].append(cui)
            else:
                type_groups["Unknown"].append(cui)

        all_topics = {}
        globally_surviving = set()
        removal_candidates = {}
        tid = 0

        for sty in sorted(type_groups):
            group_cuis = sorted(set(type_groups[sty]))
            with_emb = [c for c in group_cuis if c in embeddings]
            without_emb = [c for c in group_cuis if c not in embeddings]

            for c in without_emb:
                globally_surviving.add(c)

            if len(with_emb) <= 1:
                all_topics[f"topic_{tid}"] = group_cuis
                for c in group_cuis:
                    globally_surviving.add(c)
                tid += 1
                continue

            scann_topics = TopicBuilder.build(with_emb, embeddings)

            for t_cuis in scann_topics.values():
                topic_id = f"topic_{tid}"
                tid += 1

                if len(t_cuis) <= 1:
                    all_topics[topic_id] = t_cuis
                    for c in t_cuis:
                        globally_surviving.add(c)
                    continue

                surviving, removed = _reduce_topic(
                    t_cuis, embeddings, metadata, ic_map, hierarchy_edges)
                all_topics[topic_id] = surviving
                for c in surviving:
                    globally_surviving.add(c)
                for entry in removed:
                    removal_candidates[entry.removed_cui] = entry

            for cui in without_emb:
                all_topics[f"topic_{tid}"] = [cui]
                tid += 1

        final_audit = []
        for cui, entry in removal_candidates.items():
            if cui not in globally_surviving:
                final_audit.append(entry)

        seen = set()
        surviving = []
        for t_cuis in all_topics.values():
            for c in t_cuis:
                if c in globally_surviving and c not in seen:
                    seen.add(c)
                    surviving.append(c)

        cleaned = {}
        for topic_id, t_cuis in all_topics.items():
            members = []
            seen_t = set()
            for c in t_cuis:
                if c in globally_surviving and c not in seen_t:
                    seen_t.add(c)
                    members.append(c)
            if members:
                cleaned[topic_id] = members

        return cleaned, surviving, final_audit


def _reduce_topic(t_cuis, embeddings, metadata, ic_map, hierarchy_edges):
    matrix = np.array([embeddings[c] for c in t_cuis])
    sim = matrix @ matrix.T
    np.clip(sim, -1, 1, out=sim)
    dist = 1.0 - sim
    np.fill_diagonal(dist, 0)
    dist = np.maximum(dist, 0)

    if np.any(~np.isfinite(dist)):
        dist = np.nan_to_num(dist, nan=1.0, posinf=1.0, neginf=0.0)

    n = len(t_cuis)
    idx = {c: i for i, c in enumerate(t_cuis)}
    upper = dist[np.triu_indices(n, k=1)]

    if len(upper) == 0 or np.std(upper) < 1e-9:
        return t_cuis, []

    guardrail = float(np.median(upper))
    tight_t = float(np.mean(upper) - np.std(upper))
    nonzero = upper[upper > 0]
    floor = float(np.min(nonzero)) if len(nonzero) > 0 else float(np.finfo(np.float32).eps)
    tight_t = max(tight_t, floor)
    tight_t = min(tight_t, guardrail)

    condensed = squareform(dist, checks=False)
    Z = linkage(condensed, method="average")
    labels = fcluster(Z, t=tight_t, criterion="distance")

    r_groups = defaultdict(list)
    for cui, lbl in zip(t_cuis, labels):
        r_groups[lbl].append(cui)

    all_kept, all_removed = [], []
    for r_cuis in r_groups.values():
        kept, removed = _resolve_group(
            r_cuis, metadata, ic_map, hierarchy_edges, dist, idx, tight_t, guardrail)
        all_kept.extend(kept)
        all_removed.extend(removed)

    return all_kept, all_removed


def _resolve_group(r_cuis, metadata, ic_map, hierarchy_edges, dist, index_map, tight_t, guardrail):
    if len(r_cuis) <= 1:
        return r_cuis, []

    best = max(r_cuis, key=lambda c: ic_map.get(c, 0))
    b_meta = metadata.get(best)
    b_types = set(b_meta.semantic_types) if b_meta and b_meta.semantic_types else set()

    kept = [best]
    audit = []

    for c in r_cuis:
        if c == best:
            continue

        c_meta = metadata.get(c)
        c_types = set(c_meta.semantic_types) if c_meta and c_meta.semantic_types else set()

        if c_types and b_types and not (c_types & b_types):
            kept.append(c)
            continue

        if (c, best) in hierarchy_edges or (best, c) in hierarchy_edges:
            kept.append(c)
            continue

        if c_meta and b_meta and terms_diverge(c_meta.all_tokens, b_meta.all_tokens):
            kept.append(c)
            continue

        if c_meta and b_meta and qualifier_conflict(c_meta.all_tokens, b_meta.all_tokens):
            kept.append(c)
            continue

        ci = index_map.get(c)
        bi = index_map.get(best)
        pair_dist = dist[ci, bi] if ci is not None and bi is not None else 0.0

        audit.append(AuditEntry(
            removed_cui=c,
            removed_term=c_meta.preferred_term if c_meta else "",
            kept_cui=best,
            kept_term=b_meta.preferred_term if b_meta else "",
            reason="embedding_redundant",
            detail=f"dist {pair_dist:.3f} < tight {tight_t:.3f} | "
                   f"guard {guardrail:.3f} | "
                   f"IC {ic_map.get(best,0):.2f} vs {ic_map.get(c,0):.2f}"))

    return kept, audit


# %% [13] Cross-Topic Dedup
class CrossTopicDedup:
    """Removes near-duplicate CUIs that survived in different topics.

    Topics are NOT modified — they keep all members for grouping context.
    Only the final output list is trimmed.

    Uses text alignment as tiebreaker: keeps the CUI closer to the
    input narrative. Threshold is adaptive from pairwise distances.
    """

    @staticmethod
    @timed("cross_topic_dedup")
    def run(surviving, embeddings, text_vec, metadata, ic_map):
        with_emb = [c for c in surviving if c in embeddings]
        without_emb = [c for c in surviving if c not in embeddings]

        if len(with_emb) <= 1:
            return surviving, []

        matrix = np.array([embeddings[c] for c in with_emb], dtype=np.float32)
        idx_map = {c: i for i, c in enumerate(with_emb)}

        # text alignment for tiebreaking
        alignments = {}
        for c in with_emb:
            alignments[c] = float(np.dot(text_vec, embeddings[c]))

        # pairwise cosine distances
        sim = matrix @ matrix.T
        np.clip(sim, -1, 1, out=sim)
        dist = 1.0 - sim
        np.fill_diagonal(dist, 999.0)

        # adaptive threshold: very tight — only true near-duplicates
        upper = dist[np.triu_indices(len(with_emb), k=1)]
        upper = upper[upper < 999.0]
        if len(upper) == 0:
            return surviving, []

        # 5th percentile of distances = tightest pairs
        threshold = float(np.percentile(upper, 5))
        # floor: don't go above 0.05 cosine distance
        threshold = min(threshold, 0.05)

        # find pairs below threshold, remove the one with lower alignment
        removed = set()
        audit = []

        for i in range(len(with_emb)):
            if with_emb[i] in removed:
                continue
            for j in range(i + 1, len(with_emb)):
                if with_emb[j] in removed:
                    continue
                if dist[i, j] > threshold:
                    continue

                ci, cj = with_emb[i], with_emb[j]

                # safety guards — don't collapse qualifier variants or divergent terms
                ci_meta = metadata.get(ci)
                cj_meta = metadata.get(cj)
                ci_tok = ci_meta.all_tokens if ci_meta else frozenset()
                cj_tok = cj_meta.all_tokens if cj_meta else frozenset()
                if qualifier_conflict(ci_tok, cj_tok):
                    continue
                if terms_diverge(ci_tok, cj_tok):
                    continue

                # keep higher alignment, break ties by IC
                if (alignments.get(ci, 0), ic_map.get(ci, 0)) >= \
                   (alignments.get(cj, 0), ic_map.get(cj, 0)):
                    keep, drop = ci, cj
                else:
                    keep, drop = cj, ci

                removed.add(drop)
                km = metadata.get(keep)
                dm = metadata.get(drop)
                audit.append(AuditEntry(
                    removed_cui=drop,
                    removed_term=dm.preferred_term if dm else "",
                    kept_cui=keep,
                    kept_term=km.preferred_term if km else "",
                    reason="cross_topic_dedup",
                    detail=f"dist {dist[idx_map[keep], idx_map[drop]]:.4f} "
                           f"< threshold {threshold:.4f} | "
                           f"align {alignments.get(keep,0):.3f} vs {alignments.get(drop,0):.3f}"))

        final = [c for c in surviving if c not in removed] 
        return final, audit


# %% [14] Retention Scorer
class RetentionScorer:
    """Scores each CUI by narrative alignment via cosine(text_emb, cui_emb).
    Cost: 1 embedding API call (~100ms) per input text."""

    def __init__(self, project_id, location="us-central1"):
        self.client = genai.Client(
            vertexai=True, project=project_id, location=location)

    @timed("text_embed")
    def embed_text(self, text):
        """Embed input text. Returns normalized 3072-dim vector."""
        response = self.client.models.embed_content(
            model="gemini-embedding-001",
            contents=text,
        )
        text_vec = np.array(response.embeddings[0].values, dtype=np.float32)
        norm = np.linalg.norm(text_vec)
        if norm > 0:
            text_vec = text_vec / norm
        return text_vec

    @timed("retention_score")
    def score(self, text_vec, surviving, embeddings, metadata, audit_trail):

        alignments = {}
        for cui in surviving:
            if cui in embeddings:
                alignments[cui] = float(np.dot(text_vec, embeddings[cui]))
            else:
                alignments[cui] = 0.0

        explains = defaultdict(list)
        for entry in audit_trail:
            if entry.kept_cui and entry.removed_cui:
                explains[entry.kept_cui].append(entry.removed_cui)

        entries = []
        for cui in surviving:
            m = metadata.get(cui)
            entries.append(RetentionEntry(
                cui=cui,
                term=m.preferred_term if m else cui,
                alignment=round(alignments.get(cui, 0.0), 4),
                semantic_types=list(m.semantic_types) if m else [],
                explains_cuis=explains.get(cui, []),
            ))

        entries.sort(key=lambda e: e.alignment, reverse=True)
        sorted_cuis = [e.cui for e in entries]

        return sorted_cuis, entries


# %% [14B] Relevance Filter
class RelevanceFilter:
    """Post-retention cleanup using pure embedding geometry.

    No term dependency. Uses the top-K most aligned CUIs as an
    'anchor cluster' and measures each CUI's distance to that cluster.

    Steps:
    1. Top 10% by alignment = anchor cluster (what 'relevant' looks like)
    2. Compute centroid of anchor embeddings
    3. Each CUI: distance to centroid
    4. Otsu on centroid distances to find natural split

    'Patellar chondromalacia' survives because its embedding is near
    other knee CUIs — even though its term doesn't say 'knee'.
    """

    @staticmethod
    @timed("relevance_filter")
    def run(retention, context_string, metadata, embeddings=None):
        if len(retention) < 10 or embeddings is None:
            sorted_cuis = [e.cui for e in retention]
            return sorted_cuis, retention, []

        # Step 1: anchor cluster = top 10% by alignment
        anchor_n = max(10, int(len(retention) * 0.10))
        anchor_cuis = [e.cui for e in retention[:anchor_n]]

        # Step 2: compute centroid of anchor embeddings
        anchor_vecs = []
        for cui in anchor_cuis:
            if cui in embeddings:
                anchor_vecs.append(embeddings[cui])
        if not anchor_vecs:
            sorted_cuis = [e.cui for e in retention]
            return sorted_cuis, retention, []

        centroid = np.mean(anchor_vecs, axis=0).astype(np.float32)
        norm = np.linalg.norm(centroid)
        if norm > 0:
            centroid = centroid / norm

        # Step 3: each CUI's similarity to centroid
        scored = []
        for entry in retention:
            if entry.cui in embeddings:
                sim = float(np.dot(centroid, embeddings[entry.cui]))
            else:
                sim = 0.0
            scored.append((entry, sim))

        # sort by centroid similarity (highest = most relevant)
        scored.sort(key=lambda x: x[1], reverse=True)

        # Step 4: Otsu on centroid similarities
        sims = np.array([s for _, s in scored])
        otsu_cutoff = RelevanceFilter._otsu_threshold(sims)

        audit = []
        kept = []
        for entry, sim in scored:
            if sim >= otsu_cutoff:
                kept.append(entry)
            else:
                audit.append(AuditEntry(
                    removed_cui=entry.cui,
                    removed_term=entry.term,
                    kept_cui="", kept_term="",
                    reason="below_relevance_threshold",
                    detail=f"centroid_sim {sim:.4f} < otsu {otsu_cutoff:.4f}"))

        # re-sort kept by original alignment for output
        kept.sort(key=lambda e: e.alignment, reverse=True)
        sorted_cuis = [e.cui for e in kept]
        return sorted_cuis, kept, audit

    @staticmethod
    def _otsu_threshold(scores):
        lo, hi = float(scores.min()), float(scores.max())
        if hi - lo < 1e-9:
            return lo

        n_bins = 256
        bins = np.linspace(lo, hi, n_bins + 1)
        hist, _ = np.histogram(scores, bins=bins)
        hist = hist.astype(np.float64)
        total = hist.sum()

        if total == 0:
            return lo

        bin_centers = (bins[:-1] + bins[1:]) / 2.0
        best_t = lo
        best_var = -1.0
        sum_total = np.sum(hist * bin_centers)
        sum_bg = 0.0
        weight_bg = 0.0

        for i in range(n_bins):
            weight_bg += hist[i]
            if weight_bg == 0:
                continue
            weight_fg = total - weight_bg
            if weight_fg == 0:
                break
            sum_bg += hist[i] * bin_centers[i]
            mean_bg = sum_bg / weight_bg
            mean_fg = (sum_total - sum_bg) / weight_fg
            between_var = weight_bg * weight_fg * (mean_bg - mean_fg) ** 2
            if between_var > best_var:
                best_var = between_var
                best_t = bin_centers[i]

        return float(best_t)


# %% [15] Pipeline
class CUIReductionSystem:

    def __init__(self, project_id, dataset_id, full_network,
                 allowed_sabs, embedding_table,
                 excluded_semantic_types=None, ic_scores=None,
                 genai_location="us-central1"):

        self.bq = bigquery.Client(project=project_id)
        self.fetcher = MetadataFetcher(self.bq, project_id, dataset_id)
        self.ic_computer = ICComputer(full_network, ic_scores)
        self.sab_filter = SABFilter(allowed_sabs)
        self.semtype_filter = SemanticTypeFilter(excluded_semantic_types or set())
        self.hierarchy_reducer = HierarchyReducer(full_network)
        self.emb_fetcher = EmbeddingFetcher(
            self.bq, project_id, dataset_id, embedding_table)
        self.retention_scorer = RetentionScorer(project_id, genai_location)

        log(f"[Init] graph={full_network.number_of_nodes()} nodes, sabs={allowed_sabs}")

    @timed("full_pipeline")
    def reduce(self, cuis, context_string):
        """Reduce a single text. For multiple texts, use reduce_batch."""
        results = self.reduce_batch([(cuis, context_string)])
        return results[0]

    @timed("batch_pipeline")
    def reduce_batch(self, inputs):
        """Reduce multiple texts efficiently.

        Args:
            inputs: list of (cuis, context_string) tuples

        Batches expensive BQ calls (metadata, embeddings) across
        all texts, then runs per-text filtering and reduction.
        """

        t0 = time.perf_counter()

        # collect all unique CUIs across all texts
        all_cuis = set()
        for cuis, _ in inputs:
            all_cuis.update(cuis)
        log(f"[Batch] {len(inputs)} texts, {len(all_cuis)} unique CUIs")

        # ONE metadata fetch for all CUIs
        metadata = self.fetcher.fetch(list(all_cuis))
        log(f"[Batch] Metadata: {len(metadata)}/{len(all_cuis)} matched")

        # ONE IC computation for all CUIs
        ic_map = self.ic_computer.compute(list(all_cuis), metadata)
        log(f"[Batch] IC Scores: {len(ic_map)} computed")

        # per-text: cheap in-memory filtering + hierarchy
        per_text = []
        all_post_hier = set()

        for cuis, context_string in inputs:
            if not cuis:
                per_text.append((context_string, [], [], set(), []))
                continue

            label = context_string[:50]
            log(f"\n  [{label}]")

            after_sab, sab_audit = self.sab_filter.run(cuis, metadata)
            log(f"    SAB Filter: {len(cuis)} -> {len(after_sab)} (removed {len(cuis) - len(after_sab)})")

            after_sty, sty_audit = self.semtype_filter.run(after_sab, metadata)
            log(f"    Semtype Filter: {len(after_sab)} -> {len(after_sty)} (removed {len(after_sab) - len(after_sty)})")

            after_term, term_audit = TermDeduplicator.run(after_sty, metadata, ic_map)
            log(f"    Term Dedup: {len(after_sty)} -> {len(after_term)} (removed {len(after_sty) - len(after_term)})")

            after_hier, hier_audit, hier_edges = self.hierarchy_reducer.run(
                after_term, ic_map, metadata)
            log(f"    Hierarchy: {len(after_term)} -> {len(after_hier)} (removed {len(after_term) - len(after_hier)})")

            audit = sab_audit + sty_audit + term_audit + hier_audit
            all_post_hier.update(after_hier)
            per_text.append((context_string, cuis, after_hier, hier_edges, audit))

        # ONE embedding fetch for all post-hierarchy CUIs
        log(f"\n[Batch] Fetching embeddings for {len(all_post_hier)} unique post-hierarchy CUIs")
        all_embeddings = self.emb_fetcher.fetch(list(all_post_hier))
        log(f"[Batch] Embeddings: {len(all_embeddings)}/{len(all_post_hier)} matched")

        # per-text: embedding reduction + topics + retention
        results = []
        for context_string, cuis, after_hier, hier_edges, all_audit in per_text:
            t_start = time.perf_counter()
            label = context_string[:50]

            if not after_hier:
                results.append(self._empty(context_string, t_start, all_audit))
                continue

            embeddings = {c: all_embeddings[c] for c in after_hier if c in all_embeddings}
            emb_cov = len(embeddings) / len(after_hier) if after_hier else 0

            topics, surviving, emb_audit = EmbeddingReducer.run(
                after_hier, metadata, embeddings, ic_map, hier_edges)
            all_audit.extend(emb_audit)
            log(f"\n  [{label}]")
            log(f"    Embedding Reduction: {len(after_hier)} -> {len(surviving)} (removed {len(after_hier) - len(surviving)})")

            topics_out = {}
            for topic_id, t_cuis in topics.items():
                if not t_cuis:
                    continue
                topic_types = set()
                cui_terms = {}
                for c in t_cuis:
                    m = metadata.get(c)
                    if m and m.semantic_types:
                        topic_types.update(m.semantic_types)
                    cui_terms[c] = m.preferred_term if m else c
                topics_out[topic_id] = TopicInfo(
                    topic_id=topic_id,
                    semantic_types=topic_types or {"Unknown"},
                    cuis=t_cuis,
                    cui_terms=cui_terms)

            log(f"    Topics: {len(surviving)} CUIs -> {len(topics_out)} topics")

            # embed input text once — used for cross-topic dedup AND retention
            text_vec = self.retention_scorer.embed_text(context_string)

            # cross-topic dedup (topics stay intact, only output list trimmed)
            before_xtd = len(surviving)
            surviving, xtd_audit = CrossTopicDedup.run(
                surviving, embeddings, text_vec, metadata, ic_map)
            all_audit.extend(xtd_audit)
            if xtd_audit:
                log(f"    Cross-Topic Dedup: {before_xtd} -> {len(surviving)} (removed {before_xtd - len(surviving)})")

            # retention scoring
            sorted_cuis, retention = self.retention_scorer.score(
                text_vec, surviving, embeddings, metadata, all_audit)
            if retention:
                log(f"    Retention: scored {len(retention)} (top={retention[0].alignment:.3f}, bottom={retention[-1].alignment:.3f})")

            # relevance filter (centroid-distance + Otsu, pure embedding)
            before_rel = len(retention)
            sorted_cuis, retention, rel_audit = RelevanceFilter.run(
                retention, context_string, metadata, embeddings=embeddings)
            all_audit.extend(rel_audit)
            if before_rel != len(retention):
                log(f"    Relevance Filter: {before_rel} -> {len(retention)} (removed {before_rel - len(retention)})")

            # prune topics to only final surviving CUIs
            final_set = set(sorted_cuis)
            before_prune = len(topics_out)
            pruned_topics = {}
            for topic_id, topic in topics_out.items():
                kept_cuis = [c for c in topic.cuis if c in final_set]
                if not kept_cuis:
                    continue
                kept_terms = {c: topic.cui_terms[c] for c in kept_cuis if c in topic.cui_terms}
                pruned_topics[topic_id] = TopicInfo(
                    topic_id=topic_id,
                    semantic_types=topic.semantic_types,
                    cuis=kept_cuis,
                    cui_terms=kept_terms)
            topics_out = pruned_topics
            log(f"    Topics after pruning: {len(topics_out)} (dropped {before_prune - len(topics_out)} empty)")

            audited = {e.removed_cui for e in all_audit}
            survived = set(surviving)
            unaccounted = set(cuis) - survived - audited
            if unaccounted:
                for cui in unaccounted:
                    m = metadata.get(cui)
                    all_audit.append(AuditEntry(
                        cui, m.preferred_term if m else "",
                        "", "", "unaccounted",
                        "CUI not in surviving or audit trail"))

            elapsed = (time.perf_counter() - t_start) * 1000
            log(f"    DONE: {len(cuis)} -> {len(surviving)} -> {len(retention)} CUIs, {len(topics_out)} topics ({elapsed:.0f}ms)")

            results.append(ReductionResult(
                context_string=context_string,
                input_count=len(cuis),
                after_filter_count=len(after_hier),
                after_hierarchy_count=len(after_hier),
                after_redundancy_count=len(surviving),
                after_relevance_count=len(retention),
                topics=topics_out,
                all_reduced_cuis=sorted_cuis,
                retention=retention,
                audit_trail=all_audit,
                processing_time_ms=elapsed,
                metadata_coverage=emb_cov,
                embedding_coverage=emb_cov))

        total = (time.perf_counter() - t0) * 1000
        log(f"\n[Batch] ALL DONE: {len(inputs)} texts in {total:.0f}ms")

        return results

    def _empty(self, ctx, t0, audit=None, meta_cov=0.0):
        return ReductionResult(
            ctx, 0, 0, 0, 0, 0, {}, [], [], audit or [],
            (time.perf_counter() - t0) * 1000, meta_cov, 0.0)

    def get_stats(self):
        with _timing_lock:
            out = {}
            for k, v in _timings.items():
                if v:
                    vals = list(v)
                    out[k] = {
                        "count": len(vals),
                        "mean_ms": float(np.mean(vals)),
                        "p99_ms": float(np.percentile(vals, 99)),
                    }
            return out


# %% [16] CUI Extraction API
class CUIExtractor:
    """Calls CUI extraction endpoint. Filters by confidence."""

    def __init__(self, api_url, min_confidence=0.0, top_k=3):
        self.url = api_url
        self.min_confidence = min_confidence
        self.top_k = top_k
        self.session = requests.Session()

        try:
            token = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE, universal_newlines=True,
                timeout=30).stdout.strip()
            self.headers = {"Authorization": f"Bearer {token}",
                            "Content-Type": "application/json"}
        except Exception:
            self.headers = {"Content-Type": "application/json"}

        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        retry = Retry(total=3, backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def extract(self, text):
        resp = self.session.post(
            self.url, headers=self.headers,
            json={"query_texts": [text], "top_k": self.top_k},
            timeout=200)
        resp.raise_for_status()
        data = resp.json()

        cuis = []
        if isinstance(data, dict):
            for val in data.values():
                if not isinstance(val, list):
                    continue
                for item in val:
                    if isinstance(item, dict) and "cui" in item:
                        raw_score = item.get("score", item.get("confidence", 1.0))
                        try:
                            score = float(raw_score)
                        except (ValueError, TypeError):
                            score = 0.0
                        if score >= self.min_confidence:
                            cuis.append(str(item["cui"]))
                    elif isinstance(item, str) and item.strip():
                        cuis.append(item.strip())
        return list(set(cuis))


# %% [17] Load Pickles -- RUN ONCE
HIERARCHY_PATH = "umls_hierarchy.pkl"
IC_PATH = "ic_precomputed.pkl"

print("Loading hierarchy...")
with open(HIERARCHY_PATH, "rb") as f:
    hierarchy = pickle.load(f)
print(f"Hierarchy: {hierarchy.number_of_nodes()} nodes, {hierarchy.number_of_edges()} edges")

print("Loading IC scores...")
with open(IC_PATH, "rb") as f:
    ic_precomputed = pickle.load(f)
print(f"IC scores: {len(ic_precomputed)} entries")


# %% [18] Init System
PROJECT_ID = "your-project-id"
DATASET_ID = "your-dataset"
API_URL = "https://your-api/extract"
EMBEDDING_TABLE = "cui_embeddings"
ALLOWED_SABS = ["ICD10CM", "ICD10PCS", "ICD9CM", "SNOMEDCT_US", "LNC"]

system = CUIReductionSystem(
    PROJECT_ID, DATASET_ID,
    hierarchy,
    ALLOWED_SABS,
    EMBEDDING_TABLE,
    ic_scores=ic_precomputed,
)
extractor = CUIExtractor(API_URL)
log("System ready.")


# %% [19] Run Pipeline
texts = [
    "Patient has severe pain in left knee with swelling",
]

# extract CUIs in parallel
def _extract(text):
    cuis = extractor.extract(text)
    return (cuis, text) if cuis else None

with ThreadPoolExecutor(max_workers=min(len(texts), 8)) as pool:
    raw = pool.map(_extract, texts)
    inputs = [r for r in raw if r is not None]

# single batch call
results = system.reduce_batch(inputs)

# display results
for result in results:
    log(f"\n{'=' * 60}")
    log(f"TEXT: {result.context_string}")
    log(f"SUMMARY: {result.input_count} -> {result.after_redundancy_count} (reduction) -> {result.after_relevance_count} (relevance) CUIs, "
        f"{len(result.topics)} topics ({result.processing_time_ms:.0f}ms)")

    log(f"\n-- Top 20 CUIs by retention (of {len(result.retention)}) --")
    for entry in result.retention[:20]:
        stys = ", ".join(entry.semantic_types[:2]) if entry.semantic_types else "?"
        line = f"  {entry.cui} | align={entry.alignment} | [{stys}] {entry.term}"
        if entry.explains_cuis:
            line += f" | explains {len(entry.explains_cuis)}"
        log(line)

    log(f"\n-- Topics: {len(result.topics)} (showing first 20) --")
    for tid, topic in list(result.topics.items())[:20]:
        log(f"  {tid} ({len(topic.cuis)} CUIs)")
    if len(result.topics) > 20:
        log(f"  ... {len(result.topics) - 20} more topics")

# %% [20] Inspect CUIs under a topic
target_topic = "topic_0"  # change this to any topic ID
for result in results:
    topic = result.topics.get(target_topic)
    if topic:
        log(f"\n{target_topic} ({len(topic.cuis)} CUIs):")
        for c, term in topic.cui_terms.items():
            log(f"  {c}  {term}")
        break
    else:
        log(f"Topic '{target_topic}' not found")

# %% [21] Inspect explains for any CUI
target = "C4081756"  # change this to any CUI
for result in results:
    for entry in result.retention:
        if entry.cui == target:
            log(f"\n{entry.cui} ({entry.term}) explains {len(entry.explains_cuis)} CUIs:")
            for exc in entry.explains_cuis:
                term = ""
                for a in result.audit_trail:
                    if a.removed_cui == exc:
                        term = a.removed_term
                        break
                log(f"  {exc}  {term}")
            break

# %% [22] Validate: search for a term across audit + retention
search_term = "knee pain"  # change this to any term fragment
for result in results:
    log(f"\n-- Searching '{search_term}' across pipeline --")

    log(f"\n  SURVIVED (in retention):")
    for entry in result.retention:
        if search_term.lower() in entry.term.lower():
            log(f"    {entry.cui} | align={entry.alignment} | {entry.term}")

    log(f"\n  REMOVED (in audit trail):")
    for a in result.audit_trail:
        if search_term.lower() in a.removed_term.lower():
            log(f"    {a.removed_cui} ({a.removed_term})")
            log(f"      -> kept: {a.kept_cui} ({a.kept_term})")
            log(f"      -> reason: {a.reason} | {a.detail}")

# %% [23] Validate relevance filter — inspect cutoff and borderline CUIs
for result in results:
    rel_removed = [a for a in result.audit_trail if a.reason == "below_relevance_threshold"]
    if not rel_removed:
        log("Relevance filter removed 0 CUIs")
        continue

    cutoff_detail = rel_removed[0].detail
    log(f"Relevance filter: kept {len(result.retention)}, removed {len(rel_removed)}")
    log(f"Cutoff: {cutoff_detail}")

    # bottom 10 survivors
    log(f"\n-- Bottom 10 KEPT (borderline survivors) --")
    for entry in result.retention[-10:]:
        log(f"  {entry.cui} | align={entry.alignment} | {entry.term}")

    # top 10 removed (parse centroid_sim from detail)
    log(f"\n-- Top 10 REMOVED (closest to cutoff — check these) --")
    def _parse_sim(detail):
        try:
            return float(detail.split("centroid_sim ")[1].split(" ")[0])
        except:
            return 0.0
    rel_removed_sorted = sorted(rel_removed, key=lambda a: _parse_sim(a.detail), reverse=True)
    for a in rel_removed_sorted[:10]:
        log(f"  {a.removed_cui} | {a.detail} | {a.removed_term}")

    # bottom 10 removed
    log(f"\n-- Bottom 10 REMOVED (most distant — should be noise) --")
    for a in rel_removed_sorted[-10:]:
        log(f"  {a.removed_cui} | {a.detail} | {a.removed_term}")

    # distribution
    kept_scores = [e.alignment for e in result.retention]
    log(f"\n-- Distribution --")
    log(f"  Kept:    count={len(kept_scores)}  min={min(kept_scores):.4f}  max={max(kept_scores):.4f}  mean={np.mean(kept_scores):.4f}")
    log(f"  Removed: count={len(rel_removed)}")

# %% [24] Centroid analysis — validate relevance filter decisions
for result in results:
    rel_removed = [a for a in result.audit_trail if a.reason == "below_relevance_threshold"]
    if not rel_removed:
        log("Relevance filter removed 0 CUIs")
        continue

    log(f"\n-- Centroid Filter Analysis --")
    log(f"  Kept: {len(result.retention)}, Removed: {len(rel_removed)}")

    # removed CUIs with HIGH text alignment — centroid said no, alignment said yes
    log(f"\n-- REMOVED with high text alignment (>0.68) — verify these --")
    log(f"  (These are semantically close to query but far from anchor cluster)")
    high_align_removed = []
    for a in rel_removed:
        for entry in [e for e in result.retention]:
            pass  # just need alignment from audit
        # alignment is in retention entries before filter; get from detail
        try:
            csim = float(a.detail.split("centroid_sim ")[1].split(" ")[0])
        except:
            csim = 0.0
        high_align_removed.append((a.removed_cui, a.removed_term, a.detail, csim))

    # sort by centroid sim descending to see borderline cases
    high_align_removed.sort(key=lambda x: x[3], reverse=True)
    for cui, term, detail, csim in high_align_removed[:15]:
        log(f"  {cui} | {detail} | {term}")
    if len(high_align_removed) > 15:
        log(f"  ... {len(high_align_removed) - 15} more")

    # semantic type breakdown: what types were kept vs removed?
    _meta_cache = system.fetcher._cache._d
    kept_types = defaultdict(int)
    removed_types = defaultdict(int)
    for entry in result.retention:
        m = _meta_cache.get(entry.cui)
        if m:
            for sty in m.semantic_types:
                kept_types[sty] += 1
    for a in rel_removed:
        m = _meta_cache.get(a.removed_cui)
        if m:
            for sty in m.semantic_types:
                removed_types[sty] += 1

    log(f"\n-- Semantic types: kept vs removed --")
    all_types = set(kept_types.keys()) | set(removed_types.keys())
    type_data = []
    for sty in all_types:
        k = kept_types.get(sty, 0)
        r = removed_types.get(sty, 0)
        pct_kept = k / (k + r) * 100 if (k + r) > 0 else 0
        type_data.append((sty, k, r, pct_kept))
    type_data.sort(key=lambda x: x[1] + x[2], reverse=True)
    for sty, k, r, pct in type_data[:15]:
        log(f"  {sty}: kept={k}, removed={r}, {pct:.0f}% kept")
