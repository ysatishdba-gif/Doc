"""
Clinical Intent Extraction Pipeline with Schema-Based Structured Outputs
Focus: Schema definitions + LLM integration using Vertex AI response_schema
"""

import json
from datetime import datetime
from typing import Dict, Any, List, Literal
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel
from pydantic import BaseModel, Field


# ============================================================================
# SCHEMA DEFINITIONS - Three Output Formats
# ============================================================================

class SubNature(BaseModel):
    """Dimensional breakdown of clinical information."""
    category_path: str = Field(
        description="Hierarchical path (e.g., 'Medication >> Dosage >> Frequency')"
    )
    atomic_concepts: List[str] = Field(
        description="Terminal granular concepts"
    )


# OUTPUT 1: Intent Extractions (High-level only)
class IntentBasic(BaseModel):
    intent_title: str
    description: str


class IntentExtractionsOutput(BaseModel):
    """Schema for Output 1: High-level intent extractions."""
    original_query: str
    expanded_query: str
    count_of_intents: int
    intents: List[IntentBasic]


# OUTPUT 2: Nature Breakdown (Complete dimensional analysis)
class IntentWithNature(BaseModel):
    intent_title: str
    description: str
    nature: str
    sub_natures: List[SubNature]


class NatureBreakdownOutput(BaseModel):
    """Schema for Output 2: Complete nature and dimensional breakdown."""
    original_query: str
    expanded_query: str
    count_of_intents: int
    intents: List[IntentWithNature]


# OUTPUT 3: Queries (Primary for CUI extraction)
class IntentQueries(BaseModel):
    intent_title: str
    final_queries: List[str]


class QueriesOutput(BaseModel):
    """Schema for Output 3: Flattened query extraction for CUI processing."""
    original_query: str
    expanded_query: str
    count_of_intents: int
    total_queries: int
    intents: List[IntentQueries]
    all_queries: List[str] = Field(
        description="Flattened list of all queries for easy processing"
    )


# INTERNAL: Full structure (for processing)
class IntentFull(BaseModel):
    intent_title: str
    description: str
    nature: str
    sub_natures: List[SubNature]
    final_queries: List[str]


class FullIntentExtractionOutput(BaseModel):
    """Complete internal schema with all fields."""
    is_clinical: bool
    original_query: str
    expanded_query: str
    total_intents_detected: int
    intents: List[IntentFull]
    reason: str = ""


# Query Expansion Schema
class QueryExpansionOutput(BaseModel):
    """Schema for query expansion step."""
    expanded_query: str
    abbreviations_expanded: List[str] = Field(default_factory=list)


# ============================================================================
# PROMPTS
# ============================================================================

QUERY_EXPANSION_PROMPT = """
You are an expert medical AI assistant specializing in clinical query expansion.

TASK: Expand the user's query into a comprehensive, detailed clinical description.

INSTRUCTIONS:
1. Expand ALL medical abbreviations to full terms (e.g., HTN → Hypertension)
2. Clarify vague medical terms with specific clinical language
3. Add relevant medical context based on standard clinical practice
4. Identify implicit clinical concepts that should be explicit
5. DO NOT add assumptions beyond reasonable clinical interpretation
6. DO NOT include action verbs like "analyze", "review" unless in original query
7. Maintain the original query's intent and scope
8. Make sure the temporal aspect is relevant to the query context

EXAMPLES:
- "Pt with DM" → "Patient with Diabetes Mellitus"
- "SOB on exertion" → "Shortness of breath on exertion"

User Input: {query}
"""

INTENT_EXTRACTION_PROMPT = """
You are a clinical intent extraction engine for medical document retrieval.

CLINICAL VALIDATION: Return is_clinical=false if query is non-clinical.

CORE PRINCIPLES:
When someone requests clinical information, extract everything needed to fully understand, 
act upon, or make decisions about that information safely and effectively.

INTENT GENERATION:
Analyze the expanded query and identify distinct clinical intents. Each intent represents 
a clinically independent concept that could be documented or understood separately.

INTENT STRUCTURE:
For each intent:
1. intent_title - What is this about?
2. description - What does this represent and why does it matter?
3. nature - Primary informational purpose (Format: [Context] / [Purpose])
4. sub_natures[] - Distinct dimensions with category_path and atomic_concepts
5. final_queries[] - Atomic-level queries for CUI extraction (2-4 words each)

SUB_NATURE DECOMPOSITION:
- category_path: "Broad >> Specific >> Detail" (navigation path)
- atomic_concepts: Terminal granular elements (the actual data points)

FINAL_QUERIES - ATOMIC SPECIFICITY:
Generate SHORT, PRECISE queries (2-4 words max). Each query should represent ONE atomic 
clinical concept. Shorter queries = fewer CUIs = more precise matching.

Examples:
- ✗ "metformin 500mg twice daily for diabetes" → Too long, generates 10+ CUIs
- ✓ "metformin 500mg" → Focused, generates 2-3 CUIs
- ✓ "twice daily dosing" → Atomic, generates 1-2 CUIs

User Input (original): {original_query}
User Input (expanded): {expanded_query}
Timestamp: {timestamp}
"""


# ============================================================================
# PIPELINE CLASS
# ============================================================================

class SchemaBasedIntentPipeline:
    """
    Clinical intent extraction pipeline using schema-enforced structured outputs.
    Uses Vertex AI's response_schema parameter with Pydantic models.
    """
    
    def __init__(
        self, 
        project: str, 
        location: str = "us-central1", 
        model: str = "gemini-2.0-flash-exp"
    ):
        aiplatform.init(project=project, location=location)
        self.model = GenerativeModel(model)
        
    def _call_with_schema(
        self, 
        prompt: str, 
        response_schema: type[BaseModel],
        temperature: float = 0.0,
        max_tokens: int = 8096
    ) -> BaseModel:
        """
        Call LLM with schema enforcement using Vertex AI's response_schema.
        
        This is the key method - it passes the Pydantic schema to Vertex AI
        which guarantees the response will match the schema structure.
        """
        response = self.model.generate_content(
            prompt,
            generation_config={
                "temperature": temperature,
                "max_output_tokens": max_tokens,
                "response_mime_type": "application/json",
                "response_schema": response_schema  # Schema enforcement!
            }
        )
        
        # Parse and validate against schema
        json_response = json.loads(response.text)
        return response_schema(**json_response)
    
    def expand_query(self, query: str) -> QueryExpansionOutput:
        """Step 1: Expand query with schema enforcement."""
        prompt = QUERY_EXPANSION_PROMPT.format(query=query)
        
        try:
            result = self._call_with_schema(
                prompt=prompt,
                response_schema=QueryExpansionOutput,
                max_tokens=2048
            )
            return result
        except Exception as e:
            print(f"Expansion failed: {e}")
            return QueryExpansionOutput(
                expanded_query=query,
                abbreviations_expanded=[]
            )
    
    def extract_intents_full(
        self, 
        original_query: str, 
        expanded_query: str
    ) -> FullIntentExtractionOutput:
        """Step 2: Extract complete intent structure with schema enforcement."""
        prompt = INTENT_EXTRACTION_PROMPT.format(
            original_query=original_query,
            expanded_query=expanded_query,
            timestamp=datetime.utcnow().isoformat()
        )
        
        result = self._call_with_schema(
            prompt=prompt,
            response_schema=FullIntentExtractionOutput,
            max_tokens=8096
        )
        return result
    
    def _convert_to_format(
        self, 
        full_output: FullIntentExtractionOutput,
        output_type: Literal["intent_extractions", "nature_breakdown", "queries"]
    ) -> BaseModel:
        """Convert full output to requested format."""
        
        if output_type == "intent_extractions":
            return IntentExtractionsOutput(
                original_query=full_output.original_query,
                expanded_query=full_output.expanded_query,
                count_of_intents=full_output.total_intents_detected,
                intents=[
                    IntentBasic(
                        intent_title=intent.intent_title,
                        description=intent.description
                    )
                    for intent in full_output.intents
                ]
            )
        
        elif output_type == "nature_breakdown":
            return NatureBreakdownOutput(
                original_query=full_output.original_query,
                expanded_query=full_output.expanded_query,
                count_of_intents=full_output.total_intents_detected,
                intents=[
                    IntentWithNature(
                        intent_title=intent.intent_title,
                        description=intent.description,
                        nature=intent.nature,
                        sub_natures=intent.sub_natures
                    )
                    for intent in full_output.intents
                ]
            )
        
        else:  # queries
            all_queries = []
            for intent in full_output.intents:
                all_queries.extend(intent.final_queries)
            
            return QueriesOutput(
                original_query=full_output.original_query,
                expanded_query=full_output.expanded_query,
                count_of_intents=full_output.total_intents_detected,
                total_queries=len(all_queries),
                intents=[
                    IntentQueries(
                        intent_title=intent.intent_title,
                        final_queries=intent.final_queries
                    )
                    for intent in full_output.intents
                ],
                all_queries=all_queries
            )
    
    def run(
        self,
        query: str,
        output_type: Literal["intent_extractions", "nature_breakdown", "queries", "full"] = "queries",
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        Main pipeline execution with schema-enforced outputs.
        
        Args:
            query: Clinical query to process
            output_type: Which output format to return
                - "intent_extractions": High-level intents only
                - "nature_breakdown": Complete dimensional analysis
                - "queries": Atomic queries for CUI extraction (DEFAULT)
                - "full": Complete internal format
            verbose: Print processing steps
        
        Returns:
            Dict with status and schema-validated data
        """
        start_time = datetime.utcnow()
        
        if verbose:
            print(f"\n{'='*70}")
            print(f"Query: {query}")
            print(f"Output Type: {output_type}")
            print(f"{'='*70}\n")
        
        # Step 1: Query Expansion (with schema)
        if verbose:
            print("Step 1: Expanding query...")
        expansion = self.expand_query(query)
        
        if verbose:
            print(f"✓ Expanded: {expansion.expanded_query}\n")
        
        # Step 2: Intent Extraction (with schema)
        if verbose:
            print("Step 2: Extracting intents...")
        full_output = self.extract_intents_full(query, expansion.expanded_query)
        
        # Check if clinical
        if not full_output.is_clinical:
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            if verbose:
                print(f"✗ Non-clinical query rejected\n")
            
            return {
                "status": "rejected",
                "is_clinical": False,
                "reason": full_output.reason,
                "query": query,
                "processing_time_seconds": processing_time
            }
        
        if verbose:
            print(f"✓ Extracted {full_output.total_intents_detected} intents\n")
        
        # Step 3: Convert to requested format
        if output_type == "full":
            output_data = full_output.model_dump()
        else:
            converted = self._convert_to_format(full_output, output_type)
            output_data = converted.model_dump()
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        if verbose:
            print(f"✓ Complete! ({processing_time:.2f}s)\n")
        
        return {
            "status": "success",
            "output_type": output_type,
            "data": output_data,
            "metadata": {
                "processing_time_seconds": processing_time,
                "timestamp": datetime.utcnow().isoformat(),
                "abbreviations_expanded": expansion.abbreviations_expanded
            }
        }
    
    def run_batch(
        self,
        queries: List[str],
        output_type: Literal["intent_extractions", "nature_breakdown", "queries", "full"] = "queries",
        verbose: bool = False
    ) -> Dict[str, Any]:
        """Process multiple queries in batch."""
        batch_start = datetime.utcnow()
        results = []
        
        if verbose:
            print(f"\nProcessing {len(queries)} queries...\n")
        
        for idx, query in enumerate(queries, 1):
            if verbose:
                print(f"[{idx}/{len(queries)}] {query[:50]}...")
            
            try:
                result = self.run(query, output_type=output_type, verbose=False)
                results.append(result)
                
                if verbose and result["status"] == "success":
                    print(f"  ✓ Success\n")
            except Exception as e:
                if verbose:
                    print(f"  ✗ Error: {e}\n")
                results.append({"status": "failed", "error": str(e), "query": query})
        
        total_time = (datetime.utcnow() - batch_start).total_seconds()
        
        return {
            "total_queries": len(queries),
            "successful": sum(1 for r in results if r["status"] == "success"),
            "failed": sum(1 for r in results if r["status"] != "success"),
            "results": results,
            "total_processing_time_seconds": total_time
        }


# ============================================================================
# USAGE EXAMPLES
# ============================================================================

if __name__ == "__main__":
    # Initialize pipeline
    PROJECT_ID = "your-project-id"  # Replace with your GCP project
    
    pipeline = SchemaBasedIntentPipeline(
        project=PROJECT_ID,
        location="us-central1",
        model="gemini-2.0-flash-exp"
    )
    
    # Example query
    query = "history of Glioblastoma in the last 6 years"
    
    print("\n" + "="*70)
    print("EXAMPLE 1: Get Queries for CUI Extraction")
    print("="*70)
    
    # Get queries output (primary use case for CUI pipeline)
    result = pipeline.run(
        query=query,
        output_type="queries",
        verbose=True
    )
    
    if result["status"] == "success":
        queries = result["data"]["all_queries"]
        print(f"Generated {len(queries)} atomic queries:")
        for i, q in enumerate(queries, 1):
            print(f"  {i}. {q}")
        
        # Save for downstream CUI pipeline
        with open("queries_for_cui_pipeline.json", "w") as f:
            json.dump(result, f, indent=2)
        print(f"\n✓ Saved: queries_for_cui_pipeline.json")
    
    print("\n" + "="*70)
    print("EXAMPLE 2: Get All Three Formats")
    print("="*70)
    
    # Get all three output formats
    for output_type in ["intent_extractions", "nature_breakdown", "queries"]:
        result = pipeline.run(query, output_type=output_type, verbose=False)
        
        filename = f"output_{output_type}.json"
        with open(filename, "w") as f:
            json.dump(result, f, indent=2)
        
        print(f"✓ {output_type:20s} → {filename}")
    
    print("\n" + "="*70)
    print("EXAMPLE 3: Batch Processing")
    print("="*70)
    
    # Batch process multiple queries
    queries = [
        "Patient with HTN and DM",
        "SOB on exertion",
        "Family history of CAD"
    ]
    
    batch_result = pipeline.run_batch(
        queries=queries,
        output_type="queries",
        verbose=True
    )
    
    print(f"\nBatch Summary:")
    print(f"  Total: {batch_result['total_queries']}")
    print(f"  Successful: {batch_result['successful']}")
    print(f"  Failed: {batch_result['failed']}")
    print(f"  Time: {batch_result['total_processing_time_seconds']:.2f}s")
