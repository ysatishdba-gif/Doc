import json
import copy
import time
import base64
import asyncio
import httpx

from typing import Dict, Optional, Any
from fastapi import FastAPI, Response, status
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, model_validator
from app.mod_llm_classification import ModLLMClassificationMultimodal
from app.classes.definitions import tag_definitions
from app.exceptions import TagNotExist
from app.utils import (
    run_genai,
    run_genai_async,
    get_genai_usage_metadata,
    clean_enclosed_markdown,
    genai_pool,
)

import os
from aie_logging import GCPLogger, Severity
from starlette.middleware import Middleware
from app.middleware.logging_middleware import StructuredLoggingMiddleware

SERVICE_NAME = "llm_chunk_classification"
SERVICE_VERSION = "1.0.0"

project_id = os.getenv("PROJECT_ID", "")

# Base structured metadata for every log
base_log = {"service_name": SERVICE_NAME, "version": SERVICE_VERSION}

# Initialize GCPLogger with base_log_metadata
lg = GCPLogger(
    gcp_project=project_id,
    console_logging=True,
    log_name=SERVICE_NAME,
    base_log_metadata=base_log,
)

middleware = [Middleware(StructuredLoggingMiddleware, logger=lg, severity=Severity)]

app = FastAPI(middleware=middleware)


@app.on_event("startup")
async def startup_event():
    """Initialize the GenAI client pool at application startup."""
    genai_pool.initialize()
    lg.log_struct(
        message=f"GenAI client pool initialized with {genai_pool.pool_size} clients",
        structured_data={
            **base_log,
            "event": "genai_pool_initialized",
            "pool_size": genai_pool.pool_size,
        },
        severity="INFO",
    )


# classifier = ModLLMClassification("ODE_LLM_MOD", tag_definitions)
classifier = ModLLMClassificationMultimodal("ODE_LLM_MOD", tag_definitions)

version_id = "1.4.0"
service_id = "ODE LLM-based Chunk Tagging - " + version_id

ALLOWED_GEN_MODELS = [
    "gemini-2.0-flash-001",
    "gemini-2.5-flash",
    "gemini-2.5-flash-lite",
]
genai_model_name = ALLOWED_GEN_MODELS[2]


async def call_with_retry_for_tag(
    client, prompt, schema, genai_model_name, config, tag, retries=3
):
    """
    Call Gemini LLM with retries specifically for individual tag processing.
    Uses exponential backoff with jitter.
    """
    import random

    for attempt in range(retries):
        try:
            return await run_genai_async(
                client, prompt, schema, genai_model_name, config=config
            )

        except (
            httpx.ConnectError,
            httpx.ReadTimeout,
            httpx.ReadError,
            httpx.RemoteProtocolError,
        ) as e:
            if attempt < retries - 1:
                wait_time = (2**attempt) + random.uniform(0, 1)
                lg.log_struct(
                    message=f"Network error on Gemini call (tag: {tag}), retrying in {wait_time:.1f}s",
                    structured_data={
                        "event": "gemini_retry_network",
                        "error_type": type(e).__name__,
                        "error": str(e),
                        "attempt": attempt + 1,
                        "tag": tag,
                        "wait_time": wait_time,
                    },
                    severity="WARNING",
                )
                await asyncio.sleep(wait_time)
            else:
                lg.log_struct(
                    message=f"Network error (tag: {tag}), max retries exceeded",
                    structured_data={
                        "event": "gemini_retry_exhausted",
                        "error_type": type(e).__name__,
                        "error": str(e),
                        "tag": tag,
                    },
                    severity="ERROR",
                )
                raise

        except Exception as e:
            error_msg = str(e).lower()

            # Check if it's a retryable service error
            is_retryable = any(
                x in error_msg
                for x in [
                    "503",
                    "unavailable",
                    "resource_exhausted",
                    "429",
                    "deadline exceeded",
                    "server error",
                ]
            )

            if is_retryable:
                if attempt < retries - 1:
                    wait_time = (5 * (2**attempt)) + random.uniform(0, 2)
                    lg.log_struct(
                        message=f"Service error (tag: {tag}), retrying in {wait_time:.1f}s",
                        structured_data={
                            "event": "gemini_retry_service",
                            "error_type": type(e).__name__,
                            "error": str(e),
                            "attempt": attempt + 1,
                            "tag": tag,
                            "wait_time": wait_time,
                        },
                        severity="WARNING",
                    )
                    await asyncio.sleep(wait_time)
                else:
                    lg.log_struct(
                        message=f"Service error (tag: {tag}), max retries exceeded",
                        structured_data={
                            "event": "gemini_retry_exhausted_service",
                            "error_type": type(e).__name__,
                            "error": str(e),
                            "tag": tag,
                        },
                        severity="ERROR",
                    )
                    raise
            else:
                # Non-retryable error, raise immediately
                raise


class RequestClassify(BaseModel):
    text: Optional[str] = Field(
        None,
        description="Text content to classify. This is the main input for classification.",
    )
    image: Optional[str] = Field(
        None,
        description="Optional image content for multimodal classification in Base64 string. If provided, the model will process both text and image.",
    )
    image_mime: Optional[str] = Field(
        None,
        description="MIME type of the image content. Required if 'image' is provided.",
    )
    image_file: Optional[str] = Field(
        None,
        description="Optional image file path for multimodal classification. If provided, the model will process both text and image.",
    )
    context_cache: Optional[str] = Field(
        None,
        description="Optional context cache to provide additional context for classification.",
    )

    tags: list[str] = Field(
        None,
        description="List of tags to classify against. If not provided, all tags will be used.",
    )
    classify: bool = Field(
        True, description="If True, the response will include classes. Default is True."
    )
    extract: bool = Field(
        False,
        description="If True, the response will include extracted attributes. Default is False.",
    )
    definitions: Dict[str, dict] = Field(
        default_factory=dict,
        description="Custom definitions for tags. If provided, these will override the default definitions.",
    )
    individual: bool = Field(
        False,
        description="If True, each tag definition is evaluated individually. Default is False.",
    )
    shared_tags: bool = Field(
        True,
        description="If True, shared tags, if existed in the model, will be included in the schema. Default is True.",
    )
    genai_model_name: str = Field(default_factory=lambda: genai_model_name)
    config: Dict[str, Any] = Field(
        default_factory=lambda: {
            "temperature": 0.0,
            "max_output_tokens": 8192,
            "top_p": 0.8,
            "top_k": 40,
            "cached_content": None,
        },
        description="Configuration for content generation. Defaults to a predefined configuration.",
    )

    @property
    def decoded_image(self):
        if self.image:
            try:
                return base64.b64decode(self.image)
            except Exception as e:
                raise ValueError(f"Invalid Base64-encoded image: {e}")
        return None

    @model_validator(mode="before")
    def validate_config(cls, values):
        if isinstance(values.get("config"), dict):
            config = values.get("config")
            if "response_schema" in config:
                raise ValueError(
                    "The 'response_schema' field is not allowed in the 'config'."
                )
            if "system_instruction" in config:
                raise ValueError(
                    "The 'system_instruction' field is not allowed in the 'config'."
                )
            if "response_mime_type" in config:
                if config["response_mime_type"] != "application/json":
                    raise ValueError(
                        "The 'response_mime_type' in 'config' cannot be set to anything than 'application/json'."
                    )
        return values

    @model_validator(mode="after")
    def validate_main_content(self):
        text = self.text
        image = self.image
        image_mime = self.image_mime
        image_file = self.image_file
        context_cache = self.context_cache

        if not text and not image and not image_file and not context_cache:
            raise ValueError(
                "At least one of 'text', 'image', or 'image_file' must be provided."
            )
        if image and not image_mime:
            raise ValueError(
                "'image_mime' must be provided when 'image' (bytes) is included."
            )
        if isinstance(context_cache, str):
            if context_cache.strip() == "":
                raise ValueError(
                    "'context_cache' cannot be an empty string. It can be None or a valid cache identifier."
                )
        return self

    @model_validator(mode="after")
    def validate_classify_or_extract(self):
        classify = self.classify
        extract = self.extract
        if not classify and not extract:
            raise ValueError(
                "Either 'classify' or 'extract' must be True. Both cannot be False."
            )
        if self.genai_model_name not in ALLOWED_GEN_MODELS:
            raise ValueError(
                f"Invalid 'genai_model_name'. Allowed models are: {', '.join(ALLOWED_GEN_MODELS)}"
            )
        return self

    @model_validator(mode="after")
    def validate_definitions_structure(self):
        definitions = self.definitions
        if not classifier.validate_definitions(definitions):
            raise ValueError("Invalid definition structure.")
        # for key, value in definitions.items():
        #     try:
        #         value_copy = copy.deepcopy(value)
        #
        #         # Validate structure against TagModel
        #         TagModel.model_validate(value_copy)
        #     except ValidationError as e:
        #         raise ValueError(f"Invalid structure for tag '{key}': {e}")
        return self


class RequestValidateDefinitions(BaseModel):
    definitions: dict = Field(
        ...,
        description="Nested structure object that contains tag definitions to validate.",
    )


class RequestDefinitions(BaseModel):
    tags: list[str] = Field(
        ..., description="List of tags to validate against the definitions."
    )


@app.get("/tags", status_code=200)
def get_tags():
    """
    Endpoint to retrieve the list of available tags.
    """
    try:
        servid = service_id + " - All Tags"
        output = classifier.labels
        resp_status = 1
        details = {"description": "List of labels currently supported by this API"}
    except Exception as e:
        lg.log_text(
            f"Error retrieving list of available tags: {e}", severity=Severity.ERROR
        )
        resp_status = 0
        details = {"error": str(e)}

    result = {
        "status": resp_status,
        "service": servid,
        "output": output if resp_status == 1 else [],
        "details": details,
    }

    return JSONResponse(content=result, status_code=200 if resp_status == 1 else 400)


@app.post("/validate", status_code=200)
def validate_definitions(request_data: RequestValidateDefinitions, response: Response):
    """
    Endpoint to validate tag definitions.
    """
    servid = service_id + " - Validate Custom Definitions"
    result = {"status": 1, "service": servid, "output": 0, "details": {}}

    try:
        assessment = classifier.validate_definitions(request_data.definitions)
        if assessment:
            result["output"] = 1
            result["details"] = {"message": "Definitions are valid"}
        else:
            result["details"] = {"message": "Definitions are invalid"}
        return result
    except Exception as e:
        lg.log_text(
            f"Error in validating tag definitions: {e}", severity=Severity.ERROR
        )
        result["status"] = 0
        result["details"] = {
            "message": f"Error during definitions validation: {str(e)}"
        }
        return JSONResponse(content=result, status_code=400)


@app.post("/classify", status_code=200)
async def classify_text(request_data: RequestClassify, response: Response):
    try:
        definitions = request_data.definitions
        tags = request_data.tags
        include_definition = request_data.classify
        include_attributes = request_data.extract
        individual = request_data.individual
        shared_tags = request_data.shared_tags
        config = request_data.config
        genai_model_name = request_data.genai_model_name

        if definitions:
            _classifier = ModLLMClassificationMultimodal(
                "ODE_LLM_MOD_CUSTOM", definitions
            )
            custom_run = "Custom Run"
        else:
            _classifier = classifier
            custom_run = "Default Run"
        servid = service_id + f" - Classification - {custom_run} - " + _classifier.name

        # Retrieve input content ---------------------------------------------------------------------------------------------------------------------
        text = request_data.text
        image = None
        if request_data.image:
            image = request_data.decoded_image
            if not image:
                raise ValueError("Failed to decode the Base64-encoded image.")
        image_mime = request_data.image_mime
        image_file = request_data.image_file
        context_cache = request_data.context_cache

        # Retrieve LLM configurations ----------------------------------------------------------------------------------------------------------------
        # config_dict = request_data.config.model_dump() if isinstance(request_data.config, CustomGenerateContentConfig) else request_data.config
        config_dict = request_data.config
        config_dict = copy.deepcopy(config_dict)
        config["cached_content"] = context_cache
        config["response_mime_type"] = "application/json"
        config["response_schema"] = None
        config["system_instruction"] = None

        if tags is not None and len(tags) <= 0:
            raise TagNotExist("")
        elif tags is not None:
            for tag in tags:
                try:
                    _ = _classifier.definitions[tag]
                except KeyError:
                    raise TagNotExist(tag)
        else:
            tags = list(_classifier.definitions.keys())

        # Filter tags to include only those that are defined as tags in the classifier ---------------------------------------------------------------
        # Note classification model objects always have 'is_tag' one even not specified by users as the pasting process will add it in automatically.
        # Note the default value of 'is_tag' is True.
        tmp = []
        for tag in tags:
            if _classifier.definitions[tag]["configurations"]["is_tag"]:
                tmp.append(tag)
        tags = tmp

        remove_tags = []
        if not include_definition:
            remove_tags += [
                tag + "_" + ele
                for tag in tags
                for ele in _classifier.tag_models[tag].definition_tags_meaning.keys()
            ]
        if not include_attributes:
            remove_tags += [
                tag + "_" + ele
                for tag in tags
                for ele in _classifier.tag_models[tag].definition["attributes"].keys()
            ]
        remove_tags = set(remove_tags)

        # Record all tags, including shared tag, and their attributes --------------------------------------------------------------------------------
        dict_tags = {
            "shared": {"name": "Shared Tags", "fields": _classifier.shared_tags}
        }
        for tag in tags:
            dict_tags[tag] = {
                "name": _classifier.definitions[tag]["name"],
                "fields": [],
            }
            for def_tag in _classifier.tag_models[tag].definition_tags_meaning.keys():
                dict_tags[tag]["fields"].append(tag + "_" + def_tag)

        # Create a result skeleton -------------------------------------------------------------------------------------------------------------------
        result = {
            "status": 1,
            "service": servid,
            "output": None,
            "details": {
                "classify": include_definition,
                "extract": include_attributes,
                "tags": dict_tags,
                "individual": individual,
                "shared_tags": shared_tags,
                "custom": custom_run,
                "genai_model_name": genai_model_name,
                "config": config_dict,
            },
        }

        # Tagging Process Starts ---------------------------------------------------------------------------------------------------------------------
        # If process all tags altogether, then the process ends in this if-clause. Otherwise, proceeds.
        if not individual:
            st = time.time()
            try:
                schema = _classifier.schema(
                    include_definition=include_definition,
                    include_attributes=include_attributes,
                    tags=tags,
                    shared_tags=shared_tags,
                )
                if context_cache:
                    prompt = _classifier.prompt(schema, context_cache=context_cache)
                elif text:
                    prompt = _classifier.prompt(schema, text=text)
                elif image:
                    prompt = _classifier.prompt(
                        schema, image=image, image_mime=image_mime
                    )
                elif image_file:
                    prompt = _classifier.prompt(
                        schema, image_file=image_file, image_mime=image_mime
                    )
                else:
                    raise ValueError(
                        "Either 'text', 'image', or 'image_file' must be provided for classification."
                    )
            except Exception as ex:
                lg.log_text(
                    f"Error generating schema or prompt for the provided tags: {ex}",
                    severity=Severity.ERROR,
                )
                result["status"] = 0
                result["details"] = {
                    "error": {
                        "message": f"Error generating schema or prompt for the provided tags : {type(ex).__name__} -- {ex}"
                    }
                }
                return JSONResponse(content=result, status_code=400)

            try:
                client = genai_pool.get_client()

                # Log before Gemini API call
                start_time = time.time()
                lg.log_struct(
                    message="Gemini API call started (non-individual)",
                    structured_data={
                        **base_log,
                        "event": "gemini_call_start",
                        "start_time": start_time,
                    },
                    severity="INFO",
                )

                resai = run_genai(
                    client, prompt, schema, genai_model_name, config=config
                )

                # Log after Gemini API call
                end_time = time.time()
                lg.log_struct(
                    message="Gemini API call ended (non-individual)",
                    structured_data={
                        **base_log,
                        "event": "gemini_call_end",
                        "end_time": end_time,
                        "duration": end_time - start_time,
                    },
                    severity="INFO",
                )
                res = json.loads(clean_enclosed_markdown(resai.text))
            except Exception as ex:
                lg.log_text(
                    f"Error running LLM: {type(ex).__name__} -- {ex}",
                    severity=Severity.ERROR,
                )
                result["status"] = 0
                result["details"] = {
                    "error": {
                        "message": f"Error running LLM : {type(ex).__name__} -- {ex}"
                    }
                }
                return JSONResponse(content=result, status_code=400)

            # # Make sure that the model does not force the definition tags to be included, despite the user not asking for them.
            # # Note some models require definition tags to be included in the schema for answering the shared tags.
            if len(remove_tags) > 0:
                removed = []
                for k in res:
                    if k in remove_tags:
                        removed.append(k)
                for ele in removed:
                    _ = res.pop(ele)

            result["output"] = res
            result["details"]["run_strategy"] = "All"
            result["details"]["duration"] = time.time() - st
            result["details"]["usage_metadata"] = get_genai_usage_metadata(
                resai, only_numeric=True
            )
            lg.log_struct(
                message="ODE LLM-based Chunk Tagging - /classify successful",
                structured_data=base_log,
                severity="INFO",
            )
            return result

        tag_client = genai_pool.get_client()
        MAX_CONCURRENT_API_CALLS = 8
        api_semaphore = asyncio.Semaphore(MAX_CONCURRENT_API_CALLS)
        # Per-client semaphore: limit concurrent requests on the shared client
        # 2 concurrent per client prevents client overload
        client_semaphore = asyncio.Semaphore(2)

        # At this point, the user asks for individual processing of tags. ----------------------------------------------------------------------------
        async def process_tag(tag):
            async with client_semaphore:  # Per-client limit (2 concurrent)
                async with api_semaphore:  # Global limit (8 concurrent)
                    try:
                        schema = _classifier.schema(
                            include_definition=include_definition,
                            include_attributes=include_attributes,
                            tags=[tag],
                            shared_tags=False,
                        )
                        # prompt = _classifier.prompt(schema, text)
                        if context_cache:
                            prompt = _classifier.prompt(
                                schema, context_cache=context_cache
                            )
                        elif text:
                            prompt = _classifier.prompt(schema, text=text)
                        elif image:
                            prompt = _classifier.prompt(
                                schema, image=image, image_mime=image_mime
                            )
                        elif image_file:
                            prompt = _classifier.prompt(
                                schema, image_file=image_file, image_mime=image_mime
                            )
                        else:
                            raise ValueError(
                                "Either 'text', 'image', or 'image_file' must be provided for classification."
                            )

                        # client = get_genai_client()
                        # Log before Gemini API call
                        start_time = time.time()
                        lg.log_struct(
                            message=f"Gemini API call started for tag: {tag}",
                            structured_data={
                                **base_log,
                                "event": "gemini_call_start",
                                "tag": tag,
                                "start_time": start_time,
                            },
                            severity="INFO",
                        )

                        resai = await call_with_retry_for_tag(
                            tag_client,
                            prompt,
                            schema,
                            genai_model_name,
                            config,
                            tag,
                            retries=3,
                        )

                        # Log after Gemini API call
                        end_time = time.time()
                        lg.log_struct(
                            message=f"Gemini API call ended for tag: {tag}",
                            structured_data={
                                **base_log,
                                "event": "gemini_call_end",
                                "tag": tag,
                                "end_time": end_time,
                                "duration": end_time - start_time,
                            },
                            severity="INFO",
                        )

                        t_output = json.loads(clean_enclosed_markdown(resai.text))
                        t_output.update(
                            {
                                "usage_metadata": get_genai_usage_metadata(
                                    resai, only_numeric=True
                                )
                            }
                        )

                        return {tag: t_output}
                    except Exception as e:
                        lg.log_struct(
                            message=f"Error processing tag {tag}: {type(e).__name__} -- {e}",
                            structured_data={
                                **base_log,
                                "event": "tag_processing_error",
                                "tag": tag,
                                "error_type": type(e).__name__,
                                "error_message": str(e),
                            },
                            severity="ERROR",
                        )
                        return {
                            tag: {
                                "error": f"Error processing tag {tag}: {type(e).__name__} -- {e}"
                            }
                        }

        async def process_shared():
            async with client_semaphore:  # Per-client limit (2 concurrent)
                async with api_semaphore:  # Global limit (8 concurrent)
                    try:
                        schema = _classifier.schema(
                            include_definition=include_definition,
                            include_attributes=False,
                            tags=tags,
                            shared_tags=shared_tags,
                        )
                    except Exception:
                        return {
                            "Shared": {
                                "error": {
                                    "message": "Error generating schema for shared questions",
                                    "code": 400,
                                }
                            }
                        }

                    if schema:
                        try:
                            if context_cache:
                                prompt = _classifier.prompt(
                                    schema, context_cache=context_cache
                                )
                            elif text:
                                prompt = _classifier.prompt(schema, text=text)
                            elif image:
                                prompt = _classifier.prompt(
                                    schema, image=image, image_mime=image_mime
                                )
                            elif image_file:
                                prompt = _classifier.prompt(
                                    schema, image_file=image_file, image_mime=image_mime
                                )
                            else:
                                raise ValueError(
                                    "Either 'text', 'image', or 'image_file' must be provided for classification."
                                )

                            # client = get_genai_client()
                            # Log before Gemini API call
                            start_time = time.time()
                            lg.log_struct(
                                message="Gemini API call started for tag: Shared",
                                structured_data={
                                    **base_log,
                                    "event": "gemini_call_start",
                                    "tag": "Shared",
                                    "start_time": start_time,
                                },
                                severity="INFO",
                            )
                            resai = await call_with_retry_for_tag(
                                tag_client,
                                prompt,
                                schema,
                                genai_model_name,
                                config,
                                "Shared",
                                retries=3,
                            )

                            # Log after Gemini API call
                            end_time = time.time()
                            lg.log_struct(
                                message="Gemini API call finished for tag: Shared",
                                structured_data={
                                    **base_log,
                                    "event": "gemini_call_end",
                                    "tag": "Shared",
                                    "end_time": end_time,
                                    "duration": end_time - start_time,
                                },
                                severity="INFO",
                            )

                            t_output = json.loads(clean_enclosed_markdown(resai.text))
                            t_output.update(
                                {
                                    "usage_metadata": get_genai_usage_metadata(
                                        resai, only_numeric=True
                                    )
                                }
                            )
                            return {"Shared": t_output}

                        except Exception as e:
                            lg.log_struct(
                                message=f"Error running LLM on shared questions: {type(e).__name__} -- {e}",
                                structured_data={
                                    **base_log,
                                    "event": "shared_processing_error",
                                    "error_type": type(e).__name__,
                                    "error_message": str(e),
                                },
                                severity="ERROR",
                            )
                            return {
                                "Shared": {
                                    "error": {
                                        "message": f"Error running LLM on shared questions: {type(e).__name__} -- {e}",
                                        "code": 400,
                                    }
                                }
                            }
                    else:
                        return {"Shared": {}}

        st = time.time()
        output = {}
        usages = {}
        tasks = []
        try:
            if include_attributes:
                tasks += [asyncio.create_task(process_tag(tag)) for tag in tags]

            if (shared_tags and _classifier.shared_tags) or include_definition:
                tasks.append(asyncio.create_task(process_shared()))

            outcomes = await asyncio.gather(*tasks)

            shared = None
            for res in outcomes:
                for k, values in res.items():
                    if "error" in values:
                        lg.log_struct(
                            message=f"Error during asynchronous processing for tag {k}: {values['error']}",
                            structured_data={
                                **base_log,
                                "event": "async_processing_error",
                                "tag": k,
                                "error_message": values["error"],
                            },
                            severity=Severity.ERROR,
                        )
                        result["status"] = 0
                        result["details"] = {"error": {"message": values["error"]}}
                        return JSONResponse(content=result, status_code=400)
                    if k == "Shared":
                        shared = values
                        continue
                    for u_k, u_v in values.pop("usage_metadata", {}).items():
                        try:
                            usages[u_k] += u_v
                        except KeyError:
                            usages[u_k] = u_v
                    output.update(values)
            if shared:
                for u_k, u_v in shared.pop("usage_metadata", {}).items():
                    try:
                        usages[u_k] += u_v
                    except KeyError:
                        usages[u_k] = u_v
                output.update(shared)
        except Exception as ex:
            lg.log_text("Error during asynchronous processing", severity=Severity.ERROR)
            result["status"] = 0
            result["details"] = {
                "error": {
                    "message": f"Error during asynchronous processing: {type(ex).__name__} -- {str(ex)}"
                }
            }
            return JSONResponse(content=result, status_code=400)

        if len(remove_tags) > 0:
            removed = []
            for k in output:
                if k in remove_tags:
                    removed.append(k)
            for ele in removed:
                _ = output.pop(ele)

        result["output"] = output
        result["details"]["run_strategy"] = "Individual"
        result["details"]["duration"] = time.time() - st
        result["details"]["usage_metadata"] = usages
        lg.log_struct(
            message="ODE LLM-based Chunk Tagging - /classify successful",
            structured_data=base_log,
            severity="INFO",
        )

        return result
    except TagNotExist as e:
        lg.log_text(
            f"Error in classify_text endpoint: {type(e).__name__} -- {e}",
            severity=Severity.ERROR,
        )
        response.status_code = status.HTTP_400_BAD_REQUEST
        details = {
            "error": {
                "message": f"{type(e).__name__} -- {e}",
                "details": {
                    "classify": include_definition,
                    "extract": include_attributes,
                    "custom": custom_run,
                    "tags": tags,
                    "individual": individual,
                },
            }
        }
        return {"status": 0, "service": servid, "output": None, "details": details}
    except Exception as e:
        lg.log_text(
            f"Error in classify_text endpoint: {type(e).__name__} -- {e}",
            severity=Severity.ERROR,
        )
        response.status_code = status.HTTP_400_BAD_REQUEST
        return {
            "status": 0,
            "service": servid,
            "output": None,
            "details": {
                "error": {
                    "message": f"Error during classification: {type(e).__name__} -- {e}"
                }
            },
        }


@app.post("/definitions", status_code=200)
def get_definitions(request_data: RequestDefinitions):
    """
    Endpoint to retrieve definitions for specified tags.
    """
    servid = service_id + " - Definitions - " + classifier.name
    tags = request_data.tags
    result = {}
    for tag in tags:
        try:
            result[tag] = classifier._org_definitions[tag]
        except KeyError:
            pass

    result = {
        "status": 1,
        "service": servid,
        "output": result,
        "details": {"tags": tags},
    }
    return result


if __name__ == "__main__":
    import time
    import os
    import base64
    from fastapi.testclient import TestClient
    from app.main import app

    client = TestClient(app)

    # text = textwrap.dedent("""Clinical notes are an essential component of patient care.""")
    # image_file = os.getcwd() + r'\..\test_osm_1-test.pdf'
    image_file = os.getcwd() + r".\tests\test_pdf.pdf"
    with open(image_file, "rb") as pdf_file:
        pdf_data = pdf_file.read()
    pdf_data = base64.b64encode(pdf_data).decode("utf-8")

    image_file = r"gs://ode-d-storage-bucket/test_osm_1.pdf"

    request_data = {
        # "text": text,
        # "image": pdf_data,
        "image_file": image_file,
        "image_mime": "application/pdf",
        # "tags": ["Radiology_Tag1"],
        "classify": True,
        "extract": True,
        "definitions": {
            "Radiology_Tag1": {
                "name": "Radiology Report",
                "description": "Reports related to radiological studies.",
                "configurations": {"extend": "ModLLMClasses"},
                "attributes": {
                    "EXT_0_Entities": [
                        "n",
                        "str",
                        "Entities mentioned in the document.",
                    ]
                },
            }
        },
        # "individual": True,
        # "shared_tags": True,
        # "extra_combine_run": True
    }

    st = time.time()
    response = client.post("/classify", json=request_data)
    print(response.status_code)
    for k, ele in response.json()["output"].items():
        print(k, ele)
    print(response.json()["details"]["run_strategy"])
    print(time.time() - st)

    # for k, ele in response.json()['details']['combine_def_run'].items():
    #     print(k, ele)
